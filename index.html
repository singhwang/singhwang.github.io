<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  <title>Singh Wang</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Singh Wang">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Singh Wang">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Singh Wang">
  
    <link rel="alternative" href="/atom.xml" title="Singh Wang" type="application/atom+xml">
  
  
    <link rel="icon" href="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg">
  
  <link rel="stylesheet" href="/css/style.css">
  
  

  <script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>
  <script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>

  
</head></html>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="https://avatars0.githubusercontent.com/u/17465385?s=400&amp;u=cea56ed1710ec6ebab160c1ce1e068317e05b025&amp;v=4" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Singh Wang</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/archives">所有文章</a></li>
				        
							<li><a href="/categories/容器云技术/">容器云技术</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/singhwang" title="github">github</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/Calico/" style="font-size: 11.67px;">Calico</a> <a href="/tags/Ceph/" style="font-size: 10px;">Ceph</a> <a href="/tags/Common/" style="font-size: 10px;">Common</a> <a href="/tags/Docker/" style="font-size: 20px;">Docker</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Fluentd/" style="font-size: 11.67px;">Fluentd</a> <a href="/tags/Ingress/" style="font-size: 10px;">Ingress</a> <a href="/tags/Kubernetes/" style="font-size: 18.33px;">Kubernetes</a> <a href="/tags/Label/" style="font-size: 10px;">Label</a> <a href="/tags/Logging/" style="font-size: 13.33px;">Logging</a> <a href="/tags/Monitoring/" style="font-size: 10px;">Monitoring</a> <a href="/tags/Network/" style="font-size: 15px;">Network</a> <a href="/tags/Prometheus/" style="font-size: 10px;">Prometheus</a> <a href="/tags/Setup/" style="font-size: 10px;">Setup</a> <a href="/tags/Storage/" style="font-size: 10px;">Storage</a> <a href="/tags/TimeZone/" style="font-size: 10px;">TimeZone</a> <a href="/tags/Trouble-Shooting/" style="font-size: 16.67px;">Trouble Shooting</a>
					</div>
				</section>
				
				
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">Singh Wang</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="https://avatars0.githubusercontent.com/u/17465385?s=400&amp;u=cea56ed1710ec6ebab160c1ce1e068317e05b025&amp;v=4" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author">Singh Wang</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/archives">所有文章</a></li>
		        
					<li><a href="/categories/容器云技术/">容器云技术</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/singhwang" title="github">github</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap">
  
    <article id="post-kubernetes_trouble_shooting_002" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/01/21/kubernetes_trouble_shooting_002/" class="article-date">
  	<time datetime="2019-01-21T01:47:22.815Z" itemprop="datePublished">2019-01-21</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/21/kubernetes_trouble_shooting_002/">
        如何保护系统级Pod不被Kubelet驱逐？
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>一、环境版本信息<br>Kubeadm v1.11.0<br>Kubelet v1.11.0<br>Calico v3.1.3</p>
<p>二、问题描述<br>使用Kubeadm搭建的Kubernetes集群在资源紧张的情况下，Kubernetes Control Plan开始挂掉了，集群开始变得不好用。比如你可以让你的Kubernetes Control Plan所在的服务器，磁盘使用率超过90%，问题就会立马复现。</p>
<p>三、问题定位<br>经排查发现，资源紧张的情况下，Kubelet会驱逐对应Node上的Pod，当然，这也包括Kubernetes Control Plan的相关Pod，会导致其被杀死及其相关镜像被删除。Kubelet这么做是为了释放资源，当然把Kubernetes Control Plan的相关Pod都干掉，并不是用户所希望看到的。一旦放大到Node节点上，问题就变成Node节点上非常重要的系统Pod会被干掉，导致相关的Node变成不好用的Node节点了。一旦放大到整个集群上，就变成Kubernetes Control Plan的相关Pod、Node节点上非常重要的系统级别Pod和集群上非常重要的Addon级别的Pod都会被干掉。问题的严重性，在规模效应下，可想而知了。</p>
<p>四、问题分析<br>有些Kubernetes的使用者可能会说，Kubeadm搭建的Kubernetes集群不推荐在生产环境使用，你非要用它，肯定会有问题。这一点也不奇怪。因为一直以来，使用Kubeadm搭建的Kubernetes集群都不推荐在生产环境使用，最开始是官方不推荐，大部分使用者也随着不推荐。随着Kubernetes的发展，我们逐渐发现使用Kubeadm搭建Kubernetes集群原有那句不推荐在生产环境使用的禁止提示不见了，你也会发现官方文档上花费了大量的笔墨介绍Kubeadm这种安装方式，甚至关于这种方式的高可用方案，官方文档也开始有了描述。但是对于其他的安装方式都是一笔带过，甚至只字未提。说明了什么？那便是官方推荐并鼓励用户使用这种方式部署Kubernetes集群。</p>
<p>该种方式搭建的集群真得就能应用于生产环境吗？如果不能，那么又是哪些问题导致的它不能应用于生产环境呢？笔者在实际工作中做了大量的实践，目前已经将其应用于生产环境了。实践过程中发现，Kubeadm已经相当成熟了，问题并不是很多，不过还是有一些的，但是就算有问题，读其规范的代码结构，改起来也相当方便。笔者在生产环境使用的版本，就做过一些修改。</p>
<p>这里我只给出Kubeadm在实际应用中两个大问题，并针对其中一个致命问题在本文中详细描述其解决方案。</p>
<ol>
<li>时区问题。一直以来，时区问题都是个看似简单又不引起重视的问题，特别对于美国用户，默认的UTC时区足够满足用户的需求了。那么生活在其他时区的用户怎么办？特别是我们这些生活在中国的用户该怎么办？修改Kubeadm的源码，在Kubernetes Control Plan的相关Pod的YAML上，加入社区设置的相关支持，详见《Docker容器的时区设置》。</li>
<li>当节点资源紧张，Kubelet开始驱逐Pod，删除其对应的镜像，来释放资源。看似没有问题，实际呢？很悲剧，这是个悲催的致命问题。为什么呢？对于Kubeadm这种安装方式，kube-apiserver、kube-controller-manager、kube-scheduler这种静态Pod没有问题，官方有解决方法不让其被驱逐，但是对于以Addon方式安装的系统级Pod呢？悲催了，没有保护机制。想重现问题很简单，磁盘使用率到90%以上，集群立刻开始往复于自杀和恢复之间。能解决吗？当然可以，不过社区现在并没有解决，笔者查看了GitHub上相关的Issue，详见参考资料。看来为了应用到生产环境，只能自己动手解决了。怎么解决呢？这个问题是Kubelet导致的，肯定是需要修改Kubelet的源码。</li>
</ol>
<p>五、问题解决（这里以Kubernetes v1.11.0版本为例，更高的版本会有所变化）<br>示例如下：（以示例描述，如何看笔者要添加和修改的代码）<br>example.go<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">这里是要添加和修改的代码</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>下面看具体的源码修改：</p>
<ol>
<li><p>kube_features.go</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">// defaultKubernetesFeatureGates consists of all known Kubernetes-specific feature keys.</span><br><span class="line">// To add a new feature, define a key for it above and add it here. The features will be</span><br><span class="line">// available throughout Kubernetes binaries.</span><br><span class="line">var defaultKubernetesFeatureGates = map[utilfeature.Feature]utilfeature.FeatureSpec&#123;</span><br><span class="line">	AppArmor:                                    &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	DynamicKubeletConfig:                        &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	ExperimentalHostUserNamespaceDefaultingGate: &#123;Default: false, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	ExperimentalCriticalPodAnnotation:           &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	...</span><br><span class="line">	ExperimentalNotEvictedPodAnnotation:         &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	...</span><br><span class="line">	DevicePlugins:                               &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	TaintBasedEvictions:                         &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	RotateKubeletServerCertificate:              &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	RotateKubeletClientCertificate:              &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	PersistentLocalVolumes:                      &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	LocalStorageCapacityIsolation:               &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	HugePages:                                   &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	Sysctls:                                     &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	DebugContainers:                             &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	PodShareProcessNamespace:                    &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	PodPriority:                                 &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	EnableEquivalenceClassCache:                 &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	TaintNodesByCondition:                       &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	MountPropagation:                            &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	QOSReserved:                                 &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	ExpandPersistentVolumes:                     &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	ExpandInUsePersistentVolumes:                &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	AttachVolumeLimit:                           &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	CPUManager:                                  &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	ServiceNodeExclusion:                        &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	MountContainers:                             &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	VolumeScheduling:                            &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	CSIPersistentVolume:                         &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	CustomPodDNS:                                &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	BlockVolume:                                 &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	StorageObjectInUseProtection:                &#123;Default: true, PreRelease: utilfeature.GA&#125;,</span><br><span class="line">	ResourceLimitsPriorityFunction:              &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	SupportIPVSProxyMode:                        &#123;Default: true, PreRelease: utilfeature.GA&#125;,</span><br><span class="line">	SupportPodPidsLimit:                         &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	HyperVContainer:                             &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	ScheduleDaemonSetPods:                       &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	TokenRequest:                                &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	TokenRequestProjection:                      &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	CRIContainerLogRotation:                     &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	GCERegionalPersistentDisk:                   &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	RunAsGroup:                                  &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	VolumeSubpath:                               &#123;Default: true, PreRelease: utilfeature.GA&#125;,</span><br><span class="line">	BalanceAttachedNodeVolumes:                  &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	DynamicProvisioningScheduling:               &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	PodReadinessGates:                           &#123;Default: false, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	VolumeSubpathEnvExpansion:                   &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	KubeletPluginsWatcher:                       &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	ResourceQuotaScopeSelectors:                 &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	CSIBlockVolume:                              &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line"></span><br><span class="line">	// inherited features from generic apiserver, relisted here to get a conflict if it is changed</span><br><span class="line">	// unintentionally on either side:</span><br><span class="line">	genericfeatures.StreamingProxyRedirects: &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	genericfeatures.AdvancedAuditing:        &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	genericfeatures.APIResponseCompression:  &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	genericfeatures.Initializers:            &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	genericfeatures.APIListChunking:         &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line"></span><br><span class="line">	// inherited features from apiextensions-apiserver, relisted here to get a conflict if it is changed</span><br><span class="line">	// unintentionally on either side:</span><br><span class="line">	apiextensionsfeatures.CustomResourceValidation:   &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	apiextensionsfeatures.CustomResourceSubresources: &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line"></span><br><span class="line">	// features that enable backwards compatibility but are scheduled to be removed</span><br><span class="line">	ServiceProxyAllowExternalIPs: &#123;Default: false, PreRelease: utilfeature.Deprecated&#125;,</span><br><span class="line">	ReadOnlyAPIDataVolumes:       &#123;Default: true, PreRelease: utilfeature.Deprecated&#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	// owner: @vishh</span><br><span class="line">	// alpha: v1.5</span><br><span class="line">	//</span><br><span class="line">	// Ensures guaranteed scheduling of pods marked with a special pod annotation `scheduler.alpha.kubernetes.io/critical-pod`</span><br><span class="line">	// and also prevents them from being evicted from a node.</span><br><span class="line">	// Note: This feature is not supported for `BestEffort` pods.</span><br><span class="line">	ExperimentalCriticalPodAnnotation utilfeature.Feature = &quot;ExperimentalCriticalPodAnnotation&quot;</span><br><span class="line">...</span><br><span class="line">	// owner: @singhwang</span><br><span class="line">	// alpha: v1.5</span><br><span class="line">	//</span><br><span class="line">	// Ensures guaranteed scheduling of pods marked with a special pod annotation `scheduler.alpha.kubernetes.io/not-evicted-pod`</span><br><span class="line">	// and also prevents them from being evicted from a node.</span><br><span class="line">	ExperimentalNotEvictedPodAnnotation utilfeature.Feature = &quot;ExperimentalNotEvictedPodAnnotation&quot;</span><br><span class="line">...</span><br><span class="line">	// owner: @jiayingz</span><br><span class="line">	// beta: v1.10</span><br><span class="line">	//</span><br><span class="line">	// Enables support for Device Plugins</span><br><span class="line">	DevicePlugins utilfeature.Feature = &quot;DevicePlugins&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>pod_update.go</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">const (</span><br><span class="line">	ConfigSourceAnnotationKey    = &quot;kubernetes.io/config.source&quot;</span><br><span class="line">	ConfigMirrorAnnotationKey    = v1.MirrorPodAnnotationKey</span><br><span class="line">	ConfigFirstSeenAnnotationKey = &quot;kubernetes.io/config.seen&quot;</span><br><span class="line">	ConfigHashAnnotationKey      = &quot;kubernetes.io/config.hash&quot;</span><br><span class="line">	CriticalPodAnnotationKey     = &quot;scheduler.alpha.kubernetes.io/critical-pod&quot;</span><br><span class="line">	...</span><br><span class="line">	NotEvictedPodAnnotationKey   = &quot;scheduler.alpha.kubernetes.io/not-evicted-pod&quot;</span><br><span class="line">	...</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">// IsCriticalPodBasedOnPriority checks if the given pod is a critical pod based on priority resolved from pod Spec.</span><br><span class="line">func IsCriticalPodBasedOnPriority(priority int32) bool &#123;</span><br><span class="line">	if priority &gt;= scheduling.SystemCriticalPriority &#123;</span><br><span class="line">		return true</span><br><span class="line">	&#125;</span><br><span class="line">	return false</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">// IsNotEvictedPod returns true if the pod bears the not evicted pod annotation key.</span><br><span class="line">func IsNotEvictedPod(pod *v1.Pod) bool &#123;</span><br><span class="line">	return IsNotEvicted(pod.Namespace, pod.Annotations)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// IsNotEvicted returns true if parameters bear the not evicted pod annotation key.</span><br><span class="line">func IsNotEvicted(ns string, annotations map[string]string) bool &#123;</span><br><span class="line">	// NotEvicted pods are restricted to &quot;kube-system&quot; namespace as of now.</span><br><span class="line">	if ns != kubeapi.NamespaceSystem &#123;</span><br><span class="line">		return false</span><br><span class="line">	&#125;</span><br><span class="line">	val, ok := annotations[NotEvictedPodAnnotationKey]</span><br><span class="line">	if ok &amp;&amp; val == &quot;&quot; &#123;</span><br><span class="line">		return true</span><br><span class="line">	&#125;</span><br><span class="line">	return false</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
</li>
<li><p>eviction_manager.go</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">func (m *managerImpl) evictPod(pod *v1.Pod, gracePeriodOverride int64, evictMsg string, annotations map[string]string) bool &#123;</span><br><span class="line">	// If the pod is marked as critical and static, and support for critical pod annotations is enabled,</span><br><span class="line">	// do not evict such pods. Static pods are not re-admitted after evictions.</span><br><span class="line">	// https://github.com/kubernetes/kubernetes/issues/40573 has more details.</span><br><span class="line">	if utilfeature.DefaultFeatureGate.Enabled(features.ExperimentalCriticalPodAnnotation) &amp;&amp;</span><br><span class="line">		kubelettypes.IsCriticalPod(pod) &amp;&amp; kubepod.IsStaticPod(pod) &#123;</span><br><span class="line">		glog.Errorf(&quot;eviction manager: cannot evict a critical static pod %s&quot;, format.Pod(pod))</span><br><span class="line">		return false</span><br><span class="line">	&#125;</span><br><span class="line">...</span><br><span class="line">	// If the pod is marked as not evicted, and support for not evicted pod annotations is enabled,</span><br><span class="line">	// do not evict such pods.</span><br><span class="line">	if utilfeature.DefaultFeatureGate.Enabled(features.ExperimentalNotEvictedPodAnnotation) &amp;&amp;</span><br><span class="line">		kubelettypes.IsNotEvictedPod(pod) &#123;</span><br><span class="line">		glog.Errorf(&quot;eviction manager: cannot evict a marked not evicted pod %s&quot;, format.Pod(pod))</span><br><span class="line">		return false</span><br><span class="line">	&#125;</span><br><span class="line">...	</span><br><span class="line">	status := v1.PodStatus&#123;</span><br><span class="line">		Phase:   v1.PodFailed,</span><br><span class="line">		Message: evictMsg,</span><br><span class="line">		Reason:  Reason,</span><br><span class="line">	&#125;</span><br><span class="line">	// record that we are evicting the pod</span><br><span class="line">	m.recorder.AnnotatedEventf(pod, annotations, v1.EventTypeWarning, Reason, evictMsg)</span><br><span class="line">	// this is a blocking call and should only return when the pod and its containers are killed.</span><br><span class="line">	err := m.killPodFunc(pod, status, &amp;gracePeriodOverride)</span><br><span class="line">	if err != nil &#123;</span><br><span class="line">		glog.Errorf(&quot;eviction manager: pod %s failed to evict %v&quot;, format.Pod(pod), err)</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		glog.Infof(&quot;eviction manager: pod %s is evicted successfully&quot;, format.Pod(pod))</span><br><span class="line">	&#125;</span><br><span class="line">	return true</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>重新编译源代码，生成新的Kubelet二进制文件，替换的对应的Node上，使用方式如下：<br>注意–feature-gates部分，其中ExperimentalCriticalPodAnnotation为官方原生支持的功能，它可以保证kube-system下的Critical级别的静态Pod始终不被Kubelet所驱逐。另一个ExperimentalNotEvictedPodAnnotation是笔者加入的功能特性，它可以保证kube-system下的Pod如果其Annotations中包括scheduler.alpha.kubernetes.io/not-evicted-pod，那么这样的Pod也始终不会被Kubelet所驱逐。这个功能特的目的是保证，Kubernetes容器系统的系统级Pod和重要Addon级别的Pod，允许用户通过这个scheduler.alpha.kubernetes.io/not-evicted-pod把那些系统级Pod（用户为了扩充Kubernetes功能的而增加的非常重要的Pod）保护起来，提高Kubernetes容器系统的稳健性，避免集群资源紧张时，陷入集群自杀和自我修复的循环中而无法恢复。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KUBELET_EXTRA_ARGS=&quot;--cgroup-driver=cgroupfs --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1 --feature-gates=ExperimentalCriticalPodAnnotation=true,ExperimentalNotEvictedPodAnnotation=true&quot;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>六、参考资料<br><a href="https://github.com/kubernetes/kubernetes/tree/v1.11.0" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/tree/v1.11.0</a><br><a href="https://github.com/kubernetes/kubernetes/issues/53659" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/issues/53659</a><br><a href="https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/</a><br><a href="https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/</a><br><a href="https://www.centos.bz/2017/09/linux-命令行-创建指定大小的文件/" target="_blank" rel="noopener">https://www.centos.bz/2017/09/linux-命令行-创建指定大小的文件/</a><br><a href="https://www.hi-linux.com/posts/59095.html" target="_blank" rel="noopener">https://www.hi-linux.com/posts/59095.html</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Trouble-Shooting/">Trouble Shooting</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kubernetes_calico_001" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/01/17/kubernetes_calico_001/" class="article-date">
  	<time datetime="2019-01-17T04:35:52.644Z" itemprop="datePublished">2019-01-17</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/17/kubernetes_calico_001/">
        Calico IPIP（Always和CrossSubnet）模式和BGP模式的区别
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>一、分析网路配置的区别</p>
<ol>
<li><p>IPIP Always 模式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"># ip link</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:21:eb:a0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default</span><br><span class="line">    link/ether 02:42:2f:64:85:3d brd ff:ff:ff:ff:ff:ff</span><br><span class="line">8: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1440 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">9: cali436bd393c1c@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">10: cali09539dd2c0c@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br><span class="line"># ip address</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:21:eb:a0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.16.170.128/24 brd 172.16.170.255 scope global ens33</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::20c:29ff:fe21:eba0/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default</span><br><span class="line">    link/ether 02:42:2f:64:85:3d brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.17.0.1/16 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">8: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1440 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">    inet 10.211.0.1/32 brd 10.211.0.1 scope global tunl0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">9: cali436bd393c1c@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet6 fe80::ecee:eeff:feee:eeee/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">10: cali09539dd2c0c@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br><span class="line">    inet6 fe80::ecee:eeff:feee:eeee/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"># ip route</span><br><span class="line">default via 172.16.170.2 dev ens33</span><br><span class="line">blackhole 10.211.0.0/24 proto bird</span><br><span class="line">10.211.0.16 dev cali436bd393c1c scope link</span><br><span class="line">10.211.0.17 dev cali09539dd2c0c scope link</span><br><span class="line">10.211.1.0/24 via 172.16.170.129 dev tunl0 proto bird onlink</span><br><span class="line">10.211.2.0/24 via 172.16.170.130 dev tunl0 proto bird onlink</span><br><span class="line">169.254.0.0/16 dev ens33 scope link metric 1002</span><br><span class="line">172.16.170.0/24 dev ens33 proto kernel scope link src 172.16.170.128</span><br><span class="line">172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1</span><br></pre></td></tr></table></figure>
</li>
<li><p>IPIP CrossSubnet 模式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"># ip link</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:21:eb:a0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default</span><br><span class="line">    link/ether 02:42:74:e2:55:16 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">4: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1440 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">5: califb703006b7d@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">6: cali3c997f406a6@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br><span class="line"># ip address</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:21:eb:a0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.16.170.128/24 brd 172.16.170.255 scope global ens33</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::20c:29ff:fe21:eba0/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default</span><br><span class="line">    link/ether 02:42:74:e2:55:16 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.17.0.1/16 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">4: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1440 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">    inet 10.211.0.1/32 brd 10.211.0.1 scope global tunl0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">5: califb703006b7d@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet6 fe80::ecee:eeff:feee:eeee/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">6: cali3c997f406a6@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br><span class="line">    inet6 fe80::ecee:eeff:feee:eeee/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"># ip route</span><br><span class="line">default via 172.16.170.2 dev ens33</span><br><span class="line">blackhole 10.211.0.0/24 proto bird</span><br><span class="line">10.211.0.18 dev califb703006b7d scope link</span><br><span class="line">10.211.0.19 dev cali3c997f406a6 scope link</span><br><span class="line">10.211.1.0/24 via 172.16.170.129 dev ens33 proto bird</span><br><span class="line">10.211.2.0/24 via 172.16.170.130 dev ens33 proto bird</span><br><span class="line">169.254.0.0/16 dev ens33 scope link metric 1002</span><br><span class="line">172.16.170.0/24 dev ens33 proto kernel scope link src 172.16.170.128</span><br><span class="line">172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1</span><br></pre></td></tr></table></figure>
</li>
<li><p>BGP 模式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"># ip link</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:21:eb:a0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default</span><br><span class="line">    link/ether 02:42:b7:6f:cc:b2 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">6: cali72590f2ff6e@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">7: cali439e67ff763@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br><span class="line"># ip address</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:21:eb:a0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.16.170.128/24 brd 172.16.170.255 scope global ens33</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::20c:29ff:fe21:eba0/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default</span><br><span class="line">    link/ether 02:42:b7:6f:cc:b2 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.17.0.1/16 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">6: cali72590f2ff6e@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet6 fe80::ecee:eeff:feee:eeee/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">7: cali439e67ff763@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br><span class="line">    inet6 fe80::ecee:eeff:feee:eeee/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"># ip route</span><br><span class="line">default via 172.16.170.2 dev ens33</span><br><span class="line">blackhole 10.211.0.0/24 proto bird</span><br><span class="line">10.211.0.22 dev cali72590f2ff6e scope link</span><br><span class="line">10.211.0.23 dev cali439e67ff763 scope link</span><br><span class="line">10.211.1.0/24 via 172.16.170.129 dev ens33 proto bird</span><br><span class="line">10.211.2.0/24 via 172.16.170.130 dev ens33 proto bird</span><br><span class="line">169.254.0.0/16 dev ens33 scope link metric 1002</span><br><span class="line">172.16.170.0/24 dev ens33 proto kernel scope link src 172.16.170.128</span><br><span class="line">172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>二、区别总结<br>Calico的IP Pool包括IPIP模式和BGP模式，其中IPIP模式又包括Always和CrossSubnet。IPIP Always简单说是指，Calico网路的路由的分发始终通过Node上的tunl0隧道实现；IPIP CrossSubnet简单说是指，当两个Pod所在的Node的地址在同一网段时，Calico网路的路由的分发则通过各个Node上的主机网卡实现。当两个Pod所在的Node的地址不在同一网段时，Calico网路的路由的分发才通过Node上的tunl0隧道实现。这种模式是IPIP Always和BGP模式的合体实现。</p>
<p>三、参考资料<br>Calico开启BGP模式：<br><a href="http://www.cnblogs.com/jinxj/p/9414830.html" target="_blank" rel="noopener">http://www.cnblogs.com/jinxj/p/9414830.html</a></p>
<p>Calico原理解读：<br><a href="https://blog.csdn.net/ccy19910925/article/details/82423452" target="_blank" rel="noopener">https://blog.csdn.net/ccy19910925/article/details/82423452</a></p>
<p>Calico基本原理和模拟：<br><a href="http://ju.outofmemory.cn/entry/367749" target="_blank" rel="noopener">http://ju.outofmemory.cn/entry/367749</a></p>
<p>calico/node配置文档：<br><a href="https://docs.projectcalico.org/v3.1/reference/node/configuration" target="_blank" rel="noopener">https://docs.projectcalico.org/v3.1/reference/node/configuration</a></p>
<p>Calico IP Pool介绍：<br><a href="https://www.jianshu.com/p/dcad6d74e526" target="_blank" rel="noopener">https://www.jianshu.com/p/dcad6d74e526</a><br><a href="http://www.361way.com/linux-tunnel/5199.html" target="_blank" rel="noopener">http://www.361way.com/linux-tunnel/5199.html</a><br><a href="https://blog.csdn.net/kkdelta/article/details/39611061" target="_blank" rel="noopener">https://blog.csdn.net/kkdelta/article/details/39611061</a></p>
<p>Calico 跨网段问题：<br><a href="https://blog.csdn.net/mailjoin/article/details/79695463" target="_blank" rel="noopener">https://blog.csdn.net/mailjoin/article/details/79695463</a><br><a href="https://www.lijiaocn.com/项目/2017/09/25/calico-ipip.html" target="_blank" rel="noopener">https://www.lijiaocn.com/项目/2017/09/25/calico-ipip.html</a></p>
<p>静态路由配置示例：<br><a href="https://jingyan.baidu.com/article/6dad5075f7c67aa123e36eb9.html" target="_blank" rel="noopener">https://jingyan.baidu.com/article/6dad5075f7c67aa123e36eb9.html</a><br><a href="http://blog.51cto.com/11101034/1906726" target="_blank" rel="noopener">http://blog.51cto.com/11101034/1906726</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Calico/">Calico</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Network/">Network</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kubernetes_calico_000" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/01/16/kubernetes_calico_000/" class="article-date">
  	<time datetime="2019-01-16T05:09:55.218Z" itemprop="datePublished">2019-01-16</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/16/kubernetes_calico_000/">
        Calico的网路通信过程跟踪（IPIP CrossSubnet 模式）
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>一、环境版本信息<br>Kubernetes v1.11.0<br>Calico v3.1.3<br>Calicoctl v3.1.3</p>
<p>二、核心概念解惑</p>
<ol>
<li>BGP协议是路由器与路由器之间的通信协议，建立在TCP上。路由器之间可以通过BGP协议交换彼此的路由信息。<br>A路由器 <----> BGP协议 <----> B路由器 <----> BGP协议 <----> C路由器</----></----></----></----></li>
<li>宿主机上运行了很多的Pod，这些Pod的IP地址和通信要怎么处理？<br>即把宿主机变成一台路由器。宿主机变身路由器后，现实的网络是怎么联通的，Pod之间就怎么联通，技术都是现成的，并且都已经支撑起连接全地球的互联网了。</li>
</ol>
<p>三、网络通信过程跟踪</p>
<ol>
<li><p>以Daemonset形式，在default下部署一组Pod，用于测试Pod的跨主机网络通信：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get node -o wide</span><br><span class="line">NAME       STATUS    ROLES     AGE       VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME</span><br><span class="line">server01   Ready     master    128d      v1.11.0   172.16.170.128   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-862.11.6.el7.x86_64   docker://17.3.1</span><br><span class="line">server02   Ready     &lt;none&gt;    128d      v1.11.0   172.16.170.129   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-514.el7.x86_64        docker://17.3.1</span><br><span class="line">server03   Ready     &lt;none&gt;    128d      v1.11.0   172.16.170.130   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-514.el7.x86_64        docker://17.3.1</span><br><span class="line"></span><br><span class="line"># kubectl get pod --all-namespaces -o wide</span><br><span class="line">NAMESPACE     NAME                               READY     STATUS    RESTARTS   AGE       IP               NODE</span><br><span class="line">default       network-5z4qp                      1/1       Running   0          12s       10.211.0.6       server01</span><br><span class="line">default       network-6k87z                      1/1       Running   0          12s       10.211.2.4       server03</span><br><span class="line">default       network-mngxw                      1/1       Running   0          12s       10.211.1.4       server02</span><br><span class="line">kube-system   calico-node-644wq                  2/2       Running   0          128d      172.16.170.128   server01</span><br><span class="line">kube-system   calico-node-hdkf6                  2/2       Running   0          128d      172.16.170.130   server03</span><br><span class="line">kube-system   calico-node-wltgp                  2/2       Running   0          128d      172.16.170.129   server02</span><br><span class="line">kube-system   coredns-777d78ff6f-6sjt5           1/1       Running   0          128d      10.211.0.3       server01</span><br><span class="line">kube-system   coredns-777d78ff6f-mr977           1/1       Running   0          128d      10.211.0.2       server01</span><br><span class="line">kube-system   etcd-server01                      1/1       Running   0          128d      172.16.170.128   server01</span><br><span class="line">kube-system   kube-apiserver-server01            1/1       Running   0          128d      172.16.170.128   server01</span><br><span class="line">kube-system   kube-controller-manager-server01   1/1       Running   0          128d      172.16.170.128   server01</span><br><span class="line">kube-system   kube-proxy-94k52                   1/1       Running   0          128d      172.16.170.129   server02</span><br><span class="line">kube-system   kube-proxy-czg29                   1/1       Running   0          128d      172.16.170.128   server01</span><br><span class="line">kube-system   kube-proxy-mnhrb                   1/1       Running   0          128d      172.16.170.130   server03</span><br><span class="line">kube-system   kube-scheduler-server01            1/1       Running   0          128d      172.16.170.128   server01</span><br></pre></td></tr></table></figure>
</li>
<li><p>选取master节点的calico-node，进入安装calicoctl命令行工具：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.1.3/calicoctl</span><br><span class="line"># kubectl cp calicoctl calico-node-644wq:/usr/local/bin/ -n kube-system -c calico-node</span><br><span class="line"># kubectl exec calico-node-644wq -n kube-system -c calico-node -- chmod 0755 /usr/local/bin/calicoctl</span><br></pre></td></tr></table></figure>
</li>
<li><p>进入master节点的calico-node，使用calicoctl查看需要关注的工作负载的相关信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"># kubectl exec -it calico-node-644wq /bin/sh -n kube-system -c calico-node</span><br><span class="line">/ # calicoctl get workloadendpoint -n default</span><br><span class="line">NAMESPACE   WORKLOAD        NODE       NETWORKS        INTERFACE</span><br><span class="line">default     network-5z4qp   server01   10.211.0.6/32   cali50914021272</span><br><span class="line">default     network-6k87z   server03   10.211.2.4/32   calif17d1193010</span><br><span class="line">default     network-mngxw   server02   10.211.1.4/32   calid406b8b6c93</span><br><span class="line"></span><br><span class="line">/ # calicoctl get workloadendpoint -n default -o yaml</span><br><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">items:</span><br><span class="line">- apiVersion: projectcalico.org/v3</span><br><span class="line">  kind: WorkloadEndpoint</span><br><span class="line">  metadata:</span><br><span class="line">    creationTimestamp: 2019-01-16T06:46:19Z</span><br><span class="line">    labels:</span><br><span class="line">      app: network</span><br><span class="line">      controller-revision-hash: &quot;66086030&quot;</span><br><span class="line">      pod-template-generation: &quot;1&quot;</span><br><span class="line">      projectcalico.org/namespace: default</span><br><span class="line">      projectcalico.org/orchestrator: k8s</span><br><span class="line">    name: server01-k8s-network--5z4qp-eth0</span><br><span class="line">    namespace: default</span><br><span class="line">    resourceVersion: &quot;25227&quot;</span><br><span class="line">    uid: 6df53e81-195a-11e9-b5d1-000c2921eba0</span><br><span class="line">  spec:</span><br><span class="line">    endpoint: eth0</span><br><span class="line">    interfaceName: cali50914021272</span><br><span class="line">    ipNetworks:</span><br><span class="line">    - 10.211.0.6/32</span><br><span class="line">    node: server01</span><br><span class="line">    orchestrator: k8s</span><br><span class="line">    pod: network-5z4qp</span><br><span class="line">    profiles:</span><br><span class="line">    - kns.default</span><br><span class="line">- apiVersion: projectcalico.org/v3</span><br><span class="line">  kind: WorkloadEndpoint</span><br><span class="line">  metadata:</span><br><span class="line">    creationTimestamp: 2019-01-16T06:46:19Z</span><br><span class="line">    labels:</span><br><span class="line">      app: network</span><br><span class="line">      controller-revision-hash: &quot;66086030&quot;</span><br><span class="line">      pod-template-generation: &quot;1&quot;</span><br><span class="line">      projectcalico.org/namespace: default</span><br><span class="line">      projectcalico.org/orchestrator: k8s</span><br><span class="line">    name: server03-k8s-network--6k87z-eth0</span><br><span class="line">    namespace: default</span><br><span class="line">    resourceVersion: &quot;25084&quot;</span><br><span class="line">    uid: 6dfe39ec-195a-11e9-b5d1-000c2921eba0</span><br><span class="line">  spec:</span><br><span class="line">    endpoint: eth0</span><br><span class="line">    interfaceName: calif17d1193010</span><br><span class="line">    ipNetworks:</span><br><span class="line">    - 10.211.2.4/32</span><br><span class="line">    node: server03</span><br><span class="line">    orchestrator: k8s</span><br><span class="line">    pod: network-6k87z</span><br><span class="line">    profiles:</span><br><span class="line">    - kns.default</span><br><span class="line">- apiVersion: projectcalico.org/v3</span><br><span class="line">  kind: WorkloadEndpoint</span><br><span class="line">  metadata:</span><br><span class="line">    creationTimestamp: 2019-01-16T06:46:19Z</span><br><span class="line">    labels:</span><br><span class="line">      app: network</span><br><span class="line">      controller-revision-hash: &quot;66086030&quot;</span><br><span class="line">      pod-template-generation: &quot;1&quot;</span><br><span class="line">      projectcalico.org/namespace: default</span><br><span class="line">      projectcalico.org/orchestrator: k8s</span><br><span class="line">    name: server02-k8s-network--mngxw-eth0</span><br><span class="line">    namespace: default</span><br><span class="line">    resourceVersion: &quot;25082&quot;</span><br><span class="line">    uid: 6e0213f6-195a-11e9-b5d1-000c2921eba0</span><br><span class="line">  spec:</span><br><span class="line">    endpoint: eth0</span><br><span class="line">    interfaceName: calid406b8b6c93</span><br><span class="line">    ipNetworks:</span><br><span class="line">    - 10.211.1.4/32</span><br><span class="line">    node: server02</span><br><span class="line">    orchestrator: k8s</span><br><span class="line">    pod: network-mngxw</span><br><span class="line">    profiles:</span><br><span class="line">    - kns.default</span><br><span class="line">kind: WorkloadEndpointList</span><br><span class="line">metadata: &#123;&#125;</span><br><span class="line"></span><br><span class="line">/ # calicoctl get workloadendpoint server01-k8s-network--5z4qp-eth0 -n default -o yaml</span><br><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">kind: WorkloadEndpoint</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: 2019-01-16T06:46:19Z</span><br><span class="line">  labels:</span><br><span class="line">    app: network</span><br><span class="line">    controller-revision-hash: &quot;66086030&quot;</span><br><span class="line">    pod-template-generation: &quot;1&quot;</span><br><span class="line">    projectcalico.org/namespace: default</span><br><span class="line">    projectcalico.org/orchestrator: k8s</span><br><span class="line">  name: server01-k8s-network--5z4qp-eth0</span><br><span class="line">  namespace: default</span><br><span class="line">  resourceVersion: &quot;25227&quot;</span><br><span class="line">  uid: 6df53e81-195a-11e9-b5d1-000c2921eba0</span><br><span class="line">spec:</span><br><span class="line">  endpoint: eth0</span><br><span class="line">  interfaceName: cali50914021272</span><br><span class="line">  ipNetworks:</span><br><span class="line">  - 10.211.0.6/32</span><br><span class="line">  node: server01</span><br><span class="line">  orchestrator: k8s</span><br><span class="line">  pod: network-5z4qp</span><br><span class="line">  profiles:</span><br><span class="line">  - kns.default</span><br><span class="line"></span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure>
</li>
<li><p>选取master节点上的用于测试网络通信的Pod，进入查看它的网络配置信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"># kubectl exec -it network-5z4qp /bin/sh -n default</span><br><span class="line">/ # ip link</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">2: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">4: eth0@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether 4e:30:25:dc:21:88 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">/ # ip address</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">4: eth0@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether 4e:30:25:dc:21:88 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 10.211.0.6/32 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::4c30:25ff:fedc:2188/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">/ # ip neigh</span><br><span class="line">/ # ping -c 3 10.211.0.1</span><br><span class="line">PING 10.211.0.1 (10.211.0.1) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.0.1: icmp_seq=1 ttl=64 time=0.085 ms</span><br><span class="line">64 bytes from 10.211.0.1: icmp_seq=2 ttl=64 time=0.072 ms</span><br><span class="line">64 bytes from 10.211.0.1: icmp_seq=3 ttl=64 time=0.069 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.0.1 ping statistics ---</span><br><span class="line">3 packets transmitted, 3 received, 0% packet loss, time 2001ms</span><br><span class="line">rtt min/avg/max/mdev = 0.069/0.075/0.085/0.009 ms</span><br><span class="line">/ # ip neigh</span><br><span class="line">169.254.1.1 dev eth0 lladdr ee:ee:ee:ee:ee:ee REACHABLE</span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure>
</li>
<li><p>回到master节点上，查看calico的workload的yaml中cali50914021272对应的设备信息和所有设备的地址信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"># ip link show cali50914021272</span><br><span class="line">11: cali50914021272@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 2</span><br><span class="line"></span><br><span class="line"># ip address</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:21:eb:a0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.16.170.128/24 brd 172.16.170.255 scope global ens33</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::20c:29ff:fe21:eba0/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default</span><br><span class="line">    link/ether 02:42:ca:a4:09:a5 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.17.0.1/16 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::42:caff:fea4:9a5/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">4: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1440 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">    inet 10.211.0.1/32 brd 10.211.0.1 scope global tunl0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">5: cali009d9b46eef@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet6 fe80::ecee:eeff:feee:eeee/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">6: calib99e709bd2c@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br><span class="line">    inet6 fe80::ecee:eeff:feee:eeee/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">11: cali50914021272@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 2</span><br><span class="line">    inet6 fe80::ecee:eeff:feee:eeee/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>
</li>
<li><p>验证master上的Pod的网络数据包是否可以发送到master上：（其中10.211.0.1为tunl0设备的地址）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 开启一个新的终端执行</span><br><span class="line"># tcpdump -i cali50914021272 icmp -v</span><br><span class="line">tcpdump: listening on cali50914021272, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">02:00:24.930193 IP (tos 0x0, ttl 64, id 51910, offset 0, flags [DF], proto ICMP (1), length 84)</span><br><span class="line">    10.211.0.6 &gt; server01: ICMP echo request, id 18, seq 1, length 64</span><br><span class="line">02:00:24.930235 IP (tos 0x0, ttl 64, id 22594, offset 0, flags [none], proto ICMP (1), length 84)</span><br><span class="line">    server01 &gt; 10.211.0.6: ICMP echo reply, id 18, seq 1, length 64</span><br><span class="line"></span><br><span class="line"># 在原有终端上执行</span><br><span class="line"># kubectl exec -it network-5z4qp /bin/sh -n default</span><br><span class="line">/ # ping -c 1 10.211.0.1</span><br><span class="line">PING 10.211.0.1 (10.211.0.1) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.0.1: icmp_seq=1 ttl=64 time=0.081 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.0.1 ping statistics ---</span><br><span class="line">1 packets transmitted, 1 received, 0% packet loss, time 0ms</span><br><span class="line">rtt min/avg/max/mdev = 0.081/0.081/0.081/0.000 ms</span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看master上的路由表，看我们即将测试的目标Pod（地址为10.211.1.4）在master上对应的下一跳在哪里：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># ip route</span><br><span class="line">...</span><br><span class="line">blackhole 10.211.0.0/24 proto bird</span><br><span class="line">10.211.0.2 dev cali009d9b46eef scope link</span><br><span class="line">10.211.0.3 dev calib99e709bd2c scope link</span><br><span class="line">10.211.0.6 dev cali50914021272 scope link</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">10.211.1.0/24 via 172.16.170.129 dev ens33 proto bird</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
</li>
<li><p>验证发往目标Pod（地址为10.211.1.4）的网络数据包是否发往master的主机网卡ens33上：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 开启一个新的终端执行</span><br><span class="line"># tcpdump -i ens33 icmp -v</span><br><span class="line">tcpdump: listening on ens33, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">02:30:22.537037 IP (tos 0x0, ttl 63, id 23441, offset 0, flags [DF], proto ICMP (1), length 84)</span><br><span class="line">    10.211.0.6 &gt; 10.211.1.4: ICMP echo request, id 23, seq 1, length 64</span><br><span class="line">02:30:22.537476 IP (tos 0x0, ttl 63, id 62650, offset 0, flags [none], proto ICMP (1), length 84)</span><br><span class="line">    10.211.1.4 &gt; 10.211.0.6: ICMP echo reply, id 23, seq 1, length 64</span><br><span class="line"></span><br><span class="line"># 在原有终端上执行</span><br><span class="line"># kubectl exec -it network-5z4qp /bin/sh -n default</span><br><span class="line">/ # ping -c 1 10.211.1.4</span><br><span class="line">PING 10.211.1.4 (10.211.1.4) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.1.4: icmp_seq=1 ttl=62 time=0.526 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.1.4 ping statistics ---</span><br><span class="line">1 packets transmitted, 1 received, 0% packet loss, time 0ms</span><br><span class="line">rtt min/avg/max/mdev = 0.526/0.526/0.526/0.000 ms</span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure>
</li>
<li><p>验证发往目标Pod（地址为10.211.1.4）的网络数据包是否发送到Node（地址为172.16.170.129）的网卡ens33上：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 在地址为10.211.1.4的Pod所在的Node上</span><br><span class="line"># tcpdump -i ens33 icmp -v</span><br><span class="line">tcpdump: listening on ens33, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">02:33:17.808254 IP (tos 0x0, ttl 63, id 61238, offset 0, flags [DF], proto ICMP (1), length 84)</span><br><span class="line">    10.211.0.6 &gt; 10.211.1.4: ICMP echo request, id 33, seq 1, length 64</span><br><span class="line">02:33:17.808415 IP (tos 0x0, ttl 63, id 1878, offset 0, flags [none], proto ICMP (1), length 84)</span><br><span class="line">    10.211.1.4 &gt; 10.211.0.6: ICMP echo reply, id 33, seq 1, length 64</span><br><span class="line"></span><br><span class="line"># 回到地址为10.211.0.6的Pod所在的Node上</span><br><span class="line"># kubectl exec -it network-5z4qp /bin/sh -n default</span><br><span class="line">/ # ping -c 1 10.211.1.4</span><br><span class="line">PING 10.211.1.4 (10.211.1.4) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.1.4: icmp_seq=1 ttl=62 time=0.556 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.1.4 ping statistics ---</span><br><span class="line">1 packets transmitted, 1 received, 0% packet loss, time 0ms</span><br><span class="line">rtt min/avg/max/mdev = 0.556/0.556/0.556/0.000 ms</span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure>
</li>
<li><p>在Node（地址为172.16.170.129）上查看发往目标Pod（地址为10.211.1.4）的下一跳在哪里：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># ip route</span><br><span class="line">...</span><br><span class="line">10.211.1.4 dev calid406b8b6c93 scope link</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
</li>
<li><p>验证发往目标Pod（地址为10.211.1.4）的网络数据包是否发送到Node（地址为172.16.170.129）的网络设备calid406b8b6c93上：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># 在地址为10.211.1.4的Pod所在的Node上</span><br><span class="line"># ip link show calid406b8b6c93</span><br><span class="line">7: calid406b8b6c93@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line"></span><br><span class="line"># tcpdump -i calid406b8b6c93 icmp -v</span><br><span class="line">tcpdump: listening on calid406b8b6c93, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">02:39:15.303283 IP (tos 0x0, ttl 62, id 44511, offset 0, flags [DF], proto ICMP (1), length 84)</span><br><span class="line">    10.211.0.6 &gt; 10.211.1.4: ICMP echo request, id 38, seq 1, length 64</span><br><span class="line">02:39:15.303392 IP (tos 0x0, ttl 64, id 47955, offset 0, flags [none], proto ICMP (1), length 84)</span><br><span class="line">    10.211.1.4 &gt; 10.211.0.6: ICMP echo reply, id 38, seq 1, length 64</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 回到地址为10.211.0.6的Pod所在的Node上（即master上），开启一个新的终端</span><br><span class="line"># kubectl exec -it network-5z4qp /bin/sh -n default</span><br><span class="line">/ # ping -c 1 10.211.1.4</span><br><span class="line">PING 10.211.1.4 (10.211.1.4) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.1.4: icmp_seq=1 ttl=62 time=0.602 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.1.4 ping statistics ---</span><br><span class="line">1 packets transmitted, 1 received, 0% packet loss, time 0ms</span><br><span class="line">rtt min/avg/max/mdev = 0.602/0.602/0.602/0.000 ms</span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure>
</li>
<li><p>验证发往目标Pod（地址为10.211.1.4）的网络数据包是否发送到它的网络设备eth0上：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"># kubectl exec -it network-mngxw /bin/sh -n default</span><br><span class="line">/ # ip link</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">2: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN mode DEFAULT group default qlen 1</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">4: eth0@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether e2:89:27:9e:6f:a6 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">/ # ip address</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">4: eth0@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether e2:89:27:9e:6f:a6 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 10.211.1.4/32 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::e089:27ff:fe9e:6fa6/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">/ # tcpdump -i eth0 icmp -v</span><br><span class="line">tcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">07:42:45.870953 IP (tos 0x0, ttl 62, id 13540, offset 0, flags [DF], proto ICMP (1), length 84)</span><br><span class="line">    10.211.0.6 &gt; network-mngxw: ICMP echo request, id 43, seq 1, length 64</span><br><span class="line">07:42:45.871065 IP (tos 0x0, ttl 64, id 19616, offset 0, flags [none], proto ICMP (1), length 84)</span><br><span class="line">    network-mngxw &gt; 10.211.0.6: ICMP echo reply, id 43, seq 1, length 64</span><br><span class="line"></span><br><span class="line"># 回到地址为10.211.0.6的Pod所在的Node上（即master上）</span><br><span class="line"># kubectl exec -it network-5z4qp /bin/sh -n default</span><br><span class="line">/ # ping -c 1 10.211.1.4</span><br><span class="line">PING 10.211.1.4 (10.211.1.4) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.1.4: icmp_seq=1 ttl=62 time=0.602 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.1.4 ping statistics ---</span><br><span class="line">1 packets transmitted, 1 received, 0% packet loss, time 0ms</span><br><span class="line">rtt min/avg/max/mdev = 0.602/0.602/0.602/0.000 ms</span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>四、参考资料<br><a href="https://docs.projectcalico.org/v3.1/usage/calicoctl/install" target="_blank" rel="noopener">https://docs.projectcalico.org/v3.1/usage/calicoctl/install</a><br><a href="https://mp.weixin.qq.com/s/MZIj_cvvtTiAfNf_0lpfTg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/MZIj_cvvtTiAfNf_0lpfTg</a><br><a href="https://mp.weixin.qq.com/s/oKxsWDTvoLeOSHAuPIxnGw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/oKxsWDTvoLeOSHAuPIxnGw</a><br><a href="https://blog.csdn.net/ccy19910925/article/details/82424275" target="_blank" rel="noopener">https://blog.csdn.net/ccy19910925/article/details/82424275</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Calico/">Calico</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Network/">Network</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-fluentd_trouble_shooting_001" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/01/15/fluentd_trouble_shooting_001/" class="article-date">
  	<time datetime="2019-01-15T02:32:04.727Z" itemprop="datePublished">2019-01-15</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/15/fluentd_trouble_shooting_001/">
        Fluentd不使用运行时的时区向Elasticsearch写入日志记录，怎么解决？
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>一、环境版本信息<br>Fluentd 1.1.0<br>Elasticsearch 6.3.0<br>fluent-plugin-elasticsearch 2.4.1</p>
<p>二、报错现象描述<br>以时区设置为Asia/Shanghai为例：<br>虽然Fluentd所在的运行时已经设置正确，但是上报到Elasticsearch对应索引中的日志记录，与实际相比还是相差八个小时。更直接的现象是，你发现明明应该归档在今天对应的索引中的日志，却归档在了昨天对应的索引中。再有就是你会发现，无论你怎么修改运行时的时区，Fluentd向Elasticsearch写入日志记录都始终使用UTC时区。</p>
<p>三、解决方法<br>把utc_index的值设置为false，允许Fluentd使用本地运行时设置的时区。</p>
<p>官方文档的原文描述如下：（详见参考资料部分的链接）<br>By default, the records inserted into index logstash-YYMMDD with UTC (Coordinated Universal Time). This option allows to use local time if you describe utc_index to false.</p>
<p>配置修改示例如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&lt;match **&gt;</span><br><span class="line">  @id elasticsearch</span><br><span class="line">  @type elasticsearch</span><br><span class="line">  @log_level info</span><br><span class="line">  include_tag_key true</span><br><span class="line">  host elasticsearch-ingest.logging</span><br><span class="line">  port 9200</span><br><span class="line">  logstash_format true</span><br><span class="line">  utc_index false</span><br><span class="line">  slow_flush_log_threshold 60.0</span><br><span class="line">  &lt;buffer&gt;</span><br><span class="line">    @type file</span><br><span class="line">    path /var/log/fluentd-buffers/kubernetes.system.buffer</span><br><span class="line">    flush_mode interval</span><br><span class="line">    retry_type exponential_backoff</span><br><span class="line">    flush_thread_count 2</span><br><span class="line">    flush_interval 5s</span><br><span class="line">    retry_forever</span><br><span class="line">    retry_max_interval 30</span><br><span class="line">    chunk_limit_size 6M</span><br><span class="line">    queue_limit_length 256</span><br><span class="line">    overflow_action block</span><br><span class="line">  &lt;/buffer&gt;</span><br><span class="line">&lt;/match&gt;</span><br></pre></td></tr></table></figure></p>
<p>四、参考资料<br><a href="https://github.com/uken/fluent-plugin-elasticsearch/tree/v2.4.1#utc_index" target="_blank" rel="noopener">https://github.com/uken/fluent-plugin-elasticsearch/tree/v2.4.1#utc_index</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Fluentd/">Fluentd</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Trouble-Shooting/">Trouble Shooting</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-fluentd_trouble_shooting_000" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/01/15/fluentd_trouble_shooting_000/" class="article-date">
  	<time datetime="2019-01-15T02:09:02.679Z" itemprop="datePublished">2019-01-15</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/15/fluentd_trouble_shooting_000/">
        Fluentd报错批处理索引队列满了，该怎么解决？
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>一、环境版本信息<br>Fluentd 1.1.0<br>Elasticsearch 6.3.0</p>
<p>二、报错信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">2018-12-20 03:50:41 +0000 [info]: [elasticsearch] Connection opened to Elasticsearch cluster =&gt; &#123;:host=&gt;&quot;elasticsearch.logging&quot;, :port=&gt;9200, :scheme=&gt;&quot;http&quot;&#125;</span><br><span class="line">2018-12-20 20:28:23 +0000 [warn]: [elasticsearch] failed to flush the buffer. retry_time=0 next_retry_seconds=2018-12-20 20:28:24 +0000 chunk=&quot;57d79f767802e630777d6da9f12a1fd7&quot; error_class=Fluent::Plugin::ElasticsearchErrorHandler::BulkIndexQueueFull error=&quot;Bulk index queue is full, retrying&quot;</span><br><span class="line">  2018-12-20 20:28:23 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluent-plugin-elasticsearch-2.4.1/lib/fluent/plugin/elasticsearch_error_handler.rb:90:in `block in handle_error&apos;</span><br><span class="line">  2018-12-20 20:28:23 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluent-plugin-elasticsearch-2.4.1/lib/fluent/plugin/elasticsearch_error_handler.rb:85:in `each_key&apos;</span><br><span class="line">  2018-12-20 20:28:23 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluent-plugin-elasticsearch-2.4.1/lib/fluent/plugin/elasticsearch_error_handler.rb:85:in `handle_error&apos;</span><br><span class="line">  2018-12-20 20:28:23 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluent-plugin-elasticsearch-2.4.1/lib/fluent/plugin/out_elasticsearch.rb:434:in `send_bulk&apos;</span><br><span class="line">  2018-12-20 20:28:23 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluent-plugin-elasticsearch-2.4.1/lib/fluent/plugin/out_elasticsearch.rb:418:in `write&apos;</span><br><span class="line">  2018-12-20 20:28:23 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin/output.rb:1094:in `try_flush&apos;</span><br><span class="line">  2018-12-20 20:28:23 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin/output.rb:1319:in `flush_thread_run&apos;</span><br><span class="line">  2018-12-20 20:28:23 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin/output.rb:439:in `block (2 levels) in start&apos;</span><br><span class="line">  2018-12-20 20:28:23 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin_helper/thread.rb:78:in `block in thread_create&apos;</span><br><span class="line">2018-12-20 20:28:24 +0000 [warn]: [elasticsearch] failed to flush the buffer. retry_time=0 next_retry_seconds=2018-12-20 20:28:24 +0000 chunk=&quot;57d79f767802e630777d6da9f12a1fd7&quot; error_class=Fluent::Plugin::ElasticsearchErrorHandler::BulkIndexQueueFull error=&quot;Bulk index queue is full, retrying&quot;</span><br><span class="line">  2018-12-20 20:28:24 +0000 [warn]: suppressed same stacktrace</span><br><span class="line">2018-12-20 20:28:25 +0000 [warn]: [elasticsearch] failed to flush the buffer. retry_time=1 next_retry_seconds=2018-12-20 20:28:25 +0000 chunk=&quot;57d79f767802e630777d6da9f12a1fd7&quot; error_class=Fluent::Plugin::ElasticsearchErrorHandler::BulkIndexQueueFull error=&quot;Bulk index queue is full, retrying&quot;</span><br><span class="line">  2018-12-20 20:28:25 +0000 [warn]: suppressed same stacktrace</span><br><span class="line">2018-12-20 20:28:26 +0000 [warn]: [elasticsearch] failed to flush the buffer. retry_time=2 next_retry_seconds=2018-12-20 20:28:27 +0000 chunk=&quot;57d79f78b9fce084be8b1f544a046a09&quot; error_class=Fluent::Plugin::ElasticsearchErrorHandler::BulkIndexQueueFull error=&quot;Bulk index queue is full, retrying&quot;</span><br><span class="line">  2018-12-20 20:28:26 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluent-plugin-elasticsearch-2.4.1/lib/fluent/plugin/elasticsearch_error_handler.rb:90:in `block in handle_error&apos;</span><br><span class="line">  2018-12-20 20:28:26 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluent-plugin-elasticsearch-2.4.1/lib/fluent/plugin/elasticsearch_error_handler.rb:85:in `each_key&apos;</span><br><span class="line">  2018-12-20 20:28:26 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluent-plugin-elasticsearch-2.4.1/lib/fluent/plugin/elasticsearch_error_handler.rb:85:in `handle_error&apos;</span><br><span class="line">  2018-12-20 20:28:26 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluent-plugin-elasticsearch-2.4.1/lib/fluent/plugin/out_elasticsearch.rb:434:in `send_bulk&apos;</span><br><span class="line">  2018-12-20 20:28:26 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluent-plugin-elasticsearch-2.4.1/lib/fluent/plugin/out_elasticsearch.rb:418:in `write&apos;</span><br><span class="line">  2018-12-20 20:28:26 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin/output.rb:1094:in `try_flush&apos;</span><br><span class="line">  2018-12-20 20:28:26 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin/output.rb:1319:in `flush_thread_run&apos;</span><br><span class="line">  2018-12-20 20:28:26 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin/output.rb:439:in `block (2 levels) in start&apos;</span><br><span class="line">  2018-12-20 20:28:26 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin_helper/thread.rb:78:in `block in thread_create&apos;</span><br><span class="line">2018-12-20 20:28:30 +0000 [warn]: [elasticsearch] failed to flush the buffer. retry_time=3 next_retry_seconds=2018-12-20 20:28:30 +0000 chunk=&quot;57d79f78b9fce084be8b1f544a046a09&quot; error_class=Fluent::Plugin::ElasticsearchErrorHandler::BulkIndexQueueFull error=&quot;Bulk index queue is full, retrying&quot;</span><br><span class="line">  2018-12-20 20:28:30 +0000 [warn]: suppressed same stacktrace</span><br><span class="line">2018-12-20 20:28:30 +0000 [warn]: [elasticsearch] failed to flush the buffer. retry_time=4 next_retry_seconds=2018-12-20 20:28:39 +0000 chunk=&quot;57d79f767802e630777d6da9f12a1fd7&quot; error_class=Fluent::Plugin::ElasticsearchErrorHandler::BulkIndexQueueFull error=&quot;Bulk index queue is full, retrying&quot;</span><br><span class="line">  2018-12-20 20:28:30 +0000 [warn]: suppressed same stacktrace</span><br><span class="line">2018-12-20 20:28:45 +0000 [warn]: [elasticsearch] failed to flush the buffer. retry_time=5 next_retry_seconds=2018-12-20 20:28:44 +0000 chunk=&quot;57d79f767802e630777d6da9f12a1fd7&quot; error_class=Fluent::Plugin::ElasticsearchErrorHandler::BulkIndexQueueFull error=&quot;Bulk index queue is full, retrying&quot;</span><br><span class="line">  2018-12-20 20:28:45 +0000 [warn]: suppressed same stacktrace</span><br><span class="line">2018-12-20 20:28:45 +0000 [warn]: [elasticsearch] failed to flush the buffer. retry_time=6 next_retry_seconds=2018-12-20 20:29:18 +0000 chunk=&quot;57d79f78b9fce084be8b1f544a046a09&quot; error_class=Fluent::Plugin::ElasticsearchErrorHandler::BulkIndexQueueFull error=&quot;Bulk index queue is full, retrying&quot;</span><br><span class="line">  2018-12-20 20:28:45 +0000 [warn]: suppressed same stacktrace</span><br><span class="line">2018-12-20 20:29:13 +0000 [warn]: [elasticsearch] retry succeeded. chunk_id=&quot;57d79f78b9fce084be8b1f544a046a09&quot;</span><br></pre></td></tr></table></figure></p>
<p>三、解决方法<br>在配置文件elasticsearch.yml中，增加如下配置：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">thread_pool:</span><br><span class="line">  index:</span><br><span class="line">    queue_size: $&#123;INDEX_QUEUE_SIZE:200&#125;</span><br><span class="line">  write:</span><br><span class="line">    queue_size: $&#123;WRITE_QUEUE_SIZE:200&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>在pod、deployment或者statefulset等pod相关的yaml中，增加如下环境变量的配置：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">        env:</span><br><span class="line">        - name: INDEX_QUEUE_SIZE</span><br><span class="line">          value: &quot;1000&quot;</span><br><span class="line">        - name: WRITE_QUEUE_SIZE</span><br><span class="line">          value: &quot;1000&quot;</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>如何查看配置的修改是否生效？即查看Elasticsearch的线程池配置。（注意：不通版本的Elasticsearch查看方式有所不同，详见参考资料中的链接文档）<br>提示：推荐使用 Kibana 的 Dev Tools进行查询，这种方式最为方便。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GET _cat/thread_pool/index?v&amp;h=id,node_name,name,active,queue,rejected,completed,queue_size</span><br><span class="line">GET _cat/thread_pool/write?v&amp;h=id,node_name,name,active,queue,rejected,completed,queue_size</span><br></pre></td></tr></table></figure></p>
<p>四、参考资料<br><a href="https://blog.csdn.net/opensure/article/details/51491815" target="_blank" rel="noopener">https://blog.csdn.net/opensure/article/details/51491815</a><br><a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.4/modules-threadpool.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/2.4/modules-threadpool.html</a><br><a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.3/modules-threadpool.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/6.3/modules-threadpool.html</a><br><a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.3/cat-thread-pool.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/6.3/cat-thread-pool.html</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Fluentd/">Fluentd</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Trouble-Shooting/">Trouble Shooting</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-elasticsearch_resources" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/12/17/elasticsearch_resources/" class="article-date">
  	<time datetime="2018-12-17T09:20:09.967Z" itemprop="datePublished">2018-12-17</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/17/elasticsearch_resources/">
        Elasticsearch 经典资料汇总
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="中文资料"><a href="#中文资料" class="headerlink" title="中文资料"></a>中文资料</h1><p><a href="https://es.xiaoleilu.com" target="_blank" rel="noopener">https://es.xiaoleilu.com</a><br><a href="https://github.com/elasticsearch-cn/elasticsearch-definitive-guide" target="_blank" rel="noopener">https://github.com/elasticsearch-cn/elasticsearch-definitive-guide</a><br><a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/index.html" target="_blank" rel="noopener">https://www.elastic.co/guide/cn/elasticsearch/guide/current/index.html</a></p>
<h1 id="Elasticsearch-Client-Golang-SDK"><a href="#Elasticsearch-Client-Golang-SDK" class="headerlink" title="Elasticsearch Client Golang SDK"></a>Elasticsearch Client Golang SDK</h1><p><a href="https://github.com/olivere/elastic/tree/v6.2.11" target="_blank" rel="noopener">https://github.com/olivere/elastic/tree/v6.2.11</a></p>
<h1 id="深度分页问题"><a href="#深度分页问题" class="headerlink" title="深度分页问题"></a>深度分页问题</h1><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.3/search-request-search-after.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/6.3/search-request-search-after.html</a><br><a href="https://blog.csdn.net/u011228889/article/details/79760167" target="_blank" rel="noopener">https://blog.csdn.net/u011228889/article/details/79760167</a><br><a href="https://blog.csdn.net/WangPing1223/article/details/79148244" target="_blank" rel="noopener">https://blog.csdn.net/WangPing1223/article/details/79148244</a></p>
<h1 id="Kubernetes上非常好用的Elasticsearch"><a href="#Kubernetes上非常好用的Elasticsearch" class="headerlink" title="Kubernetes上非常好用的Elasticsearch"></a>Kubernetes上非常好用的Elasticsearch</h1><p><a href="https://github.com/pires/kubernetes-elasticsearch-cluster/tree/6.3.0" target="_blank" rel="noopener">https://github.com/pires/kubernetes-elasticsearch-cluster/tree/6.3.0</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Elasticsearch/">Elasticsearch</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Logging/">Logging</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-prometheus_resources" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/12/17/prometheus_resources/" class="article-date">
  	<time datetime="2018-12-17T08:52:43.737Z" itemprop="datePublished">2018-12-17</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/17/prometheus_resources/">
        Prometheus 经典资料汇总
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="官方文档"><a href="#官方文档" class="headerlink" title="官方文档"></a>官方文档</h1><p><a href="https://prometheus.io/docs/prometheus/2.2/getting_started/" target="_blank" rel="noopener">https://prometheus.io/docs/prometheus/2.2/getting_started/</a></p>
<h1 id="Prometheus-Operator-开源项目地址"><a href="#Prometheus-Operator-开源项目地址" class="headerlink" title="Prometheus Operator 开源项目地址"></a>Prometheus Operator 开源项目地址</h1><p><a href="https://github.com/coreos/prometheus-operator/tree/v0.25.0" target="_blank" rel="noopener">https://github.com/coreos/prometheus-operator/tree/v0.25.0</a></p>
<h1 id="经典中文资料"><a href="#经典中文资料" class="headerlink" title="经典中文资料"></a>经典中文资料</h1><p><a href="https://yunlzheng.gitbook.io/prometheus-book/introduction" target="_blank" rel="noopener">https://yunlzheng.gitbook.io/prometheus-book/introduction</a></p>
<h1 id="使用Golang实现Prometheus-Exporter"><a href="#使用Golang实现Prometheus-Exporter" class="headerlink" title="使用Golang实现Prometheus Exporter"></a>使用Golang实现Prometheus Exporter</h1><p><a href="https://blog.csdn.net/u014029783/article/details/80001251" target="_blank" rel="noopener">https://blog.csdn.net/u014029783/article/details/80001251</a></p>
<h1 id="解惑Prometheus-API查询的时间戳格式"><a href="#解惑Prometheus-API查询的时间戳格式" class="headerlink" title="解惑Prometheus API查询的时间戳格式"></a>解惑Prometheus API查询的时间戳格式</h1><p><a href="https://www.crifan.com/timestamp_format_support_decimal_point_or_not/" target="_blank" rel="noopener">https://www.crifan.com/timestamp_format_support_decimal_point_or_not/</a></p>
<h1 id="Golang实现四舍五入（监控指标的格式化经常用到）"><a href="#Golang实现四舍五入（监控指标的格式化经常用到）" class="headerlink" title="Golang实现四舍五入（监控指标的格式化经常用到）"></a>Golang实现四舍五入（监控指标的格式化经常用到）</h1><p><a href="https://www.jianshu.com/p/ca52f4f58353" target="_blank" rel="noopener">https://www.jianshu.com/p/ca52f4f58353</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Monitoring/">Monitoring</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Prometheus/">Prometheus</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-nginx_ingress" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/12/14/nginx_ingress/" class="article-date">
  	<time datetime="2018-12-14T12:52:16.597Z" itemprop="datePublished">2018-12-14</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/14/nginx_ingress/">
        Nginx Ingress实验小记
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="克隆源代码，切换到当前最新的稳定版本"><a href="#克隆源代码，切换到当前最新的稳定版本" class="headerlink" title="克隆源代码，切换到当前最新的稳定版本"></a>克隆源代码，切换到当前最新的稳定版本</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/kubernetes/ingress-nginx.git</span><br><span class="line">cd ingress-nginx/</span><br><span class="line">git checkout -b nginx-0.21.0 nginx-0.21.0</span><br></pre></td></tr></table></figure>
<h1 id="创建Ingress代理的后端服务"><a href="#创建Ingress代理的后端服务" class="headerlink" title="创建Ingress代理的后端服务"></a>创建Ingress代理的后端服务</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p deploy/example</span><br><span class="line">vi deploy/example/nginx.yaml</span><br></pre></td></tr></table></figure>
<p>添加如下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.15.4</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">---</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br></pre></td></tr></table></figure></p>
<h1 id="Nginx-Ingress-Controller扮演四层负载均衡器，验证四层负载均衡"><a href="#Nginx-Ingress-Controller扮演四层负载均衡器，验证四层负载均衡" class="headerlink" title="Nginx Ingress Controller扮演四层负载均衡器，验证四层负载均衡"></a>Nginx Ingress Controller扮演四层负载均衡器，验证四层负载均衡</h1><p>在ConfigMap-[tcp-services]的data部分添加要暴露的端口到service和对应端口的映射关系，详见下面的示例：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-configuration</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: tcp-services</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">data:</span><br><span class="line">  8080: &quot;default/nginx:80&quot; # 添加要暴露的端口到service和对应端口的映射关系</span><br><span class="line">---</span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: udp-services</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br></pre></td></tr></table></figure></p>
<p>编辑deploy/with-rbac.yaml，把Pod模板中网络模式改为hostNetwork。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-controller</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app.kubernetes.io/name: ingress-nginx</span><br><span class="line">      app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app.kubernetes.io/name: ingress-nginx</span><br><span class="line">        app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">      annotations:</span><br><span class="line">        prometheus.io/port: &quot;10254&quot;</span><br><span class="line">        prometheus.io/scrape: &quot;true&quot;</span><br><span class="line">    spec:</span><br><span class="line">      hostNetwork: true # 注意这里设置为hostNetwork</span><br><span class="line">      serviceAccountName: nginx-ingress-serviceaccount</span><br><span class="line">      containers:</span><br><span class="line">        - name: nginx-ingress-controller</span><br><span class="line">          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0</span><br><span class="line">          args:</span><br><span class="line">            - /nginx-ingress-controller</span><br><span class="line">            - --configmap=$(POD_NAMESPACE)/nginx-configuration</span><br><span class="line">            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services</span><br><span class="line">            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services</span><br><span class="line">            - --publish-service=$(POD_NAMESPACE)/ingress-nginx</span><br><span class="line">            - --annotations-prefix=nginx.ingress.kubernetes.io</span><br><span class="line">            # - --http-port=8080 # 监听的http端口，默认80</span><br><span class="line">            # - --https-port=8443 # 监听的https端口，默认443</span><br><span class="line">          securityContext:</span><br><span class="line">            capabilities:</span><br><span class="line">              drop:</span><br><span class="line">                - ALL</span><br><span class="line">              add:</span><br><span class="line">                - NET_BIND_SERVICE</span><br><span class="line">            # www-data -&gt; 33</span><br><span class="line">            runAsUser: 33</span><br><span class="line">          env:</span><br><span class="line">            - name: POD_NAME</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.name</span><br><span class="line">            - name: POD_NAMESPACE</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.namespace</span><br><span class="line">          ports:</span><br><span class="line">            - name: http</span><br><span class="line">              containerPort: 80</span><br><span class="line">            - name: https</span><br><span class="line">              containerPort: 443</span><br><span class="line">          livenessProbe:</span><br><span class="line">            failureThreshold: 3</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /healthz</span><br><span class="line">              port: 10254</span><br><span class="line">              scheme: HTTP</span><br><span class="line">            initialDelaySeconds: 10</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            successThreshold: 1</span><br><span class="line">            timeoutSeconds: 1</span><br><span class="line">          readinessProbe:</span><br><span class="line">            failureThreshold: 3</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /healthz</span><br><span class="line">              port: 10254</span><br><span class="line">              scheme: HTTP</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            successThreshold: 1</span><br><span class="line">            timeoutSeconds: 1</span><br><span class="line"></span><br><span class="line">---</span><br></pre></td></tr></table></figure></p>
<h1 id="在Kubernetes中执行如下操作："><a href="#在Kubernetes中执行如下操作：" class="headerlink" title="在Kubernetes中执行如下操作："></a>在Kubernetes中执行如下操作：</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f deploy/namespace.yaml</span><br><span class="line">kubectl create -f deploy/configmap.yaml</span><br><span class="line">kubectl create -f deploy/rbac.yaml</span><br><span class="line">kubectl create -f deploy/with-rbac.yaml</span><br><span class="line">kubectl create -f deploy/provider/baremetal/service-nodeport.yaml</span><br></pre></td></tr></table></figure>
<h1 id="查看Nginx-Ingress-Controller所在的Node信息"><a href="#查看Nginx-Ingress-Controller所在的Node信息" class="headerlink" title="查看Nginx Ingress Controller所在的Node信息"></a>查看Nginx Ingress Controller所在的Node信息</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get pod -n ingress-nginx -o wide</span><br><span class="line">NAME                                        READY     STATUS    RESTARTS   AGE       IP               NODE</span><br><span class="line">nginx-ingress-controller-7799468ccb-5bzgv   1/1       Running   0          2m        172.16.170.129   server02</span><br></pre></td></tr></table></figure>
<h1 id="浏览器访问，四层负载均衡的效果如下图所示："><a href="#浏览器访问，四层负载均衡的效果如下图所示：" class="headerlink" title="浏览器访问，四层负载均衡的效果如下图所示："></a>浏览器访问，四层负载均衡的效果如下图所示：</h1><p><img src="/2018/12/14/nginx_ingress/ingress_4.png" alt="ingress_4"></p>
<h1 id="Nginx-Ingress-Controller扮演七层负载均衡器，验证七层负载均衡"><a href="#Nginx-Ingress-Controller扮演七层负载均衡器，验证七层负载均衡" class="headerlink" title="Nginx Ingress Controller扮演七层负载均衡器，验证七层负载均衡"></a>Nginx Ingress Controller扮演七层负载均衡器，验证七层负载均衡</h1><p>修改deploy/with-rbac.yaml，这里使用hostPort暴露http端口和https端口，不再使用hostNetwork。因为hostNetwork使用的是Node的网络栈，包括DNS服务器和解析。对于七层负载均衡器，暴露http和https的默认端口即可，可以通过不同的路径映射到不同的service上。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-controller</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app.kubernetes.io/name: ingress-nginx</span><br><span class="line">      app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app.kubernetes.io/name: ingress-nginx</span><br><span class="line">        app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">      annotations:</span><br><span class="line">        prometheus.io/port: &quot;10254&quot;</span><br><span class="line">        prometheus.io/scrape: &quot;true&quot;</span><br><span class="line">    spec:</span><br><span class="line">      # hostNetwork: true</span><br><span class="line">      serviceAccountName: nginx-ingress-serviceaccount</span><br><span class="line">      containers:</span><br><span class="line">        - name: nginx-ingress-controller</span><br><span class="line">          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0</span><br><span class="line">          args:</span><br><span class="line">            - /nginx-ingress-controller</span><br><span class="line">            - --configmap=$(POD_NAMESPACE)/nginx-configuration</span><br><span class="line">            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services</span><br><span class="line">            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services</span><br><span class="line">            - --publish-service=$(POD_NAMESPACE)/ingress-nginx</span><br><span class="line">            - --annotations-prefix=nginx.ingress.kubernetes.io</span><br><span class="line">            # - --http-port=8080 # 监听的http端口，默认80</span><br><span class="line">            # - --https-port=8443 # 监听的https端口，默认443</span><br><span class="line">          securityContext:</span><br><span class="line">            capabilities:</span><br><span class="line">              drop:</span><br><span class="line">                - ALL</span><br><span class="line">              add:</span><br><span class="line">                - NET_BIND_SERVICE</span><br><span class="line">            # www-data -&gt; 33</span><br><span class="line">            runAsUser: 33</span><br><span class="line">          env:</span><br><span class="line">            - name: POD_NAME</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.name</span><br><span class="line">            - name: POD_NAMESPACE</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.namespace</span><br><span class="line">          ports:</span><br><span class="line">            - name: http</span><br><span class="line">              containerPort: 80</span><br><span class="line">              hostPort: 80 # 使用hostPort暴露http端口</span><br><span class="line">            - name: https</span><br><span class="line">              containerPort: 443</span><br><span class="line">              hostPort: 443 # 使用hostPort暴露https端口</span><br><span class="line">          livenessProbe:</span><br><span class="line">            failureThreshold: 3</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /healthz</span><br><span class="line">              port: 10254</span><br><span class="line">              scheme: HTTP</span><br><span class="line">            initialDelaySeconds: 10</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            successThreshold: 1</span><br><span class="line">            timeoutSeconds: 1</span><br><span class="line">          readinessProbe:</span><br><span class="line">            failureThreshold: 3</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /healthz</span><br><span class="line">              port: 10254</span><br><span class="line">              scheme: HTTP</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            successThreshold: 1</span><br><span class="line">            timeoutSeconds: 1</span><br><span class="line"></span><br><span class="line">---</span><br></pre></td></tr></table></figure></p>
<p>创建deploy/example/ingress.yaml，添加如下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-app</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: foo.bar.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: nginx</span><br><span class="line">          servicePort: 80</span><br></pre></td></tr></table></figure></p>
<p>在Kubernetes集群中创建Ingress资源对象：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f eploy/example/ingress.yaml</span><br></pre></td></tr></table></figure></p>
<h1 id="浏览器访问，七层负载均衡的效果如下图所示："><a href="#浏览器访问，七层负载均衡的效果如下图所示：" class="headerlink" title="浏览器访问，七层负载均衡的效果如下图所示："></a>浏览器访问，七层负载均衡的效果如下图所示：</h1><p><img src="/2018/12/14/nginx_ingress/ingress_7.png" alt="ingress_7"></p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://github.com/kubernetes/ingress-nginx/tree/nginx-0.21.0" target="_blank" rel="noopener">https://github.com/kubernetes/ingress-nginx/tree/nginx-0.21.0</a><br><a href="https://kubernetes.github.io/ingress-nginx/" target="_blank" rel="noopener">https://kubernetes.github.io/ingress-nginx/</a><br><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/services-networking/ingress/</a><br><a href="https://www.cnblogs.com/iiiiher/p/8006801.html" target="_blank" rel="noopener">https://www.cnblogs.com/iiiiher/p/8006801.html</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Ingress/">Ingress</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Network/">Network</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kubeadm_ceph_rbd" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/12/10/kubeadm_ceph_rbd/" class="article-date">
  	<time datetime="2018-12-10T07:28:44.859Z" itemprop="datePublished">2018-12-10</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/10/kubeadm_ceph_rbd/">
        Kubernetes对接Ceph RBD关键点记录
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Ceph-RBD-External-Provisioner版本"><a href="#Ceph-RBD-External-Provisioner版本" class="headerlink" title="Ceph RBD External Provisioner版本"></a>Ceph RBD External Provisioner版本</h1><p><a href="https://github.com/kubernetes-incubator/external-storage/blob/v5.1.0/ceph/rbd/" target="_blank" rel="noopener">https://github.com/kubernetes-incubator/external-storage/blob/v5.1.0/ceph/rbd/</a></p>
<h1 id="对Linux操作系统的内核版本有要求，经初步测试需要版本为4-x"><a href="#对Linux操作系统的内核版本有要求，经初步测试需要版本为4-x" class="headerlink" title="对Linux操作系统的内核版本有要求，经初步测试需要版本为4.x"></a>对Linux操作系统的内核版本有要求，经初步测试需要版本为4.x</h1><p>CentOS 7.5.1804如何升级操作系统内核？<br><a href="https://www.howtoforge.com/tutorial/how-to-upgrade-kernel-in-centos-7-server/" target="_blank" rel="noopener">https://www.howtoforge.com/tutorial/how-to-upgrade-kernel-in-centos-7-server/</a><br><a href="https://zhuanlan.zhihu.com/p/29617407" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29617407</a></p>
<h1 id="Kubernetes各个Node上需要安装匹配版本的ceph-common"><a href="#Kubernetes各个Node上需要安装匹配版本的ceph-common" class="headerlink" title="Kubernetes各个Node上需要安装匹配版本的ceph-common"></a>Kubernetes各个Node上需要安装匹配版本的ceph-common</h1><p>这里以宿主机操作系统为CentOS 7.5.1804，ceph版本为mimic为例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rpm -Uvh https://download.ceph.com/rpm-mimic/el7/noarch/ceph-release-1-1.el7.noarch.rpm</span><br><span class="line">yum install -y --nogpgcheck ceph-common</span><br></pre></td></tr></table></figure></p>
<p>参考资料：<br><a href="https://github.com/kubernetes-incubator/external-storage/blob/v5.1.0/ceph/rbd/Dockerfile" target="_blank" rel="noopener">https://github.com/kubernetes-incubator/external-storage/blob/v5.1.0/ceph/rbd/Dockerfile</a></p>
<h1 id="StorageClass和Ceph-RBD-External-Provisioner的正常工作依赖于Kubernetes内部的DNS解析"><a href="#StorageClass和Ceph-RBD-External-Provisioner的正常工作依赖于Kubernetes内部的DNS解析" class="headerlink" title="StorageClass和Ceph RBD External Provisioner的正常工作依赖于Kubernetes内部的DNS解析"></a>StorageClass和Ceph RBD External Provisioner的正常工作依赖于Kubernetes内部的DNS解析</h1><ol>
<li><p>配置DNS记录，让Kubernetes集群内部可以把ceph monitor解析到对应的外部地址上；</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: ceph-mon</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  type: ExternalName</span><br><span class="line">  externalName: 172.16.170.134.xip.io # ceph monitor的地址</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置StorageClass，让monitors参数使用1中配置的DNS记录。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">kind: StorageClass</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: managed-cephrbd-storage</span><br><span class="line">provisioner: yonghui.cn/cephrbd</span><br><span class="line">parameters:</span><br><span class="line">  monitors: ceph-mon.kube-system.svc.cluster.local:6789 # 这里需要使用Kubernetes内部的DNS配置ceph monitor的地址</span><br><span class="line">  pool: kube</span><br><span class="line">  adminId: admin</span><br><span class="line">  adminSecretNamespace: kube-system</span><br><span class="line">  adminSecretName: cephrbd-admin-secret</span><br><span class="line">  userId: kube</span><br><span class="line">  userSecretNamespace: kube-system</span><br><span class="line">  userSecretName: cephrbd-secret</span><br><span class="line">  imageFormat: &quot;2&quot;</span><br><span class="line">  imageFeatures: layering</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>推荐参考资料：<br><a href="http://blog.51cto.com/ygqygq2/2163656" target="_blank" rel="noopener">http://blog.51cto.com/ygqygq2/2163656</a><br><a href="https://segmentfault.com/q/1010000011440882" target="_blank" rel="noopener">https://segmentfault.com/q/1010000011440882</a></p>
<h1 id="Ceph-RBD的简单操作记录"><a href="#Ceph-RBD的简单操作记录" class="headerlink" title="Ceph RBD的简单操作记录"></a>Ceph RBD的简单操作记录</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># rbd ls -p kube</span><br><span class="line">kubernetes-dynamic-pvc-4392eab0-fc4c-11e8-a9f7-ee2ca7a031f1</span><br><span class="line"># rbd rm kube/kubernetes-dynamic-pvc-4392eab0-fc4c-11e8-a9f7-ee2ca7a031f1</span><br><span class="line">Removing image: 100% complete...done.</span><br><span class="line"># rbd ls -p kube</span><br></pre></td></tr></table></figure>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Ceph/">Ceph</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Storage/">Storage</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-common-label" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/12/10/common-label/" class="article-date">
  	<time datetime="2018-12-10T03:20:49.595Z" itemprop="datePublished">2018-12-10</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/10/common-label/">
        容器标签使用小记
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="docker-cli使用举例"><a href="#docker-cli使用举例" class="headerlink" title="docker cli使用举例"></a>docker cli使用举例</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker ps -a --filter &quot;label=org.hyperledger.fabric.chaincode.id.name=test-chaincode&quot; --filter &quot;label=org.hyperledger.fabric.chaincode.id.version=v0.0.1&quot;</span><br><span class="line"></span><br><span class="line">或者</span><br><span class="line"></span><br><span class="line">docker ps -a -f &quot;label=org.hyperledger.fabric.chaincode.id.name=test-chaincode&quot; -f &quot;label=org.hyperledger.fabric.chaincode.id.version=v0.0.1&quot;</span><br></pre></td></tr></table></figure>
<p>如下图所示：</p>
<p><img src="/2018/12/10/common-label/docker_label.png" alt="docker_label"></p>
<h1 id="docker-client-go编码示例"><a href="#docker-client-go编码示例" class="headerlink" title="docker client go编码示例"></a>docker client go编码示例</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">import (</span><br><span class="line">	&quot;github.com/docker/docker/api/types&quot;</span><br><span class="line"></span><br><span class="line">	&quot;github.com/docker/docker/api/types/filters&quot;</span><br><span class="line">)</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">		flagArgs := []string&#123;</span><br><span class="line">			fmt.Sprintf(&quot;label=%s=%s&quot;, ChainCodeNameLabelKey, contractName), // &quot;label=org.hyperledger.fabric.chaincode.id.name=wx-hy001&quot;</span><br><span class="line">			fmt.Sprintf(&quot;label=%s=%s&quot;, ChainCodeVersionLabelKey, contractVersion), // &quot;label=org.hyperledger.fabric.chaincode.id.version=1.2.0&quot;</span><br><span class="line">		&#125;</span><br><span class="line">		args := filters.NewArgs()</span><br><span class="line">		for i := range flagArgs &#123;</span><br><span class="line">			args, err = filters.ParseFlag(flagArgs[i], args)</span><br><span class="line">			if err != nil &#123;</span><br><span class="line">				glog.Error(err)</span><br><span class="line">				return contractChainCodes, err</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		containerList, err := cli.ContainerList(types.ContainerListOptions&#123;</span><br><span class="line">			Filters: args,</span><br><span class="line">		&#125;)</span><br><span class="line">		if err != nil &#123;</span><br><span class="line">			glog.Error(err)</span><br><span class="line">			return contractChainCodes, err</span><br><span class="line">		&#125;</span><br><span class="line">		for j := 0; j &lt; len(containerList); j++ &#123;</span><br><span class="line">			container := containerList[j]</span><br><span class="line">			containerId := container.ID</span><br><span class="line">			containerName := container.Names[0][1 : ]</span><br><span class="line">			contractChainCode := module.ContractChainCode&#123;</span><br><span class="line">				ContainerId: containerId,</span><br><span class="line">				ContainerName: containerName,</span><br><span class="line">				ContainerHost: hostIP,</span><br><span class="line">			&#125;</span><br><span class="line">			contractChainCodes = append(contractChainCodes, contractChainCode)</span><br><span class="line">		&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>如下图所示：</p>
<p><img src="/2018/12/10/common-label/docker_label_go.png" alt="docker_label"></p>
<h1 id="kubernetes-cli使用举例"><a href="#kubernetes-cli使用举例" class="headerlink" title="kubernetes cli使用举例"></a>kubernetes cli使用举例</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod --all-namespaces -o wide -l baas.yonghui.cn/network-name=test-network</span><br><span class="line"></span><br><span class="line">或者</span><br><span class="line"></span><br><span class="line">kubectl get pod --all-namespaces -o wide -l baas.yonghui.cn/network-name=test-network,baas.yonghui.cn/network-component=peer</span><br></pre></td></tr></table></figure>
<p>如下图所示：</p>
<p><img src="/2018/12/10/common-label/kubernetes_label.png" alt="kubernetes_label"></p>
<h1 id="kubernetes-client-go编码示例"><a href="#kubernetes-client-go编码示例" class="headerlink" title="kubernetes client go编码示例"></a>kubernetes client go编码示例</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">import (</span><br><span class="line">	module &quot;yonghui.cn/blockchain/yhbkas/module/network&quot;</span><br><span class="line">	pkglabels &quot;k8s.io/apimachinery/pkg/labels&quot;</span><br><span class="line">)</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">	networkNameKey := string(sc.NetworkNameKey)</span><br><span class="line">	labels := map[string]string&#123;</span><br><span class="line">		networkNameKey: networkName,</span><br><span class="line">	&#125;</span><br><span class="line">	selector := pkglabels.FormatLabels(labels)</span><br><span class="line">	opts := metav1.ListOptions&#123;</span><br><span class="line">		LabelSelector: selector,</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	podList, err := clientSet.CoreV1().Pods(metav1.NamespaceAll).List(opts)</span><br><span class="line">	if err != nil &#123;</span><br><span class="line">		return module.GetNetworkComponentsResponse&#123;</span><br><span class="line">			CommonResponse: module.CommonResponse&#123;</span><br><span class="line">				Status:  http.StatusInternalServerError,</span><br><span class="line">				Message: &quot;GetNetworkComponents unsuccessfully.&quot;,</span><br><span class="line">			&#125;,</span><br><span class="line">			NetworkComponents: networkComponents,</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	for i := 0; i &lt; len(podList.Items); i++  &#123;</span><br><span class="line">		pod := podList.Items[i]</span><br><span class="line">		podContainers := make([]module.PodContainer, 0)</span><br><span class="line">		containers := pod.Spec.Containers</span><br><span class="line">		for j := 0; j &lt; len(containers); j++ &#123;</span><br><span class="line">			container := containers[j]</span><br><span class="line">			podContainer := module.PodContainer&#123;</span><br><span class="line">				Name: container.Name,</span><br><span class="line">			&#125;</span><br><span class="line">			podContainers = append(podContainers, podContainer)</span><br><span class="line">		&#125;</span><br><span class="line">		labels := pod.Labels</span><br><span class="line">		componentTypeKey := string(sc.NetworkComponentTypeKey)</span><br><span class="line">		componentTypeValue := labels[componentTypeKey]</span><br><span class="line">		networkComponent := module.NetworkComponent&#123;</span><br><span class="line">			Id: string(pod.UID),</span><br><span class="line">			Name: pod.Name,</span><br><span class="line">			Namespace: pod.Namespace,</span><br><span class="line">			Containers: podContainers,</span><br><span class="line">			Role: componentTypeValue,</span><br><span class="line">		&#125;</span><br><span class="line">		networkComponents = append(networkComponents, networkComponent)</span><br><span class="line">	&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>如下图所示：</p>
<p><img src="/2018/12/10/common-label/kubernetes_label_go.png" alt="kubernetes_label"></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Common/">Common</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Label/">Label</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2019 Singh Wang
      </div>
        <div class="footer-right">
          <a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/smackgg/hexo-theme-smackdown" target="_blank">Smackdown</a>
        </div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="/js/main.js"></script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js"></script>


  </div>
</body>
</html>