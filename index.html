<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  <title>Singh Wang</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Singh Wang">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Singh Wang">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Singh Wang">
  
    <link rel="alternative" href="/atom.xml" title="Singh Wang" type="application/atom+xml">
  
  
    <link rel="icon" href="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg">
  
  <link rel="stylesheet" href="/css/style.css">
  
  

  <script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>
  <script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>

  
</head></html>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="https://avatars0.githubusercontent.com/u/17465385?s=400&amp;u=cea56ed1710ec6ebab160c1ce1e068317e05b025&amp;v=4" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Singh Wang</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/archives">所有文章</a></li>
				        
							<li><a href="/categories/容器云技术/">容器云技术</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/singhwang" title="github">github</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/Calico/" style="font-size: 11.43px;">Calico</a> <a href="/tags/Ceph/" style="font-size: 10px;">Ceph</a> <a href="/tags/Common/" style="font-size: 10px;">Common</a> <a href="/tags/DNS/" style="font-size: 10px;">DNS</a> <a href="/tags/Docker/" style="font-size: 20px;">Docker</a> <a href="/tags/Dragonfly/" style="font-size: 10px;">Dragonfly</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Fluentd/" style="font-size: 11.43px;">Fluentd</a> <a href="/tags/GlusterFS/" style="font-size: 14.29px;">GlusterFS</a> <a href="/tags/HAProxy/" style="font-size: 10px;">HAProxy</a> <a href="/tags/Highly-Available/" style="font-size: 10px;">Highly Available</a> <a href="/tags/Ingress/" style="font-size: 10px;">Ingress</a> <a href="/tags/Kubeadm/" style="font-size: 11.43px;">Kubeadm</a> <a href="/tags/Kubernetes/" style="font-size: 18.57px;">Kubernetes</a> <a href="/tags/Label/" style="font-size: 10px;">Label</a> <a href="/tags/Logging/" style="font-size: 12.86px;">Logging</a> <a href="/tags/Monitoring/" style="font-size: 10px;">Monitoring</a> <a href="/tags/Network/" style="font-size: 15.71px;">Network</a> <a href="/tags/P2P/" style="font-size: 10px;">P2P</a> <a href="/tags/Prometheus/" style="font-size: 10px;">Prometheus</a> <a href="/tags/Setup/" style="font-size: 12.86px;">Setup</a> <a href="/tags/Source-Build/" style="font-size: 10px;">Source Build</a> <a href="/tags/Storage/" style="font-size: 15.71px;">Storage</a> <a href="/tags/TimeZone/" style="font-size: 10px;">TimeZone</a> <a href="/tags/Trouble-Shooting/" style="font-size: 17.14px;">Trouble Shooting</a>
					</div>
				</section>
				
				
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">Singh Wang</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="https://avatars0.githubusercontent.com/u/17465385?s=400&amp;u=cea56ed1710ec6ebab160c1ce1e068317e05b025&amp;v=4" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author">Singh Wang</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/archives">所有文章</a></li>
		        
					<li><a href="/categories/容器云技术/">容器云技术</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/singhwang" title="github">github</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap">
  
    <article id="post-binary_kubernetes_cluster_000" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/binary_kubernetes_cluster_000/" class="article-date">
  	<time datetime="2019-05-27T03:13:06.060Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/binary_kubernetes_cluster_000/">
        使用二进制文件的方式安装Kubernetes集群（一）
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、实现方式介绍"><a href="#一、实现方式介绍" class="headerlink" title="一、实现方式介绍"></a>一、实现方式介绍</h1><h2 id="1-控制平面的所有组件都采用二进制方式部署，交由systemd统一管理；"><a href="#1-控制平面的所有组件都采用二进制方式部署，交由systemd统一管理；" class="headerlink" title="1. 控制平面的所有组件都采用二进制方式部署，交由systemd统一管理；"></a>1. 控制平面的所有组件都采用二进制方式部署，交由systemd统一管理；</h2><p>控制平面包括：etcd、kube-apiserver、kube-controller-manager和kube-scheduler。</p>
<h2 id="2-网络插件依然采用Addon方式部署，交由kubernetes统一管理。"><a href="#2-网络插件依然采用Addon方式部署，交由kubernetes统一管理。" class="headerlink" title="2. 网络插件依然采用Addon方式部署，交由kubernetes统一管理。"></a>2. 网络插件依然采用Addon方式部署，交由kubernetes统一管理。</h2><p>网络插件包括：calico-node及其相关的组件。</p>
<h1 id="二、实验环境版本信息"><a href="#二、实验环境版本信息" class="headerlink" title="二、实验环境版本信息"></a>二、实验环境版本信息</h1><h2 id="1-操作系统的版本信息"><a href="#1-操作系统的版本信息" class="headerlink" title="1. 操作系统的版本信息"></a>1. 操作系统的版本信息</h2><p>CentOS Linux release 7.6.1810 (Core)</p>
<h2 id="2-各组件的版本信息"><a href="#2-各组件的版本信息" class="headerlink" title="2. 各组件的版本信息"></a>2. 各组件的版本信息</h2><p>etcd v3.2.18<br>kube-apiserver v1.11.0<br>kube-controller-manager v1.11.0<br>kube-scheduler v1.11.0<br>kubectl v1.11.0</p>
<p>docker 17.03.1-ce<br>kubelet v1.11.0<br>calico v3.1.3</p>
<h1 id="三、部署架构"><a href="#三、部署架构" class="headerlink" title="三、部署架构"></a>三、部署架构</h1><h2 id="1-Kubernetes-Master（Control-Plane）"><a href="#1-Kubernetes-Master（Control-Plane）" class="headerlink" title="1. Kubernetes Master（Control Plane）"></a>1. Kubernetes Master（Control Plane）</h2><p>172.16.170.128 master -&gt; etcd kube-apiserver kube-controller-manager kube-scheduler</p>
<h2 id="2-Kubernetes-Node"><a href="#2-Kubernetes-Node" class="headerlink" title="2. Kubernetes Node"></a>2. Kubernetes Node</h2><p>172.16.170.134 node01 -&gt; docker kubelet kube-proxy calico-node<br>172.16.170.135 node02 -&gt; docker kubelet kube-proxy calico-node</p>
<h1 id="四、准备二进制文件与Docker镜像"><a href="#四、准备二进制文件与Docker镜像" class="headerlink" title="四、准备二进制文件与Docker镜像"></a>四、准备二进制文件与Docker镜像</h1><h2 id="1-下载相关的二进制文件压缩包"><a href="#1-下载相关的二进制文件压缩包" class="headerlink" title="1. 下载相关的二进制文件压缩包"></a>1. 下载相关的二进制文件压缩包</h2><p><a href="https://github.com/etcd-io/etcd/releases/download/v3.2.18/etcd-v3.2.18-linux-amd64.tar.gz" target="_blank" rel="noopener">https://github.com/etcd-io/etcd/releases/download/v3.2.18/etcd-v3.2.18-linux-amd64.tar.gz</a><br><a href="https://dl.k8s.io/v1.11.0/kubernetes-server-linux-amd64.tar.gz" target="_blank" rel="noopener">https://dl.k8s.io/v1.11.0/kubernetes-server-linux-amd64.tar.gz</a></p>
<h2 id="2-拉取相关的Docker镜像"><a href="#2-拉取相关的Docker镜像" class="headerlink" title="2. 拉取相关的Docker镜像"></a>2. 拉取相关的Docker镜像</h2><p>calico网络组件的镜像：<br>quay.io/calico/cni:v3.1.3<br>quay.io/calico/node:v3.1.3<br>quay.io/calico/typha:v0.7.4</p>
<p>kube-dns的镜像：<br>registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.1.3</p>
<p>infra容器的镜像：<br>registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1</p>
<h1 id="五、部署过程记录"><a href="#五、部署过程记录" class="headerlink" title="五、部署过程记录"></a>五、部署过程记录</h1><h2 id="1-准备基础环境（Master和Node上都执行）"><a href="#1-准备基础环境（Master和Node上都执行）" class="headerlink" title="1. 准备基础环境（Master和Node上都执行）"></a>1. 准备基础环境（Master和Node上都执行）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"># 更新系统</span><br><span class="line">yum update -y</span><br><span class="line"></span><br><span class="line"># 设置正确的时区和时间</span><br><span class="line">yum install -y ntpdate</span><br><span class="line">timedatectl set-timezone Asia/Shanghai</span><br><span class="line">ntpdate cn.ntp.org.cn</span><br><span class="line"></span><br><span class="line"># 关闭防火墙</span><br><span class="line">systemctl disable firewalld.service</span><br><span class="line">systemctl stop firewalld.service</span><br><span class="line"></span><br><span class="line"># 关闭swap分区</span><br><span class="line">swapoff -a</span><br><span class="line">sed -i &apos;s#/dev/mapper/cl-swap#\# /dev/mapper/cl-swap#&apos; /etc/fstab</span><br><span class="line"></span><br><span class="line"># 关闭selinux</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i &apos;s/SELINUX=enforcing/SELINUX=disabled/&apos; /etc/selinux/config</span><br><span class="line"></span><br><span class="line"># 设置各个节点的主机名</span><br><span class="line">## 172.16.170.128</span><br><span class="line">hostnamectl set-hostname master</span><br><span class="line"></span><br><span class="line">## 172.16.170.129</span><br><span class="line">hostnamectl set-hostname node01</span><br><span class="line"></span><br><span class="line">## 172.16.170.130</span><br><span class="line">hostnamectl set-hostname node02</span><br><span class="line"></span><br><span class="line"># 配置主机名和IP的映射</span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; /etc/hosts</span><br><span class="line"></span><br><span class="line"># For Kubernetes Cluster</span><br><span class="line">172.16.170.128 master</span><br><span class="line">172.16.170.129 node01</span><br><span class="line">172.16.170.130 node02</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="2-安装Docker环境（在所有Node上都执行）"><a href="#2-安装Docker环境（在所有Node上都执行）" class="headerlink" title="2. 安装Docker环境（在所有Node上都执行）"></a>2. 安装Docker环境（在所有Node上都执行）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">yum install -y yum-utils device-mapper-persistent-data lvm2</span><br><span class="line">yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><br><span class="line">yum makecache fast</span><br><span class="line"></span><br><span class="line">yum install -y https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch.rpm</span><br><span class="line"></span><br><span class="line">yum install -y docker-ce-17.03.1.ce-1.el7.centos</span><br><span class="line"></span><br><span class="line">systemctl enable docker.service</span><br><span class="line">systemctl start docker.service</span><br><span class="line">systemctl status docker.service</span><br><span class="line"></span><br><span class="line">docker version</span><br></pre></td></tr></table></figure>
<h2 id="3-复制所有二进制文件到操作系统-usr-bin-目录下"><a href="#3-复制所有二进制文件到操作系统-usr-bin-目录下" class="headerlink" title="3. 复制所有二进制文件到操作系统/usr/bin/目录下"></a>3. 复制所有二进制文件到操作系统/usr/bin/目录下</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 172.16.170.128 （Master上执行）</span><br><span class="line">tar -zxvf etcd-v3.2.18-linux-amd64.tar.gz</span><br><span class="line">tar -zxvf kubernetes-server-linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line">cp etcd-v3.2.18-linux-amd64/etcd /usr/bin/</span><br><span class="line">cp etcd-v3.2.18-linux-amd64/etcdctl /usr/bin/</span><br><span class="line">cp kubernetes/server/bin/kube-apiserver /usr/bin/</span><br><span class="line">cp kubernetes/server/bin/kube-controller-manager /usr/bin/</span><br><span class="line">cp kubernetes/server/bin/kube-scheduler /usr/bin/</span><br><span class="line">cp kubernetes/server/bin/kubectl /usr/bin/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 172.16.170.129 和 172.16.170.130 （Node上执行）</span><br><span class="line">tar -zxvf kubernetes-server-linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line">cp kubernetes/server/bin/kubelet /usr/bin/</span><br><span class="line">cp kubernetes/server/bin/kube-proxy /usr/bin/</span><br></pre></td></tr></table></figure>
<h1 id="4-在Master上生成所有组件的相关证书和配置文件"><a href="#4-在Master上生成所有组件的相关证书和配置文件" class="headerlink" title="4. 在Master上生成所有组件的相关证书和配置文件"></a>4. 在Master上生成所有组件的相关证书和配置文件</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"># 创建证书和配置文件的存放目录</span><br><span class="line">mkdir -p /etc/kubernetes/pki/</span><br><span class="line"></span><br><span class="line"># 生成rsa的公钥和私钥</span><br><span class="line">cd /etc/kubernetes/</span><br><span class="line">openssl genrsa -out sa.key 2048</span><br><span class="line">openssl rsa -in sa.key -pubout -out sa.pub</span><br><span class="line"></span><br><span class="line"># 进入证书目录</span><br><span class="line">cd /etc/kubernetes/pki/</span><br><span class="line"></span><br><span class="line"># 生成根证书</span><br><span class="line">openssl genrsa -out ca.key 2048</span><br><span class="line">openssl req -x509 -new -nodes -key ca.key -subj &quot;/CN=master&quot; -days 5000 -out ca.crt</span><br><span class="line"></span><br><span class="line"># 为kube-apiserver生成相关的证书和配置文件</span><br><span class="line">cat &lt;&lt;EOF &gt; master_ssl.cnf</span><br><span class="line">[req]</span><br><span class="line">req_extensions = v3_req</span><br><span class="line">distinguished_name = req_distinguished_name</span><br><span class="line"></span><br><span class="line">[req_distinguished_name]</span><br><span class="line">[v3_req]</span><br><span class="line">basicConstraints = CA:FALSE</span><br><span class="line">keyUsage = nonRepudiation,digitalSignature,keyEncipherment</span><br><span class="line">subjectAltName = @alt_names</span><br><span class="line">[alt_names]</span><br><span class="line">DNS.1 = kubernetes</span><br><span class="line">DNS.2 = kubernetes.default</span><br><span class="line">DNS.3 = kubernetes.default.svc</span><br><span class="line">DNS.4 = kubernetes.default.svc.cluster.local</span><br><span class="line">DNS.5 = master</span><br><span class="line">IP.1 = 10.96.0.1</span><br><span class="line">IP.2 = 172.16.170.128</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">openssl genrsa -out apiserver.key 2048</span><br><span class="line">openssl req -new -key apiserver.key -subj &quot;/CN=master&quot; -config master_ssl.cnf -out apiserver.csr</span><br><span class="line">openssl x509 -req -in apiserver.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days 5000 -extensions v3_req -extfile master_ssl.cnf -out apiserver.crt</span><br><span class="line"></span><br><span class="line"># 为kube-controller-manager生成相关的证书和配置文件</span><br><span class="line">openssl genrsa -out controller-manager.key 2048</span><br><span class="line">openssl req -new -key controller-manager.key -subj &quot;/CN=master&quot; -out controller-manager.csr</span><br><span class="line">openssl x509 -req -in controller-manager.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out controller-manager.crt -days 5000</span><br><span class="line"></span><br><span class="line">export KUBECONFIG=/etc/kubernetes/controller-manager.conf</span><br><span class="line">kubectl config set-cluster kubernetes --server=https://172.16.170.128:6443 --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true</span><br><span class="line">kubectl config set-credentials system:kube-controller-manager --client-certificate=/etc/kubernetes/pki/controller-manager.crt --client-key=/etc/kubernetes/pki/controller-manager.key --embed-certs=true</span><br><span class="line">kubectl config set-context system:kube-controller-manager@kubernetes --cluster=kubernetes --user=system:kube-controller-manager</span><br><span class="line">kubectl config use-context system:kube-controller-manager@kubernetes</span><br><span class="line">unset KUBECONFIG</span><br><span class="line"></span><br><span class="line"># 为kube-scheduler生成相关的证书和配置文件</span><br><span class="line">openssl genrsa -out scheduler.key 2048</span><br><span class="line">openssl req -new -key scheduler.key -subj &quot;/CN=master&quot; -out scheduler.csr</span><br><span class="line">openssl x509 -req -in scheduler.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out scheduler.crt -days 5000</span><br><span class="line"></span><br><span class="line">export KUBECONFIG=/etc/kubernetes/scheduler.conf</span><br><span class="line">kubectl config set-cluster kubernetes --server=https://172.16.170.128:6443 --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true</span><br><span class="line">kubectl config set-credentials system:kube-scheduler --client-certificate=/etc/kubernetes/pki/scheduler.crt --client-key=/etc/kubernetes/pki/scheduler.key --embed-certs=true</span><br><span class="line">kubectl config set-context system:kube-scheduler@kubernetes --cluster=kubernetes --user=system:kube-scheduler</span><br><span class="line">kubectl config use-context system:kube-scheduler@kubernetes</span><br><span class="line">unset KUBECONFIG</span><br><span class="line"></span><br><span class="line"># 为kubectl生成相关的证书和配置文件</span><br><span class="line">openssl genrsa -out kubectl.key 2048</span><br><span class="line">openssl req -new -key kubectl.key -subj &quot;/CN=master&quot; -out kubectl.csr</span><br><span class="line">openssl x509 -req -in kubectl.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out kubectl.crt -days 5000</span><br><span class="line"></span><br><span class="line">export KUBECONFIG=/etc/kubernetes/admin.conf</span><br><span class="line">kubectl config set-cluster kubernetes --server=https://172.16.170.128:6443 --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true</span><br><span class="line">kubectl config set-credentials system:admin --client-certificate=/etc/kubernetes/pki/kubectl.crt --client-key=/etc/kubernetes/pki/kubectl.key --embed-certs=true</span><br><span class="line">kubectl config set-context system:admin@kubernetes --cluster=kubernetes --user=system:admin</span><br><span class="line">kubectl config use-context system:admin@kubernetes</span><br><span class="line">unset KUBECONFIG</span><br><span class="line"></span><br><span class="line"># 为kubelet生成相关的证书和配置文件</span><br><span class="line">openssl genrsa -out kubelet.key 2048</span><br><span class="line">openssl req -new -key kubelet.key -subj &quot;/CN=node&quot; -out kubelet.csr</span><br><span class="line">openssl x509 -req -in kubelet.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out kubelet.crt -days 5000</span><br><span class="line"></span><br><span class="line">export KUBECONFIG=/etc/kubernetes/kubelet.conf</span><br><span class="line">kubectl config set-cluster kubernetes --server=https://172.16.170.128:6443 --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true</span><br><span class="line">kubectl config set-credentials system:kubelet --client-certificate=/etc/kubernetes/pki/kubelet.crt --client-key=/etc/kubernetes/pki/kubelet.key --embed-certs=true</span><br><span class="line">kubectl config set-context system:kubelet@kubernetes --cluster=kubernetes --user=system:kubelet</span><br><span class="line">kubectl config use-context system:kubelet@kubernetes</span><br><span class="line">unset KUBECONFIG</span><br><span class="line"></span><br><span class="line"># 为kube-proxy生成相关的证书和配置文件</span><br><span class="line">openssl genrsa -out proxy.key 2048</span><br><span class="line">openssl req -new -key proxy.key -subj &quot;/CN=node&quot; -out proxy.csr</span><br><span class="line">openssl x509 -req -in proxy.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out proxy.crt -days 5000</span><br><span class="line"></span><br><span class="line">export KUBECONFIG=/etc/kubernetes/proxy.conf</span><br><span class="line">kubectl config set-cluster kubernetes --server=https://172.16.170.128:6443 --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true</span><br><span class="line">kubectl config set-credentials system:proxy --client-certificate=/etc/kubernetes/pki/proxy.crt --client-key=/etc/kubernetes/pki/proxy.key --embed-certs=true</span><br><span class="line">kubectl config set-context system:proxy@kubernetes --cluster=kubernetes --user=system:proxy</span><br><span class="line">kubectl config use-context system:proxy@kubernetes</span><br><span class="line">unset KUBECONFIG</span><br></pre></td></tr></table></figure>
<h2 id="5-配置和启动Master上的所有组件"><a href="#5-配置和启动Master上的所有组件" class="headerlink" title="5. 配置和启动Master上的所有组件"></a>5. 配置和启动Master上的所有组件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"># 配置和启动etcd服务</span><br><span class="line">mkdir -p /var/lib/etcd/</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /usr/lib/systemd/system/etcd.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Etcd Server </span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=simple</span><br><span class="line">WorkingDirectory=/var/lib/etcd/</span><br><span class="line">EnvironmentFile=-/etc/etcd/etcd.conf</span><br><span class="line">ExecStart=/usr/bin/etcd</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable etcd.service</span><br><span class="line">systemctl start etcd.service</span><br><span class="line">systemctl status etcd.service</span><br><span class="line"></span><br><span class="line"># 配置和启动kube-apiserver服务</span><br><span class="line">cat &lt;&lt;EOF &gt; /usr/lib/systemd/system/kube-apiserver.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes API Server</span><br><span class="line">Documentation=https://github.com/kubernetes/kubernetes</span><br><span class="line">After=etcd.service</span><br><span class="line">Wants=etcd.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=-/etc/kubernetes/kube-apiserver</span><br><span class="line">ExecStart=/usr/bin/kube-apiserver \$KUBE_API_ARGS</span><br><span class="line">Restart=on-failure</span><br><span class="line">Type=notify</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/kubernetes/kube-apiserver</span><br><span class="line">KUBE_API_ARGS=&quot;--storage-backend=etcd3 --etcd-servers=http://127.0.0.1:2379 --allow-privileged=true --client-ca-file=/etc/kubernetes/pki/ca.crt --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key --service-account-key-file=/etc/kubernetes/sa.pub --advertise-address=172.16.170.128 --secure-port=6443 --insecure-bind-address=0.0.0.0 --insecure-port=8080 --service-cluster-ip-range=10.96.0.0/16  --service-node-port-range=30000-32767 --authorization-mode=Node,RBAC  --logtostderr=false --log-dir=/var/log/kubernetes --v=2&quot;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-apiserver.service</span><br><span class="line">systemctl start kube-apiserver.service</span><br><span class="line">systemctl status kube-apiserver.service</span><br><span class="line"></span><br><span class="line"># 为第4步中涉及的用户master和node绑定cluster-admin角色</span><br><span class="line">kubectl create clusterrolebinding system:component:master --clusterrole=cluster-admin --user=master</span><br><span class="line">kubectl create clusterrolebinding system:component:node --clusterrole=cluster-admin --user=node</span><br><span class="line"></span><br><span class="line"># 配置和启动kube-controller-manager服务</span><br><span class="line">cat &lt;&lt;EOF &gt; /usr/lib/systemd/system/kube-controller-manager.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Controller Manager</span><br><span class="line">Documentation=https://github.com/kubernetes/kubernetes</span><br><span class="line">After=kube-apiserver.service</span><br><span class="line">Requires=kube-apiserver.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=-/etc/kubernetes/kube-controller-manager</span><br><span class="line">ExecStart=/usr/bin/kube-controller-manager \$KUBE_CONTROLLER_MANAGER_ARGS</span><br><span class="line">Restart=on-failure</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/kubernetes/kube-controller-manager</span><br><span class="line">KUBE_CONTROLLER_MANAGER_ARGS=&quot;--service-account-private-key-file=/etc/kubernetes/sa.key  --leader-elect=true --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --root-ca-file=/etc/kubernetes/pki/ca.crt --kubeconfig=/etc/kubernetes/controller-manager.conf --allocate-node-cidrs=true  --cluster-cidr=10.211.0.0/16 --logtostderr=false --log-dir=/var/log/kubernetes --v=2&quot;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-controller-manager.service</span><br><span class="line">systemctl start kube-controller-manager.service</span><br><span class="line">systemctl status kube-controller-manager.service</span><br><span class="line"></span><br><span class="line"># 配置和启动kube-scheduler服务</span><br><span class="line">cat &lt;&lt;EOF &gt; /usr/lib/systemd/system/kube-scheduler.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Scheduler</span><br><span class="line">Documentation=https://github.com/kubernetes/kubernetes</span><br><span class="line">After=kube-apiserver.service</span><br><span class="line">Requires=kube-apiserver.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=-/etc/kubernetes/kube-scheduler</span><br><span class="line">ExecStart=/usr/bin/kube-scheduler \$KUBE_SCHEDULER_ARGS</span><br><span class="line">Restart=on-failure</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/kubernetes/kube-scheduler</span><br><span class="line">KUBE_SCHEDULER_ARGS=&quot;--kubeconfig=/etc/kubernetes/scheduler.conf --leader-elect=true --logtostderr=false --log-dir=/var/log/kubernetes --v=2&quot;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-scheduler.service</span><br><span class="line">systemctl start kube-scheduler.service</span><br><span class="line">systemctl status kube-scheduler.service</span><br></pre></td></tr></table></figure>
<h2 id="5-配置和启动Node上的所有组件"><a href="#5-配置和启动Node上的所有组件" class="headerlink" title="5. 配置和启动Node上的所有组件"></a>5. 配置和启动Node上的所有组件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"># 创建配置目录和工作目录</span><br><span class="line">mkdir -p /etc/kubernetes</span><br><span class="line">mkdir -p /var/lib/kubelet</span><br><span class="line"></span><br><span class="line"># 传输相关配置文件到当前节点上</span><br><span class="line">scp root@172.16.170.128:/etc/kubernetes/kubelet.conf /etc/kubernetes/</span><br><span class="line">scp root@172.16.170.128:/etc/kubernetes/proxy.conf /etc/kubernetes/</span><br><span class="line"></span><br><span class="line"># 配置和启动kubelet服务</span><br><span class="line">cat &lt;&lt;EOF &gt; /usr/lib/systemd/system/kubelet.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Kubelet Server</span><br><span class="line">Documentation=https://github.com/kubernetes/kubernetes</span><br><span class="line">After=docker.service</span><br><span class="line">Requires=docker.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=/var/lib/kubelet</span><br><span class="line">EnvironmentFile=-/etc/kubernetes/kubelet</span><br><span class="line">ExecStart=/usr/bin/kubelet \$KUBELET_ARGS</span><br><span class="line">Restart=on-failure</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/kubernetes/kubelet</span><br><span class="line">KUBELET_ARGS=&quot;--kubeconfig=/etc/kubernetes/kubelet.conf --cluster-dns=10.96.0.10 --cluster-domain=cluster.local --logtostderr=false --log-dir=/var/log/kubernetes --v=2 --cgroup-driver=cgroupfs --cni-bin-dir=/opt/cni/bin --cni-conf-dir=/etc/cni/net.d --network-plugin=cni --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1&quot;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kubelet.service</span><br><span class="line">systemctl start kubelet.service</span><br><span class="line">systemctl status kubelet.service</span><br><span class="line"></span><br><span class="line"># 配置和启动kube-proxy服务</span><br><span class="line">cat &lt;&lt;EOF &gt; /usr/lib/systemd/system/kube-proxy.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Proxy Server</span><br><span class="line">Documentation=https://github.com/kubernetes/kubernetes</span><br><span class="line">After=network.target</span><br><span class="line">Requires=network.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=-/etc/kubernetes/kube-proxy</span><br><span class="line">ExecStart=/usr/bin/kube-proxy \$KUBE_PROXY_ARGS</span><br><span class="line">Restart=on-failure</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/kubernetes/kube-proxy</span><br><span class="line">KUBE_PROXY_ARGS=&quot;--kubeconfig=/etc/kubernetes/proxy.conf --logtostderr=false --log-dir=/var/log/kubernetes --v=2&quot;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-proxy.service</span><br><span class="line">systemctl start kube-proxy.service</span><br><span class="line">systemctl status kube-proxy.service</span><br></pre></td></tr></table></figure>
<h2 id="6-配置和安装网络插件"><a href="#6-配置和安装网络插件" class="headerlink" title="6. 配置和安装网络插件"></a>6. 配置和安装网络插件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p calico</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; calico/rbac-kdd.yaml</span><br><span class="line"># Calico Version v3.1.3</span><br><span class="line"># https://docs.projectcalico.org/v3.1/releases#v3.1.3</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-node</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - namespaces</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - pods/status</span><br><span class="line">    verbs:</span><br><span class="line">      - update</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - pods</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">      - patch</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - services</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - endpoints</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - nodes</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - update</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups: [&quot;extensions&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - networkpolicies</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups: [&quot;networking.k8s.io&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - networkpolicies</span><br><span class="line">    verbs:</span><br><span class="line">      - watch</span><br><span class="line">      - list</span><br><span class="line">  - apiGroups: [&quot;crd.projectcalico.org&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - globalfelixconfigs</span><br><span class="line">      - felixconfigurations</span><br><span class="line">      - bgppeers</span><br><span class="line">      - globalbgpconfigs</span><br><span class="line">      - bgpconfigurations</span><br><span class="line">      - ippools</span><br><span class="line">      - globalnetworkpolicies</span><br><span class="line">      - globalnetworksets</span><br><span class="line">      - networkpolicies</span><br><span class="line">      - clusterinformations</span><br><span class="line">      - hostendpoints</span><br><span class="line">    verbs:</span><br><span class="line">      - create</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - update</span><br><span class="line">      - watch</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-node</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: calico-node</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: calico-node</span><br><span class="line">  namespace: kube-system</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; calico/calico.yaml</span><br><span class="line"># Calico Version v3.1.3</span><br><span class="line"># https://docs.projectcalico.org/v3.1/releases#v3.1.3</span><br><span class="line"># This manifest includes the following component versions:</span><br><span class="line">#   calico/node:v3.1.3</span><br><span class="line">#   calico/cni:v3.1.3</span><br><span class="line"></span><br><span class="line"># This ConfigMap is used to configure a self-hosted Calico installation.</span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-config</span><br><span class="line">  namespace: kube-system</span><br><span class="line">data:</span><br><span class="line">  # To enable Typha, set this to &quot;calico-typha&quot; *and* set a non-zero value for Typha replicas</span><br><span class="line">  # below.  We recommend using Typha if you have more than 50 nodes. Above 100 nodes it is</span><br><span class="line">  # essential.</span><br><span class="line">  typha_service_name: &quot;none&quot;</span><br><span class="line"></span><br><span class="line">  # The CNI network configuration to install on each node.</span><br><span class="line">  cni_network_config: |-</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;k8s-pod-network&quot;,</span><br><span class="line">      &quot;cniVersion&quot;: &quot;0.3.0&quot;,</span><br><span class="line">      &quot;plugins&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;type&quot;: &quot;calico&quot;,</span><br><span class="line">          &quot;log_level&quot;: &quot;info&quot;,</span><br><span class="line">          &quot;datastore_type&quot;: &quot;kubernetes&quot;,</span><br><span class="line">          &quot;nodename&quot;: &quot;__KUBERNETES_NODE_NAME__&quot;,</span><br><span class="line">          &quot;mtu&quot;: 1500,</span><br><span class="line">          &quot;ipam&quot;: &#123;</span><br><span class="line">            &quot;type&quot;: &quot;host-local&quot;,</span><br><span class="line">            &quot;subnet&quot;: &quot;usePodCidr&quot;</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;policy&quot;: &#123;</span><br><span class="line">            &quot;type&quot;: &quot;k8s&quot;</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;kubernetes&quot;: &#123;</span><br><span class="line">            &quot;kubeconfig&quot;: &quot;__KUBECONFIG_FILEPATH__&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;type&quot;: &quot;portmap&quot;,</span><br><span class="line">          &quot;snat&quot;: true,</span><br><span class="line">          &quot;capabilities&quot;: &#123;&quot;portMappings&quot;: true&#125;</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"># This manifest creates a Service, which will be backed by Calico&apos;s Typha daemon.</span><br><span class="line"># Typha sits in between Felix and the API server, reducing Calico&apos;s load on the API server.</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-typha</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: calico-typha</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">    - port: 5473</span><br><span class="line">      protocol: TCP</span><br><span class="line">      targetPort: calico-typha</span><br><span class="line">      name: calico-typha</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: calico-typha</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"># This manifest creates a Deployment of Typha to back the above service.</span><br><span class="line"></span><br><span class="line">apiVersion: apps/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-typha</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: calico-typha</span><br><span class="line">spec:</span><br><span class="line">  # Number of Typha replicas.  To enable Typha, set this to a non-zero value *and* set the</span><br><span class="line">  # typha_service_name variable in the calico-config ConfigMap above.</span><br><span class="line">  #</span><br><span class="line">  # We recommend using Typha if you have more than 50 nodes.  Above 100 nodes it is essential</span><br><span class="line">  # (when using the Kubernetes datastore).  Use one replica for every 100-200 nodes.  In</span><br><span class="line">  # production, we recommend running at least 3 replicas to reduce the impact of rolling upgrade.</span><br><span class="line">  replicas: 0</span><br><span class="line">  revisionHistoryLimit: 2</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: calico-typha</span><br><span class="line">      annotations:</span><br><span class="line">        # This, along with the CriticalAddonsOnly toleration below, marks the pod as a critical</span><br><span class="line">        # add-on, ensuring it gets priority scheduling and that its resources are reserved</span><br><span class="line">        # if it ever gets evicted.</span><br><span class="line">        scheduler.alpha.kubernetes.io/critical-pod: &apos;&apos;</span><br><span class="line">    spec:</span><br><span class="line">      hostNetwork: true</span><br><span class="line">      tolerations:</span><br><span class="line">        # Mark the pod as a critical add-on for rescheduling.</span><br><span class="line">        - key: CriticalAddonsOnly</span><br><span class="line">          operator: Exists</span><br><span class="line">      # Since Calico can&apos;t network a pod until Typha is up, we need to run Typha itself</span><br><span class="line">      # as a host-networked pod.</span><br><span class="line">      serviceAccountName: calico-node</span><br><span class="line">      containers:</span><br><span class="line">      - image: quay.io/calico/typha:v0.7.4</span><br><span class="line">        name: calico-typha</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 5473</span><br><span class="line">          name: calico-typha</span><br><span class="line">          protocol: TCP</span><br><span class="line">        env:</span><br><span class="line">          # Enable &quot;info&quot; logging by default.  Can be set to &quot;debug&quot; to increase verbosity.</span><br><span class="line">          - name: TYPHA_LOGSEVERITYSCREEN</span><br><span class="line">            value: &quot;info&quot;</span><br><span class="line">          # Disable logging to file and syslog since those don&apos;t make sense in Kubernetes.</span><br><span class="line">          - name: TYPHA_LOGFILEPATH</span><br><span class="line">            value: &quot;none&quot;</span><br><span class="line">          - name: TYPHA_LOGSEVERITYSYS</span><br><span class="line">            value: &quot;none&quot;</span><br><span class="line">          # Monitor the Kubernetes API to find the number of running instances and rebalance</span><br><span class="line">          # connections.</span><br><span class="line">          - name: TYPHA_CONNECTIONREBALANCINGMODE</span><br><span class="line">            value: &quot;kubernetes&quot;</span><br><span class="line">          - name: TYPHA_DATASTORETYPE</span><br><span class="line">            value: &quot;kubernetes&quot;</span><br><span class="line">          - name: TYPHA_HEALTHENABLED</span><br><span class="line">            value: &quot;true&quot;</span><br><span class="line">          # Uncomment these lines to enable prometheus metrics.  Since Typha is host-networked,</span><br><span class="line">          # this opens a port on the host, which may need to be secured.</span><br><span class="line">          #- name: TYPHA_PROMETHEUSMETRICSENABLED</span><br><span class="line">          #  value: &quot;true&quot;</span><br><span class="line">          #- name: TYPHA_PROMETHEUSMETRICSPORT</span><br><span class="line">          #  value: &quot;9093&quot;</span><br><span class="line">        livenessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /liveness</span><br><span class="line">            port: 9098</span><br><span class="line">          periodSeconds: 30</span><br><span class="line">          initialDelaySeconds: 30</span><br><span class="line">        readinessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /readiness</span><br><span class="line">            port: 9098</span><br><span class="line">          periodSeconds: 10</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: tz-config</span><br><span class="line">          mountPath: /etc/localtime</span><br><span class="line">          readOnly: true</span><br><span class="line">      volumes:</span><br><span class="line">      - name: tz-config</span><br><span class="line">        hostPath:</span><br><span class="line">          path: /etc/localtime</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"># This manifest installs the calico/node container, as well</span><br><span class="line"># as the Calico CNI plugins and network config on</span><br><span class="line"># each master and worker node in a Kubernetes cluster.</span><br><span class="line">kind: DaemonSet</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-node</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: calico-node</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: calico-node</span><br><span class="line">  updateStrategy:</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">    rollingUpdate:</span><br><span class="line">      maxUnavailable: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: calico-node</span><br><span class="line">      annotations:</span><br><span class="line">        # This, along with the CriticalAddonsOnly toleration below,</span><br><span class="line">        # marks the pod as a critical add-on, ensuring it gets</span><br><span class="line">        # priority scheduling and that its resources are reserved</span><br><span class="line">        # if it ever gets evicted.</span><br><span class="line">        scheduler.alpha.kubernetes.io/critical-pod: &apos;&apos;</span><br><span class="line">    spec:</span><br><span class="line">      hostNetwork: true</span><br><span class="line">      tolerations:</span><br><span class="line">        # Make sure calico/node gets scheduled on all nodes.</span><br><span class="line">        - effect: NoSchedule</span><br><span class="line">          operator: Exists</span><br><span class="line">        # Mark the pod as a critical add-on for rescheduling.</span><br><span class="line">        - key: CriticalAddonsOnly</span><br><span class="line">          operator: Exists</span><br><span class="line">        - effect: NoExecute</span><br><span class="line">          operator: Exists</span><br><span class="line">      serviceAccountName: calico-node</span><br><span class="line">      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a &quot;force</span><br><span class="line">      # deletion&quot;: https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.</span><br><span class="line">      terminationGracePeriodSeconds: 0</span><br><span class="line">      containers:</span><br><span class="line">        # Runs calico/node container on each Kubernetes node.  This</span><br><span class="line">        # container programs network policy and routes on each</span><br><span class="line">        # host.</span><br><span class="line">        - name: calico-node</span><br><span class="line">          image: quay.io/calico/node:v3.1.3</span><br><span class="line">          env:</span><br><span class="line">            # Use Kubernetes API as the backing datastore.</span><br><span class="line">            - name: DATASTORE_TYPE</span><br><span class="line">              value: &quot;kubernetes&quot;</span><br><span class="line">            # Enable felix info logging.</span><br><span class="line">            - name: FELIX_LOGSEVERITYSCREEN</span><br><span class="line">              value: &quot;info&quot;</span><br><span class="line">            # Cluster type to identify the deployment type</span><br><span class="line">            - name: CLUSTER_TYPE</span><br><span class="line">              value: &quot;k8s,bgp&quot;</span><br><span class="line">            # Disable file logging so \`kubectl logs\` works.</span><br><span class="line">            - name: CALICO_DISABLE_FILE_LOGGING</span><br><span class="line">              value: &quot;true&quot;</span><br><span class="line">            # Set Felix endpoint to host default action to ACCEPT.</span><br><span class="line">            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION</span><br><span class="line">              value: &quot;ACCEPT&quot;</span><br><span class="line">            # Disable IPV6 on Kubernetes.</span><br><span class="line">            - name: FELIX_IPV6SUPPORT</span><br><span class="line">              value: &quot;false&quot;</span><br><span class="line">            # Set MTU for tunnel device used if ipip is enabled</span><br><span class="line">            - name: FELIX_IPINIPMTU</span><br><span class="line">              value: &quot;1440&quot;</span><br><span class="line">            # Wait for the datastore.</span><br><span class="line">            - name: WAIT_FOR_DATASTORE</span><br><span class="line">              value: &quot;true&quot;</span><br><span class="line">            # The default IPv4 pool to create on startup if none exists. Pod IPs will be</span><br><span class="line">            # chosen from this range. Changing this value after installation will have</span><br><span class="line">            # no effect. This should fall within \`--cluster-cidr\`.</span><br><span class="line">            - name: CALICO_IPV4POOL_CIDR</span><br><span class="line">              value: &quot;10.211.0.0/16&quot;</span><br><span class="line">            # Enable IPIP</span><br><span class="line">            - name: CALICO_IPV4POOL_IPIP</span><br><span class="line">              value: &quot;CrossSubnet&quot;</span><br><span class="line">            # Enable IP-in-IP within Felix.</span><br><span class="line">            - name: FELIX_IPINIPENABLED</span><br><span class="line">              value: &quot;true&quot;</span><br><span class="line">            # Typha support: controlled by the ConfigMap.</span><br><span class="line">            - name: FELIX_TYPHAK8SSERVICENAME</span><br><span class="line">              valueFrom:</span><br><span class="line">                configMapKeyRef:</span><br><span class="line">                  name: calico-config</span><br><span class="line">                  key: typha_service_name</span><br><span class="line">            # Set based on the k8s node name.</span><br><span class="line">            - name: NODENAME</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: spec.nodeName</span><br><span class="line">            # Auto-detect the BGP IP address.</span><br><span class="line">            - name: IP</span><br><span class="line">              value: &quot;autodetect&quot;</span><br><span class="line">            - name: FELIX_HEALTHENABLED</span><br><span class="line">              value: &quot;true&quot;</span><br><span class="line">          securityContext:</span><br><span class="line">            privileged: true</span><br><span class="line">          resources:</span><br><span class="line">            requests:</span><br><span class="line">              cpu: 250m</span><br><span class="line">          livenessProbe:</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /liveness</span><br><span class="line">              port: 9099</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            initialDelaySeconds: 10</span><br><span class="line">            failureThreshold: 6</span><br><span class="line">          readinessProbe:</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /readiness</span><br><span class="line">              port: 9099</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - mountPath: /lib/modules</span><br><span class="line">              name: lib-modules</span><br><span class="line">              readOnly: true</span><br><span class="line">            - mountPath: /var/run/calico</span><br><span class="line">              name: var-run-calico</span><br><span class="line">              readOnly: false</span><br><span class="line">            - mountPath: /var/lib/calico</span><br><span class="line">              name: var-lib-calico</span><br><span class="line">              readOnly: false</span><br><span class="line">            - name: tz-config</span><br><span class="line">              mountPath: /etc/localtime</span><br><span class="line">              readOnly: true</span><br><span class="line">        # This container installs the Calico CNI binaries</span><br><span class="line">        # and CNI network config file on each node.</span><br><span class="line">        - name: install-cni</span><br><span class="line">          image: quay.io/calico/cni:v3.1.3</span><br><span class="line">          command: [&quot;/install-cni.sh&quot;]</span><br><span class="line">          env:</span><br><span class="line">            # Name of the CNI config file to create.</span><br><span class="line">            - name: CNI_CONF_NAME</span><br><span class="line">              value: &quot;10-calico.conflist&quot;</span><br><span class="line">            # The CNI network config to install on each node.</span><br><span class="line">            - name: CNI_NETWORK_CONFIG</span><br><span class="line">              valueFrom:</span><br><span class="line">                configMapKeyRef:</span><br><span class="line">                  name: calico-config</span><br><span class="line">                  key: cni_network_config</span><br><span class="line">            # Set the hostname based on the k8s node name.</span><br><span class="line">            - name: KUBERNETES_NODE_NAME</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: spec.nodeName</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - mountPath: /host/opt/cni/bin</span><br><span class="line">              name: cni-bin-dir</span><br><span class="line">            - mountPath: /host/etc/cni/net.d</span><br><span class="line">              name: cni-net-dir</span><br><span class="line">            - name: tz-config</span><br><span class="line">              mountPath: /etc/localtime</span><br><span class="line">              readOnly: true</span><br><span class="line">      volumes:</span><br><span class="line">        # Used by calico/node.</span><br><span class="line">        - name: lib-modules</span><br><span class="line">          hostPath:</span><br><span class="line">            path: /lib/modules</span><br><span class="line">        - name: var-run-calico</span><br><span class="line">          hostPath:</span><br><span class="line">            path: /var/run/calico</span><br><span class="line">        - name: var-lib-calico</span><br><span class="line">          hostPath:</span><br><span class="line">            path: /var/lib/calico</span><br><span class="line">        # Used to install CNI.</span><br><span class="line">        - name: cni-bin-dir</span><br><span class="line">          hostPath:</span><br><span class="line">            path: /opt/cni/bin</span><br><span class="line">        - name: cni-net-dir</span><br><span class="line">          hostPath:</span><br><span class="line">            path: /etc/cni/net.d</span><br><span class="line">        - name: tz-config</span><br><span class="line">          hostPath:</span><br><span class="line">            path: /etc/localtime</span><br><span class="line"></span><br><span class="line"># Create all the CustomResourceDefinitions needed for</span><br><span class="line"># Calico policy and networking mode.</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">   name: felixconfigurations.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: FelixConfiguration</span><br><span class="line">    plural: felixconfigurations</span><br><span class="line">    singular: felixconfiguration</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: bgppeers.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: BGPPeer</span><br><span class="line">    plural: bgppeers</span><br><span class="line">    singular: bgppeer</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: bgpconfigurations.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: BGPConfiguration</span><br><span class="line">    plural: bgpconfigurations</span><br><span class="line">    singular: bgpconfiguration</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: ippools.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: IPPool</span><br><span class="line">    plural: ippools</span><br><span class="line">    singular: ippool</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: hostendpoints.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: HostEndpoint</span><br><span class="line">    plural: hostendpoints</span><br><span class="line">    singular: hostendpoint</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: clusterinformations.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: ClusterInformation</span><br><span class="line">    plural: clusterinformations</span><br><span class="line">    singular: clusterinformation</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: globalnetworkpolicies.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: GlobalNetworkPolicy</span><br><span class="line">    plural: globalnetworkpolicies</span><br><span class="line">    singular: globalnetworkpolicy</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: globalnetworksets.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: GlobalNetworkSet</span><br><span class="line">    plural: globalnetworksets</span><br><span class="line">    singular: globalnetworkset</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: networkpolicies.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Namespaced</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: NetworkPolicy</span><br><span class="line">    plural: networkpolicies</span><br><span class="line">    singular: networkpolicy</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-node</span><br><span class="line">  namespace: kube-system</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl create -f calico/</span><br></pre></td></tr></table></figure>
<h2 id="7-配置和安装DNS插件"><a href="#7-配置和安装DNS插件" class="headerlink" title="7. 配置和安装DNS插件"></a>7. 配置和安装DNS插件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p kube-dns/</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; kube-dns/deployment.yaml</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kube-dns</span><br><span class="line">  name: coredns</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  revisionHistoryLimit: 10</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: kube-dns</span><br><span class="line">  strategy:</span><br><span class="line">    rollingUpdate:</span><br><span class="line">      maxSurge: 25%</span><br><span class="line">      maxUnavailable: 1</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: kube-dns</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - args:</span><br><span class="line">        - -conf</span><br><span class="line">        - /etc/coredns/Corefile</span><br><span class="line">        image: registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.1.3</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        livenessProbe:</span><br><span class="line">          failureThreshold: 5</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /health</span><br><span class="line">            port: 8080</span><br><span class="line">            scheme: HTTP</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          periodSeconds: 10</span><br><span class="line">          successThreshold: 1</span><br><span class="line">          timeoutSeconds: 5</span><br><span class="line">        name: coredns</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 53</span><br><span class="line">          name: dns</span><br><span class="line">          protocol: UDP</span><br><span class="line">        - containerPort: 53</span><br><span class="line">          name: dns-tcp</span><br><span class="line">          protocol: TCP</span><br><span class="line">        - containerPort: 9153</span><br><span class="line">          name: metrics</span><br><span class="line">          protocol: TCP</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            memory: 170Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 100m</span><br><span class="line">            memory: 70Mi</span><br><span class="line">        securityContext:</span><br><span class="line">          allowPrivilegeEscalation: false</span><br><span class="line">          capabilities:</span><br><span class="line">            add:</span><br><span class="line">            - NET_BIND_SERVICE</span><br><span class="line">            drop:</span><br><span class="line">            - all</span><br><span class="line">          readOnlyRootFilesystem: true</span><br><span class="line">        terminationMessagePath: /dev/termination-log</span><br><span class="line">        terminationMessagePolicy: File</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: /etc/coredns</span><br><span class="line">          name: config-volume</span><br><span class="line">          readOnly: true</span><br><span class="line">        - mountPath: /etc/localtime</span><br><span class="line">          name: tz-config</span><br><span class="line">          readOnly: true</span><br><span class="line">      dnsPolicy: Default</span><br><span class="line">      restartPolicy: Always</span><br><span class="line">      schedulerName: default-scheduler</span><br><span class="line">      securityContext: &#123;&#125;</span><br><span class="line">      serviceAccount: coredns</span><br><span class="line">      serviceAccountName: coredns</span><br><span class="line">      terminationGracePeriodSeconds: 30</span><br><span class="line">      tolerations:</span><br><span class="line">      - key: CriticalAddonsOnly</span><br><span class="line">        operator: Exists</span><br><span class="line">      - effect: NoSchedule</span><br><span class="line">        key: node-role.kubernetes.io/master</span><br><span class="line">      volumes:</span><br><span class="line">      - configMap:</span><br><span class="line">          defaultMode: 420</span><br><span class="line">          items:</span><br><span class="line">          - key: Corefile</span><br><span class="line">            path: Corefile</span><br><span class="line">          name: coredns</span><br><span class="line">        name: config-volume</span><br><span class="line">      - hostPath:</span><br><span class="line">          path: /etc/localtime</span><br><span class="line">          type: &quot;&quot;</span><br><span class="line">        name: tz-config</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; kube-dns/service.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    prometheus.io/scrape: &quot;true&quot;</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kube-dns</span><br><span class="line">    kubernetes.io/cluster-service: &quot;true&quot;</span><br><span class="line">    kubernetes.io/name: KubeDNS</span><br><span class="line">  name: kube-dns</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  clusterIP: 10.96.0.10</span><br><span class="line">  ports:</span><br><span class="line">  - name: dns</span><br><span class="line">    port: 53</span><br><span class="line">    protocol: UDP</span><br><span class="line">    targetPort: 53</span><br><span class="line">  - name: dns-tcp</span><br><span class="line">    port: 53</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: 53</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: kube-dns</span><br><span class="line">  sessionAffinity: None</span><br><span class="line">  type: ClusterIP</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; kube-dns/configmap.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: coredns</span><br><span class="line">  namespace: kube-system</span><br><span class="line">data:</span><br><span class="line">  Corefile: |</span><br><span class="line">    .:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        health</span><br><span class="line">        kubernetes cluster.local in-addr.arpa ip6.arpa &#123;</span><br><span class="line">           pods insecure</span><br><span class="line">           upstream</span><br><span class="line">           fallthrough in-addr.arpa ip6.arpa</span><br><span class="line">        &#125;</span><br><span class="line">        prometheus :9153</span><br><span class="line">        proxy . /etc/resolv.conf</span><br><span class="line">        cache 30</span><br><span class="line">        reload</span><br><span class="line">    &#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; kube-dns/serviceaccount.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: coredns</span><br><span class="line">  namespace: kube-system</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; kube-dns/clusterrole.yaml</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: system:coredns</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - &quot;&quot;</span><br><span class="line">  resources:</span><br><span class="line">  - endpoints</span><br><span class="line">  - services</span><br><span class="line">  - pods</span><br><span class="line">  - namespaces</span><br><span class="line">  verbs:</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; kube-dns/clusterrolebinding.yaml</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: system:coredns</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:coredns</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: coredns</span><br><span class="line">  namespace: kube-system</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl create -f kube-dns/</span><br></pre></td></tr></table></figure>
<h2 id="7-验证Pod的网络和DNS配置"><a href="#7-验证Pod的网络和DNS配置" class="headerlink" title="7. 验证Pod的网络和DNS配置"></a>7. 验证Pod的网络和DNS配置</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line"># 在node01节点和node02节点上分别操作</span><br><span class="line">mkdir -p network/</span><br><span class="line">cd network/</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; Dockerfile</span><br><span class="line">FROM alpine:3.8</span><br><span class="line"></span><br><span class="line">MAINTAINER wangxin_0611@126.com</span><br><span class="line"></span><br><span class="line">RUN apk add --no-cache ca-certificates bind-tools iputils iproute2 net-tools tcpdump</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">docker build -t alpine:3.8-network .</span><br><span class="line"></span><br><span class="line"># 在master节点上操作</span><br><span class="line">mkdir -p network/</span><br><span class="line">cd network/</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; network.yaml</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: network</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: network</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: network</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: network</span><br><span class="line">        image: alpine:3.8-network</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        command:</span><br><span class="line">        - sleep</span><br><span class="line">        - &quot;3600&quot;</span><br><span class="line">      restartPolicy: Always</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl create -f network/</span><br><span class="line"></span><br><span class="line">[root@master ~]# kubectl get pod -o wide</span><br><span class="line">NAME            READY     STATUS    RESTARTS   AGE       IP           NODE</span><br><span class="line">network-bl7jx   1/1       Running   0          6m        10.211.1.4   node02</span><br><span class="line">network-m2vp6   1/1       Running   0          6m        10.211.0.4   node01</span><br><span class="line"></span><br><span class="line"># 在node01上的pod中验证</span><br><span class="line">[root@master ~]# kubectl exec -it network-m2vp6 /bin/sh</span><br><span class="line">/ # cat /etc/resolv.conf</span><br><span class="line">nameserver 10.96.0.10</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br><span class="line">/ # ip address</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">4: eth0@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether 72:01:49:fa:fb:f3 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 10.211.0.4/32 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::7001:49ff:fefa:fbf3/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">/ # ping -c 4 10.211.1.4</span><br><span class="line">PING 10.211.1.4 (10.211.1.4) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.1.4: icmp_seq=1 ttl=62 time=0.314 ms</span><br><span class="line">64 bytes from 10.211.1.4: icmp_seq=2 ttl=62 time=0.490 ms</span><br><span class="line">64 bytes from 10.211.1.4: icmp_seq=3 ttl=62 time=0.415 ms</span><br><span class="line">64 bytes from 10.211.1.4: icmp_seq=4 ttl=62 time=0.491 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.1.4 ping statistics ---</span><br><span class="line">4 packets transmitted, 4 received, 0% packet loss, time 3000ms</span><br><span class="line">rtt min/avg/max/mdev = 0.314/0.427/0.491/0.075 ms</span><br><span class="line">/ # nslookup kubernetes.default</span><br><span class="line">Server:   10.96.0.10</span><br><span class="line">Address:  10.96.0.10#53</span><br><span class="line"></span><br><span class="line">Name: kubernetes.default.svc.cluster.local</span><br><span class="line">Address: 10.96.0.1</span><br><span class="line"></span><br><span class="line">/ # nslookup kubernetes</span><br><span class="line">Server:   10.96.0.10</span><br><span class="line">Address:  10.96.0.10#53</span><br><span class="line"></span><br><span class="line">Name: kubernetes.default.svc.cluster.local</span><br><span class="line">Address: 10.96.0.1</span><br><span class="line"></span><br><span class="line">/ # exit</span><br><span class="line"></span><br><span class="line"># 在node02上的pod中验证</span><br><span class="line">[root@master ~]# kubectl get pod -o wide</span><br><span class="line">NAME            READY     STATUS    RESTARTS   AGE       IP           NODE</span><br><span class="line">network-bl7jx   1/1       Running   0          9m        10.211.1.4   node02</span><br><span class="line">network-m2vp6   1/1       Running   0          9m        10.211.0.4   node01</span><br><span class="line">[root@master ~]# kubectl exec -it network-bl7jx /bin/sh</span><br><span class="line">/ # cat /etc/resolv.conf</span><br><span class="line">nameserver 10.96.0.10</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br><span class="line">/ # ip address</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">4: eth0@if9: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether 5a:a6:51:22:9d:2b brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 10.211.1.4/32 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::58a6:51ff:fe22:9d2b/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">/ # ping -c 4 10.211.0.4</span><br><span class="line">PING 10.211.0.4 (10.211.0.4) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.0.4: icmp_seq=1 ttl=62 time=0.450 ms</span><br><span class="line">64 bytes from 10.211.0.4: icmp_seq=2 ttl=62 time=0.685 ms</span><br><span class="line">64 bytes from 10.211.0.4: icmp_seq=3 ttl=62 time=0.726 ms</span><br><span class="line">64 bytes from 10.211.0.4: icmp_seq=4 ttl=62 time=0.707 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.0.4 ping statistics ---</span><br><span class="line">4 packets transmitted, 4 received, 0% packet loss, time 3003ms</span><br><span class="line">rtt min/avg/max/mdev = 0.450/0.642/0.726/0.111 ms</span><br><span class="line">/ # nslookup kubernetes.default</span><br><span class="line">Server:   10.96.0.10</span><br><span class="line">Address:  10.96.0.10#53</span><br><span class="line"></span><br><span class="line">Name: kubernetes.default.svc.cluster.local</span><br><span class="line">Address: 10.96.0.1</span><br><span class="line"></span><br><span class="line">/ # nslookup kubernetes</span><br><span class="line">Server:   10.96.0.10</span><br><span class="line">Address:  10.96.0.10#53</span><br><span class="line"></span><br><span class="line">Name: kubernetes.default.svc.cluster.local</span><br><span class="line">Address: 10.96.0.1</span><br><span class="line"></span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Setup/">Setup</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-p2p_distribute_images_000" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/04/29/p2p_distribute_images_000/" class="article-date">
  	<time datetime="2019-04-29T07:07:06.405Z" itemprop="datePublished">2019-04-29</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/04/29/p2p_distribute_images_000/">
        使用Dragonfly实现P2P分发镜像
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、Dragonfly概述"><a href="#一、Dragonfly概述" class="headerlink" title="一、Dragonfly概述"></a>一、Dragonfly概述</h1><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h2><p>Dragonfly 是一款基于 P2P 的智能镜像和文件分发工具。它旨在提高文件传输的效率和速率，最大限度地利用网络带宽，尤其是在分发大量数据时，例如应用分发、缓存分发、日志分发和镜像分发。<br>尽管容器技术大部分时候简化了运维工作，但是它也带来了一些挑战：例如镜像分发的效率问题，尤其是必须在多个主机上复制镜像分发时。Dragonfly 在这种场景下能够完美支持 Docker ，相比原生方式，它能将容器镜像的分发速度提高了 57 倍，并让 Registry 网络出口流量降低 99.5%。使用它可以让容器镜像的分发变得简单而经济。</p>
<h2 id="2-官方文档地址"><a href="#2-官方文档地址" class="headerlink" title="2. 官方文档地址"></a>2. 官方文档地址</h2><p><a href="https://d7y.io/en-us/" target="_blank" rel="noopener">https://d7y.io/en-us/</a></p>
<h2 id="3-GitHub地址"><a href="#3-GitHub地址" class="headerlink" title="3. GitHub地址"></a>3. GitHub地址</h2><p><a href="https://github.com/dragonflyoss/Dragonfly/tree/v0.3.0" target="_blank" rel="noopener">https://github.com/dragonflyoss/Dragonfly/tree/v0.3.0</a></p>
<h2 id="4-注意事项"><a href="#4-注意事项" class="headerlink" title="4. 注意事项"></a>4. 注意事项</h2><p>目前Dragonfly的最新版本是v0.3.0，暂不支持对镜像仓库中的对私有镜像的认证。比如使用Harbor这种镜像仓库，把某个project的权限设置为私有权限，那么该project下的镜像是无法直接通过Dragonfly实现镜像的分发的。想通过Dragonfly实现镜像的分发，最简单的办法是必须把镜像对应的project的权限设置为公有权限。还有一种办法是为docker daemon设置http全局代理，但是必须使用0.0.1版本，最新的v0.3.0版本不支持这种方式。不过很遗憾，经测试该方式不好用。官方的Issues链接如下：<br><a href="https://github.com/dragonflyoss/Dragonfly/issues/138" target="_blank" rel="noopener">https://github.com/dragonflyoss/Dragonfly/issues/138</a></p>
<p>为docker daemon配置全局代理，方法如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /etc/systemd/system/docker.service.d</span><br><span class="line">vi /etc/systemd/system/docker.service.d/http-proxy.conf</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Environment=&quot;HTTP_PROXY=http://127.0.0.1:65001&quot;</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart docker.service</span><br><span class="line">systemctl show --property=Environment docker.service</span><br></pre></td></tr></table></figure></p>
<p>为docker daemon配置全局代理，参考链接如下：<br><a href="https://docs.docker.com/config/daemon/systemd/#httphttps-proxy" target="_blank" rel="noopener">https://docs.docker.com/config/daemon/systemd/#httphttps-proxy</a></p>
<h1 id="二、环境的相关信息"><a href="#二、环境的相关信息" class="headerlink" title="二、环境的相关信息"></a>二、环境的相关信息</h1><h2 id="1-版本信息"><a href="#1-版本信息" class="headerlink" title="1. 版本信息"></a>1. 版本信息</h2><p>Docker Engine Community 18.09.5<br>Harbor 0.5.0<br>Dragonfly v0.3.0</p>
<h2 id="2-服务器信息"><a href="#2-服务器信息" class="headerlink" title="2. 服务器信息"></a>2. 服务器信息</h2><p>172.16.170.134 <-> supernode<br>172.16.170.135 <-> dfclient<br>172.16.170.136 <-> dfclient</-></-></-></p>
<h1 id="三、实验过程记录"><a href="#三、实验过程记录" class="headerlink" title="三、实验过程记录"></a>三、实验过程记录</h1><h2 id="1-在172-16-170-134上安装supernode，如下所示："><a href="#1-在172-16-170-134上安装supernode，如下所示：" class="headerlink" title="1. 在172.16.170.134上安装supernode，如下所示："></a>1. 在172.16.170.134上安装supernode，如下所示：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name supernode --restart=always -p 8001:8001 -p 8002:8002 \</span><br><span class="line">    dragonflyoss/supernode:0.3.0 -Dsupernode.advertiseIp=172.16.170.134</span><br></pre></td></tr></table></figure>
<h2 id="2-分别在172-16-170-135和172-16-170-136上安装dfclient，如下所示："><a href="#2-分别在172-16-170-135和172-16-170-136上安装dfclient，如下所示：" class="headerlink" title="2. 分别在172.16.170.135和172.16.170.136上安装dfclient，如下所示："></a>2. 分别在172.16.170.135和172.16.170.136上安装dfclient，如下所示：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOD &gt; /etc/dragonfly/dfget.yml</span><br><span class="line">nodes:</span><br><span class="line">    - 172.16.170.134</span><br><span class="line">EOD</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOD &gt; /etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;http://127.0.0.1:65001&quot;],</span><br><span class="line">  &quot;insecure-registries&quot;: [&quot;10.0.55.126&quot;]</span><br><span class="line">&#125;</span><br><span class="line">EOD</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart docker.service</span><br><span class="line"></span><br><span class="line">docker run -d --name dfclient --restart=always -p 65001:65001 \</span><br><span class="line">    -v /etc/dragonfly:/etc/dragonfly \</span><br><span class="line">    dragonflyoss/dfclient:v0.3.0 --registry http://10.0.55.126</span><br></pre></td></tr></table></figure>
<h2 id="3-分别在172-16-170-135和172-16-170-136上安装拉取一个镜像，如下所示："><a href="#3-分别在172-16-170-135和172-16-170-136上安装拉取一个镜像，如下所示：" class="headerlink" title="3. 分别在172.16.170.135和172.16.170.136上安装拉取一个镜像，如下所示："></a>3. 分别在172.16.170.135和172.16.170.136上安装拉取一个镜像，如下所示：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># docker pull base/alpine:3.8</span><br><span class="line">3.8: Pulling from base/alpine</span><br><span class="line">16f532fbdc2a: Already exists</span><br><span class="line">Digest: sha256:78903b603e9fe76129d1a59ec94bc1ad47769b98e57e8f0c0a57760b12615960</span><br><span class="line">Status: Downloaded newer image for base/alpine:3.8</span><br><span class="line"></span><br><span class="line"># docker pull base/alpine:3.8</span><br><span class="line">3.8: Pulling from base/alpine</span><br><span class="line">Digest: sha256:78903b603e9fe76129d1a59ec94bc1ad47769b98e57e8f0c0a57760b12615960</span><br><span class="line">Status: Image is up to date for base/alpine:3.8</span><br></pre></td></tr></table></figure>
<h1 id="四、参考资料"><a href="#四、参考资料" class="headerlink" title="四、参考资料"></a>四、参考资料</h1><p><a href="https://github.com/dragonflyoss/Dragonfly/tree/v0.0.1" target="_blank" rel="noopener">https://github.com/dragonflyoss/Dragonfly/tree/v0.0.1</a><br><a href="https://github.com/dragonflyoss/Dragonfly/issues/138" target="_blank" rel="noopener">https://github.com/dragonflyoss/Dragonfly/issues/138</a><br><a href="http://dockone.io/article/4646" target="_blank" rel="noopener">http://dockone.io/article/4646</a><br><a href="https://mp.weixin.qq.com/s/95mX8cDox5bmgQ2xGHLPqQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/95mX8cDox5bmgQ2xGHLPqQ</a><br><a href="https://www.cnblogs.com/atuotuo/p/7298673.html" target="_blank" rel="noopener">https://www.cnblogs.com/atuotuo/p/7298673.html</a><br><a href="https://docs.docker.com/config/daemon/systemd/" target="_blank" rel="noopener">https://docs.docker.com/config/daemon/systemd/</a><br><a href="http://likakuli.com/post/2018/09/13/dragonfly/" target="_blank" rel="noopener">http://likakuli.com/post/2018/09/13/dragonfly/</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Dragonfly/">Dragonfly</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/P2P/">P2P</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kubernetes_build_000" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/02/14/kubernetes_build_000/" class="article-date">
  	<time datetime="2019-02-14T09:38:51.222Z" itemprop="datePublished">2019-02-14</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/02/14/kubernetes_build_000/">
        如何使用Kubernetes源代码做编译构建
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、操作系统版本"><a href="#一、操作系统版本" class="headerlink" title="一、操作系统版本"></a>一、操作系统版本</h1><p>CentOS 7.6.1810<br>Kubernetes v1.11.0</p>
<h1 id="二、准备工作"><a href="#二、准备工作" class="headerlink" title="二、准备工作"></a>二、准备工作</h1><h2 id="1-直接使用yum安装rsync和gcc"><a href="#1-直接使用yum安装rsync和gcc" class="headerlink" title="1. 直接使用yum安装rsync和gcc"></a>1. 直接使用yum安装rsync和gcc</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y git rsync gcc</span><br></pre></td></tr></table></figure>
<h2 id="2-源码安装golang-1-10-2"><a href="#2-源码安装golang-1-10-2" class="headerlink" title="2. 源码安装golang 1.10.2"></a>2. 源码安装golang 1.10.2</h2><p>详见官方文档</p>
<h2 id="3-克隆源代码到GOPATH的src目录下"><a href="#3-克隆源代码到GOPATH的src目录下" class="headerlink" title="3. 克隆源代码到GOPATH的src目录下"></a>3. 克隆源代码到GOPATH的src目录下</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd $GOPATH/src/</span><br><span class="line">mkdir -p k8s.io/</span><br><span class="line">cd k8s.io/</span><br><span class="line">git clone https://github.com/kubernetes/kubernetes.git</span><br><span class="line">cd kubernetes/</span><br><span class="line">git checkout -b v1.11.0 v1.11.0</span><br></pre></td></tr></table></figure>
<h1 id="三、构建过程记录"><a href="#三、构建过程记录" class="headerlink" title="三、构建过程记录"></a>三、构建过程记录</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">[root@server01 kubernetes]# make clean</span><br><span class="line">+++ [0107 11:01:28] Verifying Prerequisites....</span><br><span class="line">+++ [0107 11:01:28] Removing _output directory</span><br><span class="line">Removing pkg/generated/openapi/zz_generated.openapi.go ..</span><br><span class="line">Removing pkg/generated/bindata.go ..</span><br><span class="line">Removing test/e2e/generated/bindata.go ..</span><br><span class="line">[root@server01 kubernetes]# KUBE_GIT_MAJOR=&quot;1&quot; KUBE_GIT_MINOR=&quot;11&quot; KUBE_GIT_VERSION=&quot;v1.11.0&quot; KUBE_GIT_COMMIT=&quot;91e7b4fd31fcd3d5f436da26c980becec37ceefe&quot; KUBE_GIT_TREE_STATE=&quot;clean&quot; make all</span><br><span class="line">+++ [0107 11:01:57] Building go targets for linux/amd64:</span><br><span class="line">    ./vendor/k8s.io/code-generator/cmd/deepcopy-gen</span><br><span class="line">+++ [0107 11:02:05] Building go targets for linux/amd64:</span><br><span class="line">    ./vendor/k8s.io/code-generator/cmd/defaulter-gen</span><br><span class="line">+++ [0107 11:02:09] Building go targets for linux/amd64:</span><br><span class="line">    ./vendor/k8s.io/code-generator/cmd/conversion-gen</span><br><span class="line">+++ [0107 11:02:13] Building go targets for linux/amd64:</span><br><span class="line">    ./vendor/k8s.io/code-generator/cmd/openapi-gen</span><br><span class="line">+++ [0107 11:02:19] Building go targets for linux/amd64:</span><br><span class="line">    ./vendor/github.com/jteeuwen/go-bindata/go-bindata</span><br><span class="line">+++ [0107 11:02:20] Building go targets for linux/amd64:</span><br><span class="line">    cmd/kube-proxy</span><br><span class="line">    cmd/kube-apiserver</span><br><span class="line">    cmd/kube-controller-manager</span><br><span class="line">    cmd/cloud-controller-manager</span><br><span class="line">    cmd/kubelet</span><br><span class="line">    cmd/kubeadm</span><br><span class="line">    cmd/hyperkube</span><br><span class="line">    cmd/kube-scheduler</span><br><span class="line">    vendor/k8s.io/kube-aggregator</span><br><span class="line">    vendor/k8s.io/apiextensions-apiserver</span><br><span class="line">    cluster/gce/gci/mounter</span><br><span class="line">    cmd/kubectl</span><br><span class="line">    cmd/gendocs</span><br><span class="line">    cmd/genkubedocs</span><br><span class="line">    cmd/genman</span><br><span class="line">    cmd/genyaml</span><br><span class="line">    cmd/genswaggertypedocs</span><br><span class="line">    cmd/linkcheck</span><br><span class="line">    vendor/github.com/onsi/ginkgo/ginkgo</span><br><span class="line">    test/e2e/e2e.test</span><br><span class="line">    cmd/kubemark</span><br><span class="line">    vendor/github.com/onsi/ginkgo/ginkgo</span><br><span class="line">    test/e2e_node/e2e_node.test</span><br><span class="line">[root@server01 kubernetes]# ls -la _output/bin/</span><br><span class="line">总用量 2323024</span><br><span class="line">drwxr-xr-x. 2 root root      4096 1月   7 11:08 .</span><br><span class="line">drwxr-xr-x. 3 root root        19 1月   7 11:01 ..</span><br><span class="line">-rwxr-xr-x. 1 root root  59300539 1月   7 11:08 apiextensions-apiserver</span><br><span class="line">-rwxr-xr-x. 1 root root 138056772 1月   7 11:08 cloud-controller-manager</span><br><span class="line">-rwxr-xr-x. 1 root root   7691655 1月   7 11:02 conversion-gen</span><br><span class="line">-rwxr-xr-x. 1 root root   7687498 1月   7 11:01 deepcopy-gen</span><br><span class="line">-rwxr-xr-x. 1 root root   7665142 1月   7 11:02 defaulter-gen</span><br><span class="line">-rwxr-xr-x. 1 root root 209978368 1月   7 11:08 e2e_node.test</span><br><span class="line">-rwxr-xr-x. 1 root root 173425408 1月   7 11:08 e2e.test</span><br><span class="line">-rwxr-xr-x. 1 root root  54135796 1月   7 11:08 gendocs</span><br><span class="line">-rwxr-xr-x. 1 root root 226087352 1月   7 11:08 genkubedocs</span><br><span class="line">-rwxr-xr-x. 1 root root 232056512 1月   7 11:08 genman</span><br><span class="line">-rwxr-xr-x. 1 root root   5477486 1月   7 11:08 genswaggertypedocs</span><br><span class="line">-rwxr-xr-x. 1 root root  54084772 1月   7 11:08 genyaml</span><br><span class="line">-rwxr-xr-x. 1 root root  10641201 1月   7 11:08 ginkgo</span><br><span class="line">-rwxr-xr-x. 1 root root   2831370 1月   7 11:02 go-bindata</span><br><span class="line">-rwxr-xr-x. 1 root root 227335584 1月   7 11:08 hyperkube</span><br><span class="line">-rwxr-xr-x. 1 root root  57251126 1月   7 11:08 kubeadm</span><br><span class="line">-rwxr-xr-x. 1 root root  57912503 1月   7 11:08 kube-aggregator</span><br><span class="line">-rwxr-xr-x. 1 root root 185160079 1月   7 11:08 kube-apiserver</span><br><span class="line">-rwxr-xr-x. 1 root root 153806893 1月   7 11:08 kube-controller-manager</span><br><span class="line">-rwxr-xr-x. 1 root root  55277901 1月   7 11:08 kubectl</span><br><span class="line">-rwxr-xr-x. 1 root root 162729192 1月   7 11:08 kubelet</span><br><span class="line">-rwxr-xr-x. 1 root root 160066432 1月   7 11:08 kubemark</span><br><span class="line">-rwxr-xr-x. 1 root root  51920351 1月   7 11:08 kube-proxy</span><br><span class="line">-rwxr-xr-x. 1 root root  55479582 1月   7 11:08 kube-scheduler</span><br><span class="line">-rwxr-xr-x. 1 root root   6698678 1月   7 11:08 linkcheck</span><br><span class="line">-rwxr-xr-x. 1 root root   2330265 1月   7 11:08 mounter</span><br><span class="line">-rwxr-xr-x. 1 root root  13634378 1月   7 11:02 openapi-gen</span><br></pre></td></tr></table></figure>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Source-Build/">Source Build</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-dns_query" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/02/11/dns_query/" class="article-date">
  	<time datetime="2019-02-11T04:05:13.681Z" itemprop="datePublished">2019-02-11</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/02/11/dns_query/">
        DNS正解和反解查询示例
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="DNS正解和反解查询示例"><a href="#DNS正解和反解查询示例" class="headerlink" title="DNS正解和反解查询示例"></a>DNS正解和反解查询示例</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nslookup www.baidu.com 8.8.8.8 -port=53</span><br><span class="line">nslookup -querytype=ptr 120.114.100.20 8.8.8.8 -port=53</span><br><span class="line"></span><br><span class="line">dig @8.8.8.8 -p 53 www.baidu.com</span><br><span class="line">dig -p 53 -x 120.114.100.20 @8.8.8.8</span><br></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DNS/">DNS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Network/">Network</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kubernetes_glusterfs_003" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/01/30/kubernetes_glusterfs_003/" class="article-date">
  	<time datetime="2019-01-30T08:19:05.435Z" itemprop="datePublished">2019-01-30</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/30/kubernetes_glusterfs_003/">
        使用Heketi工具管理GlusterFS集群
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="使用Heketi管理GlusterFS-Cluster"><a href="#使用Heketi管理GlusterFS-Cluster" class="headerlink" title="使用Heketi管理GlusterFS Cluster"></a>使用Heketi管理GlusterFS Cluster</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br></pre></td><td class="code"><pre><span class="line"># 创建集群</span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 --json=true cluster create &#123;&quot;id&quot;:&quot;0f0e5d914e3d9a17bef670dd6e295512&quot;,&quot;nodes&quot;:[],&quot;volumes&quot;:[]&#125;</span><br><span class="line"></span><br><span class="line"># 查看集群</span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 --json=true cluster list &#123;&quot;clusters&quot;:[&quot;0f0e5d914e3d9a17bef670dd6e295512&quot;]&#125;</span><br><span class="line"></span><br><span class="line"># 添加节点</span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 --json=true node add --cluster=0f0e5d914e3d9a17bef670dd6e295512 --management-host-name=192.168.86.135 --storage-host-name=192.168.86.135 --zone=1</span><br><span class="line">&#123;&quot;zone&quot;:1,&quot;hostnames&quot;:&#123;&quot;manage&quot;:[&quot;192.168.86.135&quot;],&quot;storage&quot;:[&quot;192.168.86.135&quot;]&#125;,&quot;cluster&quot;:&quot;0f0e5d914e3d9a17bef670dd6e295512&quot;,&quot;i</span><br><span class="line"></span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 --json=true node add --cluster=0f0e5d914e3d9a17bef670dd6e295512 --management-host-name=192.168.86.136 --storage-host-name=192.168.86.136 --zone=1</span><br><span class="line">&#123;&quot;zone&quot;:1,&quot;hostnames&quot;:&#123;&quot;manage&quot;:[&quot;192.168.86.136&quot;],&quot;storage&quot;:[&quot;192.168.86.136&quot;]&#125;,&quot;cluster&quot;:&quot;0f0e5d914e3d9a17bef670dd6e295512&quot;,&quot;i</span><br><span class="line"></span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 --json=true node add --cluster=0f0e5d914e3d9a17bef670dd6e295512 --management-host-name=192.168.86.137 --storage-host-name=192.168.86.137 --zone=1</span><br><span class="line">&#123;&quot;zone&quot;:1,&quot;hostnames&quot;:&#123;&quot;manage&quot;:[&quot;192.168.86.137&quot;],&quot;storage&quot;:[&quot;192.168.86.137&quot;]&#125;,&quot;cluster&quot;:&quot;0f0e5d914e3d9a17bef670dd6e295512&quot;,&quot;i</span><br><span class="line"></span><br><span class="line"># 查看节点</span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 --json=true node list Id:a2c00d537210d42d68679bfa240db8b5 Cluster:0f0e5d914e3d9a17bef670dd6e295512</span><br><span class="line">Id:a73f8bdd464f02a4c7e0dd30c4ed5afa Cluster:0f0e5d914e3d9a17bef670dd6e295512</span><br><span class="line">Id:aba2f2cfe93c0f3e4164aa05b8d6ddb2 Cluster:0f0e5d914e3d9a17bef670dd6e295512</span><br><span class="line"></span><br><span class="line"># 给节点添加设备(裸硬盘)</span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 --json=true device add --name=/dev/sdb --node=a2c00d537210d42d68679bfa240db8b5</span><br><span class="line">Device added successfully</span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 --json=true device add --name=/dev/sdb --node=a73f8bdd464f02a4c7e0dd30c4ed5afa</span><br><span class="line">Device added successfully</span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 --json=true device add --name=/dev/sdb --node=aba2f2cfe93c0f3e4164aa05b8d6ddb2</span><br><span class="line">Device added successfully</span><br><span class="line"></span><br><span class="line"># 创建volume</span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 volume create --size=10 --replica=2 </span><br><span class="line">Name: vol_d1fd574462c162f42fb61ef55c3e4e6e</span><br><span class="line">Size: 10</span><br><span class="line">Volume Id: d1fd574462c162f42fb61ef55c3e4e6e</span><br><span class="line">Cluster Id: 0f0e5d914e3d9a17bef670dd6e295512</span><br><span class="line">Mount: 192.168.86.135:vol_d1fd574462c162f42fb61ef55c3e4e6e</span><br><span class="line">Mount Options: backup-volfile-servers=192.168.86.136,192.168.86.137</span><br><span class="line">Durability Type: replicate</span><br><span class="line">Distributed+Replica: 2</span><br><span class="line"></span><br><span class="line"># 查看volume</span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 volume list Id:d1fd574462c162f42fb61ef55c3e4e6e Cluster:0f0e5d914e3d9a17bef670dd6e295512 Name:vol_d1fd574462c162f42fb61ef55c3e4e6e</span><br><span class="line"># 删除volume</span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 volume delete d1fd574462c162f42fb61ef55c3e4e6e</span><br><span class="line">Volume d1fd574462c162f42fb61ef55c3e4e6e deleted</span><br><span class="line"></span><br><span class="line"># 使用topology.json初始化GlusterFS Cluster</span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/heketi/topology.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;clusters&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;nodes&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;node&quot;: &#123;</span><br><span class="line">            &quot;hostnames&quot;: &#123;</span><br><span class="line">              &quot;manage&quot;: [</span><br><span class="line">                &quot;server07&quot;</span><br><span class="line">              ],</span><br><span class="line">              &quot;storage&quot;: [</span><br><span class="line">                &quot;172.16.170.134&quot;</span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;zone&quot;: 1</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;devices&quot;: [</span><br><span class="line">            &quot;/dev/sdb&quot;</span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;node&quot;: &#123;</span><br><span class="line">            &quot;hostnames&quot;: &#123;</span><br><span class="line">              &quot;manage&quot;: [</span><br><span class="line">                &quot;server08&quot;</span><br><span class="line">              ],</span><br><span class="line">              &quot;storage&quot;: [</span><br><span class="line">                &quot;172.16.170.135&quot;</span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;zone&quot;: 1</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;devices&quot;: [</span><br><span class="line">            &quot;/dev/sdb&quot;</span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;node&quot;: &#123;</span><br><span class="line">            &quot;hostnames&quot;: &#123;</span><br><span class="line">              &quot;manage&quot;: [</span><br><span class="line">                &quot;server09&quot;</span><br><span class="line">              ],</span><br><span class="line">              &quot;storage&quot;: [</span><br><span class="line">                &quot;172.16.170.136&quot;</span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;zone&quot;: 1</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;devices&quot;: [</span><br><span class="line">            &quot;/dev/sdb&quot;</span><br><span class="line">          ]</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">heketi-cli --user admin --secret 12345678 topology load --json=/etc/heketi/topology.json</span><br><span class="line">Creating cluster ... ID: 4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line">	Allowing file volumes on cluster.</span><br><span class="line">	Allowing block volumes on cluster.</span><br><span class="line">	Creating node server07 ... ID: 2e8b83add06cde8713d56ecb7d424033</span><br><span class="line">		Adding device /dev/sdb ... OK</span><br><span class="line">	Creating node server08 ... ID: 56a23b0579e7f79511843e046e69008f</span><br><span class="line">		Adding device /dev/sdb ... OK</span><br><span class="line">	Creating node server09 ... ID: 9fb13189ba78ef302046fe4414f633bf</span><br><span class="line">		Adding device /dev/sdb ... OK</span><br><span class="line"></span><br><span class="line">heketi-cli --user admin --secret 12345678 topology info</span><br><span class="line"></span><br><span class="line">Cluster Id: 4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line"></span><br><span class="line">    File:  true</span><br><span class="line">    Block: true</span><br><span class="line"></span><br><span class="line">    Volumes:</span><br><span class="line"></span><br><span class="line">    Nodes:</span><br><span class="line"></span><br><span class="line">	Node Id: 2e8b83add06cde8713d56ecb7d424033</span><br><span class="line">	State: online</span><br><span class="line">	Cluster Id: 4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line">	Zone: 1</span><br><span class="line">	Management Hostnames: server07</span><br><span class="line">	Storage Hostnames: 172.16.170.134</span><br><span class="line">	Devices:</span><br><span class="line">		Id:4995671aaef70cb1f640d0f411e94d2f   Name:/dev/sdb            State:online    Size (GiB):19      Used (GiB):0       Free (GiB):19</span><br><span class="line">			Bricks:</span><br><span class="line"></span><br><span class="line">	Node Id: 56a23b0579e7f79511843e046e69008f</span><br><span class="line">	State: online</span><br><span class="line">	Cluster Id: 4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line">	Zone: 1</span><br><span class="line">	Management Hostnames: server08</span><br><span class="line">	Storage Hostnames: 172.16.170.135</span><br><span class="line">	Devices:</span><br><span class="line">		Id:c4a56208419516d4fbc437d9dc3b265e   Name:/dev/sdb            State:online    Size (GiB):19      Used (GiB):0       Free (GiB):19</span><br><span class="line">			Bricks:</span><br><span class="line"></span><br><span class="line">	Node Id: 9fb13189ba78ef302046fe4414f633bf</span><br><span class="line">	State: online</span><br><span class="line">	Cluster Id: 4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line">	Zone: 1</span><br><span class="line">	Management Hostnames: server09</span><br><span class="line">	Storage Hostnames: 172.16.170.136</span><br><span class="line">	Devices:</span><br><span class="line">		Id:be2a9558f0634233be72f0c55d051898   Name:/dev/sdb            State:online    Size (GiB):19      Used (GiB):0       Free (GiB):19</span><br><span class="line">			Bricks:</span><br></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GlusterFS/">GlusterFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Storage/">Storage</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kubernetes_glusterfs_002" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/01/30/kubernetes_glusterfs_002/" class="article-date">
  	<time datetime="2019-01-30T06:56:31.063Z" itemprop="datePublished">2019-01-30</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/30/kubernetes_glusterfs_002/">
        使用Gluster工具管理GlusterFS集群
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、配置和验证GlusterFS-Cluster"><a href="#一、配置和验证GlusterFS-Cluster" class="headerlink" title="一、配置和验证GlusterFS Cluster"></a>一、配置和验证GlusterFS Cluster</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 添加节点到glusterfs cluster</span><br><span class="line">[root@server07 ~]# gluster peer probe server08</span><br><span class="line">peer probe: success.</span><br><span class="line"></span><br><span class="line">[root@server07 ~]# gluster peer probe server09 </span><br><span class="line">peer probe: success.</span><br><span class="line"></span><br><span class="line"># 查看glusterfs cluster的状态 </span><br><span class="line">[root@server07 ~]# gluster peer status </span><br><span class="line">Number of Peers: 2</span><br><span class="line"></span><br><span class="line">Hostname: server08</span><br><span class="line">Uuid: 8530c074-760f-4d03-a5a7-f1b3ccaa5cfd </span><br><span class="line">State: Peer in Cluster (Connected)</span><br><span class="line">Hostname: server09</span><br><span class="line"></span><br><span class="line">Uuid: 41a4b6df-bcb3-4650-8a21-54afc1e27cbe </span><br><span class="line">State: Peer in Cluster (Connected)</span><br></pre></td></tr></table></figure>
<h1 id="二、在GlusterFS-Cluster上操作和使用Volume"><a href="#二、在GlusterFS-Cluster上操作和使用Volume" class="headerlink" title="二、在GlusterFS Cluster上操作和使用Volume"></a>二、在GlusterFS Cluster上操作和使用Volume</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line"># 查看volume的列表</span><br><span class="line">[root@server07 ~]# gluster volume info </span><br><span class="line">No volumes present</span><br><span class="line"></span><br><span class="line"># 创建volume对应的存储目录(集群的所有主机上都要创建) </span><br><span class="line">mkdir -p /opt/gluster/data</span><br><span class="line"></span><br><span class="line"># 创建volume </span><br><span class="line">[root@server07 ~]# gluster volume create k8s-volume transport tcp server07:/opt/gluster/data server08:/opt/gluster/data server09:/opt/gluster/data force</span><br><span class="line">volume create: k8s-volume: success: please start the volume to access data</span><br><span class="line"></span><br><span class="line">[root@server07 ~]# gluster volume info k8s-volume</span><br><span class="line"></span><br><span class="line">Volume Name: k8s-volume</span><br><span class="line">Type: Distribute</span><br><span class="line">Volume ID: e4974285-1304-4ea7-b60f-ebe8375dba86</span><br><span class="line">Status: Created</span><br><span class="line">Snapshot Count: 0</span><br><span class="line">Number of Bricks: 3</span><br><span class="line">Transport-type: tcp</span><br><span class="line">Bricks:</span><br><span class="line">Brick1: server07:/opt/gluster/data</span><br><span class="line">Brick2: server08:/opt/gluster/data</span><br><span class="line">Brick3: server09:/opt/gluster/data</span><br><span class="line">Options Reconfigured:</span><br><span class="line">transport.address-family: inet </span><br><span class="line">nfs.disable: on</span><br><span class="line"></span><br><span class="line"># 启动volume</span><br><span class="line">[root@server07 ~]# gluster volume start k8s-volume </span><br><span class="line">volume start: k8s-volume: success</span><br><span class="line"></span><br><span class="line"># 查看volume的信息</span><br><span class="line">[root@server07 ~]# gluster volume info k8s-volume</span><br><span class="line"></span><br><span class="line">Volume Name: k8s-volume</span><br><span class="line">Type: Distribute</span><br><span class="line">Volume ID: e4974285-1304-4ea7-b60f-ebe8375dba86</span><br><span class="line">Status: Started</span><br><span class="line">Snapshot Count: 0</span><br><span class="line">Number of Bricks: 3</span><br><span class="line">Transport-type: tcp</span><br><span class="line">Bricks: </span><br><span class="line">Brick1: server07:/opt/gluster/data</span><br><span class="line">Brick2: server08:/opt/gluster/data</span><br><span class="line">Brick3: server09:/opt/gluster/data</span><br><span class="line">Options Reconfigured:</span><br><span class="line">transport.address-family: inet</span><br><span class="line">nfs.disable: on</span><br><span class="line"></span><br><span class="line"># 验证数据卷的挂载和数据写入</span><br><span class="line">[root@server07 ~]# mount -t glusterfs server07:k8s-volume /mnt</span><br><span class="line">[root@server07 ~]# ls -la /mnt</span><br><span class="line">drwxr-xr-x. 3 root root 4096 4月 9 23:36 .</span><br><span class="line">dr-xr-xr-x. 17 root root 224 4月 9 21:59 ..</span><br><span class="line">[root@server07 ~]# echo &quot;hello glusterfs kubernetes.&quot; &gt; /mnt/readme.md</span><br><span class="line">[root@server07 ~]# ls -la /mnt</span><br><span class="line">drwxr-xr-x. 3 root root 4096 4月 10 05:36 .</span><br><span class="line">dr-xr-xr-x. 17 root root 224 4月 9 21:59 ..</span><br><span class="line">-rw-r--r--. 1 root root 28 4月 10 05:36 readme.md</span><br><span class="line"></span><br><span class="line">[root@server07 ~]# cat /mnt/readme.md</span><br><span class="line">hello glusterfs kubernetes.</span><br><span class="line"></span><br><span class="line">[root@server07 ~]# df -h</span><br><span class="line">文件系统 容量 已用 可用 已用% 挂载点</span><br><span class="line">/dev/mapper/cl-root 8.0G 1.1G 7.0G 14% /</span><br><span class="line">devtmpfs 478M 0 478M 0% /dev</span><br><span class="line">tmpfs 489M 0 489M 0% /dev/shm</span><br><span class="line">tmpfs 489M 6.8M 482M 2% /run</span><br><span class="line">tmpfs 489M 0 489M 0% /sys/fs/cgroup</span><br><span class="line">/dev/sda1 1014M 139M 876M 14% /boot</span><br><span class="line">tmpfs 98M 0 98M 0% /run/user/0</span><br><span class="line">server07:k8s-volume 24G 3.2G 21G 14% /mnt</span><br><span class="line">[root@server07 ~]# umount server07:k8s-volume</span><br><span class="line">[root@server07 ~]# ls -la /mnt/</span><br><span class="line">drwxr-xr-x. 2 root root 6 11月 5 2016 .</span><br><span class="line">dr-xr-xr-x. 17 root root 224 4月 9 21:59 ..</span><br><span class="line"></span><br><span class="line"># 查看volume的信息</span><br><span class="line">[root@server07 ~]# gluster volume info k8s-volume</span><br><span class="line"></span><br><span class="line">Volume Name: k8s-volume</span><br><span class="line">Type: Distribute</span><br><span class="line">Volume ID: e4974285-1304-4ea7-b60f-ebe8375dba86</span><br><span class="line">Status: Started</span><br><span class="line">Snapshot Count: 0</span><br><span class="line">Number of Bricks: 3</span><br><span class="line">Transport-type: tcp</span><br><span class="line">Bricks: </span><br><span class="line">Brick1: server07:/opt/gluster/data</span><br><span class="line">Brick2: server08:/opt/gluster/data</span><br><span class="line">Brick3: server09:/opt/gluster/data</span><br><span class="line">Options Reconfigured:</span><br><span class="line">transport.address-family: inet</span><br><span class="line">nfs.disable: on</span><br><span class="line"></span><br><span class="line"># 停止volume</span><br><span class="line">[root@server07 ~]# gluster volume stop k8s-volume</span><br><span class="line">Stopping volume will make its data inaccessible. Do you want to continue? (y/n) y</span><br><span class="line">volume stop: k8s-volume: success</span><br><span class="line"></span><br><span class="line"># 查看volume的信息</span><br><span class="line">[root@server07 ~]# gluster volume info k8s-volume</span><br><span class="line"></span><br><span class="line">Volume Name: k8s-volume</span><br><span class="line">Type: Distribute</span><br><span class="line">Volume ID: e4974285-1304-4ea7-b60f-ebe8375dba86</span><br><span class="line">Status: Stopped</span><br><span class="line">Snapshot Count: 0</span><br><span class="line">Number of Bricks: 3</span><br><span class="line">Transport-type: tcp</span><br><span class="line">Bricks: </span><br><span class="line">Brick1: server07:/opt/gluster/data</span><br><span class="line">Brick2: server08:/opt/gluster/data</span><br><span class="line">Brick3: server09:/opt/gluster/data</span><br><span class="line">Options Reconfigured:</span><br><span class="line">transport.address-family: inet</span><br><span class="line">nfs.disable: on</span><br><span class="line"></span><br><span class="line"># 删除volume</span><br><span class="line">[root@server07 ~]# gluster volume delete k8s-volume</span><br><span class="line">Deleting volume will erase all information about the volume. Do you want to continue? (y/n) y</span><br><span class="line">volume delete: k8s-volume: success</span><br><span class="line"></span><br><span class="line"># 查看volume的信息</span><br><span class="line">[root@server07 ~]# gluster volume info k8s-volume</span><br><span class="line">Volume k8s-volume does not exist</span><br></pre></td></tr></table></figure>
<h1 id="三、如何重置GlusterFS-Cluster中的所有Node"><a href="#三、如何重置GlusterFS-Cluster中的所有Node" class="headerlink" title="三、如何重置GlusterFS Cluster中的所有Node"></a>三、如何重置GlusterFS Cluster中的所有Node</h1><p>假设集群中只有一个volume，它叫k8s-volume，下面对集群进行重置:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 重置glusterfs</span><br><span class="line">gluster volume list</span><br><span class="line">gluster volume stop k8s-volume </span><br><span class="line">gluster volume delete k8s-volume </span><br><span class="line">gluster volume list</span><br><span class="line">gluster peer status</span><br><span class="line">gluster peer help</span><br><span class="line">gluster peer detach server08</span><br><span class="line">gluster peer detach server09</span><br><span class="line">gluster peer status</span><br></pre></td></tr></table></figure></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GlusterFS/">GlusterFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Storage/">Storage</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kubernetes_glusterfs_001" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/01/30/kubernetes_glusterfs_001/" class="article-date">
  	<time datetime="2019-01-30T02:05:36.673Z" itemprop="datePublished">2019-01-30</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/30/kubernetes_glusterfs_001/">
        Kubernetes对接GlusterFS（把GlusterFS部署在Kubernetes集群外）
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、环境版本信息"><a href="#一、环境版本信息" class="headerlink" title="一、环境版本信息"></a>一、环境版本信息</h1><h2 id="1-版本信息"><a href="#1-版本信息" class="headerlink" title="1. 版本信息"></a>1. 版本信息</h2><p>CentOS 7.6.1810<br>Kubernetes v1.11.0<br>GlusterFS 5.3-1.el7 @centos-gluster5<br>Heketi 8.0.0-1.el7 @centos-gluster5</p>
<h2 id="2-服务器信息"><a href="#2-服务器信息" class="headerlink" title="2. 服务器信息"></a>2. 服务器信息</h2><p>server01 <-> 172.16.170.128 <-> kubernetes master<br>server02 <-> 172.16.170.129 <-> kubernetes node<br>server03 <-> 172.16.170.130 <-> kubernetes node</-></-></-></-></-></-></p>
<p>server07 <-> 172.16.170.134 <-> glusterfs node<br>server08 <-> 172.16.170.135 <-> glusterfs node<br>server09 <-> 172.16.170.136 <-> glusterfs node</-></-></-></-></-></-></p>
<h1 id="二、相关注意事项"><a href="#二、相关注意事项" class="headerlink" title="二、相关注意事项"></a>二、相关注意事项</h1><h2 id="1-Heketi所在Node到GlusterFS-Cluster的所有Node需要配置root用户的SSH免密登录；"><a href="#1-Heketi所在Node到GlusterFS-Cluster的所有Node需要配置root用户的SSH免密登录；" class="headerlink" title="1. Heketi所在Node到GlusterFS Cluster的所有Node需要配置root用户的SSH免密登录；"></a>1. Heketi所在Node到GlusterFS Cluster的所有Node需要配置root用户的SSH免密登录；</h2><h2 id="2-etc-heketi-heketi-json-中需要修改配置，不能不做任何修改就拿来直接使用；"><a href="#2-etc-heketi-heketi-json-中需要修改配置，不能不做任何修改就拿来直接使用；" class="headerlink" title="2. /etc/heketi/heketi.json 中需要修改配置，不能不做任何修改就拿来直接使用；"></a>2. /etc/heketi/heketi.json 中需要修改配置，不能不做任何修改就拿来直接使用；</h2><p>（1）安全的做法是需要开启jwt认证，两个key字段分别用来设置admin用户和user用户的访问密码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">  &quot;use_auth&quot;: true,</span><br><span class="line"></span><br><span class="line">  &quot;_jwt&quot;: &quot;Private keys for access&quot;,</span><br><span class="line">  &quot;jwt&quot;: &#123;</span><br><span class="line">    &quot;_admin&quot;: &quot;Admin has access to all APIs&quot;,</span><br><span class="line">    &quot;admin&quot;: &#123;</span><br><span class="line">      &quot;key&quot;: &quot;12345678&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;_user&quot;: &quot;User only has access to /volumes endpoint&quot;,</span><br><span class="line">    &quot;user&quot;: &#123;</span><br><span class="line">      &quot;key&quot;: &quot;12345678&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>（2）需要把executor修改为ssh，不能使用默认的mock：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">    &quot;executor&quot;: &quot;ssh&quot;,</span><br><span class="line"></span><br><span class="line">    &quot;sshexec&quot;: &#123;</span><br><span class="line">      &quot;keyfile&quot;: &quot;/root/.ssh/id_rsa&quot;,</span><br><span class="line">      &quot;user&quot;: &quot;root&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>（3）json文件中有些配置想使用默认值，需要注释掉或者删除掉，不然无法启动Heketi，因为这些字段的value部分写得是字段的使用说明而非默认值。</p>
<h2 id="3-lib-systemd-system-heketi-service-中的User值应该为root，不修改的话默认为heketi，无法成功启动。"><a href="#3-lib-systemd-system-heketi-service-中的User值应该为root，不修改的话默认为heketi，无法成功启动。" class="headerlink" title="3. /lib/systemd/system/heketi.service 中的User值应该为root，不修改的话默认为heketi，无法成功启动。"></a>3. /lib/systemd/system/heketi.service 中的User值应该为root，不修改的话默认为heketi，无法成功启动。</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">User=root</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h1 id="三、完整的实现过程记录"><a href="#三、完整的实现过程记录" class="headerlink" title="三、完整的实现过程记录"></a>三、完整的实现过程记录</h1><h2 id="1-Kubernetes集群的所有节点上（既包括Master也包括Node）上都执行："><a href="#1-Kubernetes集群的所有节点上（既包括Master也包括Node）上都执行：" class="headerlink" title="1. Kubernetes集群的所有节点上（既包括Master也包括Node）上都执行："></a>1. Kubernetes集群的所有节点上（既包括Master也包括Node）上都执行：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 安装GlusterFS的yum源</span><br><span class="line">yum install -y centos-release-gluster</span><br><span class="line"># 安装挂载GlusterFS Volume所需要的驱动</span><br><span class="line">yum install -y glusterfs glusterfs-fuse</span><br></pre></td></tr></table></figure>
<h2 id="2-在Kubernetes集群外部安装GlusterFS集群："><a href="#2-在Kubernetes集群外部安装GlusterFS集群：" class="headerlink" title="2. 在Kubernetes集群外部安装GlusterFS集群："></a>2. 在Kubernetes集群外部安装GlusterFS集群：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"># 在server07、server08和server09三台服务器上执行</span><br><span class="line">yum update -y</span><br><span class="line">setenforce 0</span><br><span class="line">systemctl disable firewalld.service</span><br><span class="line">systemctl stop firewalld.service</span><br><span class="line"></span><br><span class="line">systemctl disable NetworkManager.service</span><br><span class="line">systemctl stop NetworkManager.service</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; /etc/hosts</span><br><span class="line"></span><br><span class="line"># For GlusterFS Cluster</span><br><span class="line">172.16.170.134 server07</span><br><span class="line">172.16.170.135 server08</span><br><span class="line">172.16.170.136 server09</span><br><span class="line"></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">yum install -y centos-release-gluster</span><br><span class="line">yum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma glusterfs-geo-replication glusterfs-devel</span><br><span class="line">mkdir /opt/glusterd</span><br><span class="line">sed -i &apos;s/var\/lib/opt/g&apos; /etc/glusterfs/glusterd.vol</span><br><span class="line">systemctl enable glusterd.service</span><br><span class="line">systemctl start glusterd.service</span><br><span class="line">systemctl status glusterd.service</span><br><span class="line"></span><br><span class="line"># 在server07服务器上执行</span><br><span class="line">yum install -y heketi heketi-client</span><br><span class="line">ssh-keygen</span><br><span class="line">ssh-copy-id root@server07</span><br><span class="line">ssh-copy-id root@server08</span><br><span class="line">ssh-copy-id root@server09</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/heketi/heketi.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;_port_comment&quot;: &quot;Heketi Server Port Number&quot;,</span><br><span class="line">  &quot;port&quot;: &quot;8080&quot;,</span><br><span class="line"></span><br><span class="line">  &quot;_use_auth&quot;: &quot;Enable JWT authorization. Please enable for deployment&quot;,</span><br><span class="line">  &quot;use_auth&quot;: true,</span><br><span class="line"></span><br><span class="line">  &quot;_jwt&quot;: &quot;Private keys for access&quot;,</span><br><span class="line">  &quot;jwt&quot;: &#123;</span><br><span class="line">    &quot;_admin&quot;: &quot;Admin has access to all APIs&quot;,</span><br><span class="line">    &quot;admin&quot;: &#123;</span><br><span class="line">      &quot;key&quot;: &quot;12345678&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;_user&quot;: &quot;User only has access to /volumes endpoint&quot;,</span><br><span class="line">    &quot;user&quot;: &#123;</span><br><span class="line">      &quot;key&quot;: &quot;12345678&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  &quot;_glusterfs_comment&quot;: &quot;GlusterFS Configuration&quot;,</span><br><span class="line">  &quot;glusterfs&quot;: &#123;</span><br><span class="line">    &quot;_executor_comment&quot;: [</span><br><span class="line">      &quot;Execute plugin. Possible choices: mock, ssh&quot;,</span><br><span class="line">      &quot;mock: This setting is used for testing and development.&quot;,</span><br><span class="line">      &quot;      It will not send commands to any node.&quot;,</span><br><span class="line">      &quot;ssh:  This setting will notify Heketi to ssh to the nodes.&quot;,</span><br><span class="line">      &quot;      It will need the values in sshexec to be configured.&quot;,</span><br><span class="line">      &quot;kubernetes: Communicate with GlusterFS containers over&quot;,</span><br><span class="line">      &quot;            Kubernetes exec api.&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;executor&quot;: &quot;ssh&quot;,</span><br><span class="line"></span><br><span class="line">    &quot;sshexec&quot;: &#123;</span><br><span class="line">      &quot;keyfile&quot;: &quot;/root/.ssh/id_rsa&quot;,</span><br><span class="line">      &quot;user&quot;: &quot;root&quot;</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    &quot;_kubeexec_comment&quot;: &quot;Kubernetes configuration&quot;,</span><br><span class="line">    &quot;kubeexec&quot;: &#123;</span><br><span class="line">      &quot;host&quot; :&quot;https://kubernetes.host:8443&quot;,</span><br><span class="line">      &quot;cert&quot; : &quot;/path/to/crt.file&quot;,</span><br><span class="line">      &quot;insecure&quot;: false,</span><br><span class="line">      &quot;user&quot;: &quot;kubernetes username&quot;,</span><br><span class="line">      &quot;password&quot;: &quot;password for kubernetes user&quot;,</span><br><span class="line">      &quot;namespace&quot;: &quot;OpenShift project or Kubernetes namespace&quot;,</span><br><span class="line">      &quot;fstab&quot;: &quot;Optional: Specify fstab file on node.  Default is /etc/fstab&quot;</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    &quot;_db_comment&quot;: &quot;Database file name&quot;,</span><br><span class="line">    &quot;db&quot;: &quot;/var/lib/heketi/heketi.db&quot;,</span><br><span class="line"></span><br><span class="line">    &quot;_loglevel_comment&quot;: [</span><br><span class="line">      &quot;Set log level. Choices are:&quot;,</span><br><span class="line">      &quot;  none, critical, error, warning, info, debug&quot;,</span><br><span class="line">      &quot;Default is warning&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;loglevel&quot; : &quot;debug&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /lib/systemd/system/heketi.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Heketi Server</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=simple</span><br><span class="line">WorkingDirectory=/var/lib/heketi</span><br><span class="line">User=root</span><br><span class="line">ExecStart=/usr/bin/heketi --config=/etc/heketi/heketi.json</span><br><span class="line">Restart=on-failure</span><br><span class="line">StandardOutput=syslog</span><br><span class="line">StandardError=syslog</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable heketi.service</span><br><span class="line">systemctl start heketi.service</span><br><span class="line">systemctl status heketi.service</span><br><span class="line"></span><br><span class="line">curl -XGET http://localhost:8080/hello</span><br><span class="line">Hello from Heketi</span><br></pre></td></tr></table></figure>
<h2 id="3-使用Heketi初始化GlusterFS集群："><a href="#3-使用Heketi初始化GlusterFS集群：" class="headerlink" title="3. 使用Heketi初始化GlusterFS集群："></a>3. 使用Heketi初始化GlusterFS集群：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; /etc/heketi/topology.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;clusters&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;nodes&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;node&quot;: &#123;</span><br><span class="line">            &quot;hostnames&quot;: &#123;</span><br><span class="line">              &quot;manage&quot;: [</span><br><span class="line">                &quot;server07&quot;</span><br><span class="line">              ],</span><br><span class="line">              &quot;storage&quot;: [</span><br><span class="line">                &quot;172.16.170.134&quot;</span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;zone&quot;: 1</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;devices&quot;: [</span><br><span class="line">            &quot;/dev/sdb&quot;</span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;node&quot;: &#123;</span><br><span class="line">            &quot;hostnames&quot;: &#123;</span><br><span class="line">              &quot;manage&quot;: [</span><br><span class="line">                &quot;server08&quot;</span><br><span class="line">              ],</span><br><span class="line">              &quot;storage&quot;: [</span><br><span class="line">                &quot;172.16.170.135&quot;</span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;zone&quot;: 1</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;devices&quot;: [</span><br><span class="line">            &quot;/dev/sdb&quot;</span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;node&quot;: &#123;</span><br><span class="line">            &quot;hostnames&quot;: &#123;</span><br><span class="line">              &quot;manage&quot;: [</span><br><span class="line">                &quot;server09&quot;</span><br><span class="line">              ],</span><br><span class="line">              &quot;storage&quot;: [</span><br><span class="line">                &quot;172.16.170.136&quot;</span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;zone&quot;: 1</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;devices&quot;: [</span><br><span class="line">            &quot;/dev/sdb&quot;</span><br><span class="line">          ]</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">heketi-cli --user admin --secret 12345678 topology load --json=/etc/heketi/topology.json</span><br><span class="line">Creating cluster ... ID: 4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line">	Allowing file volumes on cluster.</span><br><span class="line">	Allowing block volumes on cluster.</span><br><span class="line">	Creating node server07 ... ID: 2e8b83add06cde8713d56ecb7d424033</span><br><span class="line">		Adding device /dev/sdb ... OK</span><br><span class="line">	Creating node server08 ... ID: 56a23b0579e7f79511843e046e69008f</span><br><span class="line">		Adding device /dev/sdb ... OK</span><br><span class="line">	Creating node server09 ... ID: 9fb13189ba78ef302046fe4414f633bf</span><br><span class="line">		Adding device /dev/sdb ... OK</span><br><span class="line"></span><br><span class="line">heketi-cli --user admin --secret 12345678 cluster list</span><br><span class="line">Clusters:</span><br><span class="line">Id:4e01ca44d2e6f0077ee3abe9c6783183 [file][block]</span><br><span class="line"></span><br><span class="line">heketi-cli --user admin --secret 12345678 --json=true node list</span><br><span class="line">Id:2e8b83add06cde8713d56ecb7d424033	Cluster:4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line">Id:56a23b0579e7f79511843e046e69008f	Cluster:4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line">Id:9fb13189ba78ef302046fe4414f633bf	Cluster:4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">heketi-cli --user admin --secret 12345678 topology info</span><br><span class="line"></span><br><span class="line">Cluster Id: 4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line"></span><br><span class="line">    File:  true</span><br><span class="line">    Block: true</span><br><span class="line"></span><br><span class="line">    Volumes:</span><br><span class="line"></span><br><span class="line">    Nodes:</span><br><span class="line"></span><br><span class="line">	Node Id: 2e8b83add06cde8713d56ecb7d424033</span><br><span class="line">	State: online</span><br><span class="line">	Cluster Id: 4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line">	Zone: 1</span><br><span class="line">	Management Hostnames: server07</span><br><span class="line">	Storage Hostnames: 172.16.170.134</span><br><span class="line">	Devices:</span><br><span class="line">		Id:4995671aaef70cb1f640d0f411e94d2f   Name:/dev/sdb            State:online    Size (GiB):19      Used (GiB):0       Free (GiB):19</span><br><span class="line">			Bricks:</span><br><span class="line"></span><br><span class="line">	Node Id: 56a23b0579e7f79511843e046e69008f</span><br><span class="line">	State: online</span><br><span class="line">	Cluster Id: 4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line">	Zone: 1</span><br><span class="line">	Management Hostnames: server08</span><br><span class="line">	Storage Hostnames: 172.16.170.135</span><br><span class="line">	Devices:</span><br><span class="line">		Id:c4a56208419516d4fbc437d9dc3b265e   Name:/dev/sdb            State:online    Size (GiB):19      Used (GiB):0       Free (GiB):19</span><br><span class="line">			Bricks:</span><br><span class="line"></span><br><span class="line">	Node Id: 9fb13189ba78ef302046fe4414f633bf</span><br><span class="line">	State: online</span><br><span class="line">	Cluster Id: 4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line">	Zone: 1</span><br><span class="line">	Management Hostnames: server09</span><br><span class="line">	Storage Hostnames: 172.16.170.136</span><br><span class="line">	Devices:</span><br><span class="line">		Id:be2a9558f0634233be72f0c55d051898   Name:/dev/sdb            State:online    Size (GiB):19      Used (GiB):0       Free (GiB):19</span><br><span class="line">			Bricks:</span><br></pre></td></tr></table></figure>
<h2 id="4-编写验证Heketi和GlusterFS集群对接Kubernetes集群的相关YAML："><a href="#4-编写验证Heketi和GlusterFS集群对接Kubernetes集群的相关YAML：" class="headerlink" title="4. 编写验证Heketi和GlusterFS集群对接Kubernetes集群的相关YAML："></a>4. 编写验证Heketi和GlusterFS集群对接Kubernetes集群的相关YAML：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p glusterfs/</span><br><span class="line">cd glusterfs/</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 01-heketi-secret.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: heketi-secret</span><br><span class="line">  namespace: default</span><br><span class="line">data:</span><br><span class="line">  # base64 encoded password. E.g.: echo -n &quot;mypassword&quot; | base64</span><br><span class="line">  key: MTIzNDU2Nzg=</span><br><span class="line">type: kubernetes.io/glusterfs</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 02-glusterfs-storageclass.yaml</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: glusterfs</span><br><span class="line">provisioner: kubernetes.io/glusterfs</span><br><span class="line">parameters:</span><br><span class="line">  resturl: &quot;http://172.16.170.134:8080&quot;</span><br><span class="line">  clusterid: &quot;4e01ca44d2e6f0077ee3abe9c6783183&quot;</span><br><span class="line">  restauthenabled: &quot;true&quot;</span><br><span class="line">  restuser: &quot;admin&quot;</span><br><span class="line">  secretNamespace: &quot;default&quot;</span><br><span class="line">  secretName: &quot;heketi-secret&quot;</span><br><span class="line">  volumetype: &quot;replicate:3&quot;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 03-persistentvolumeclaim.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: myclaim</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br><span class="line">  storageClassName: glusterfs</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 04-pod.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: alpine</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - image: alpine:3.8</span><br><span class="line">    command:</span><br><span class="line">    - sleep</span><br><span class="line">    - &quot;3600&quot;</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    name: alpine</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: myvolume</span><br><span class="line">      mountPath: &quot;/data&quot;</span><br><span class="line">  restartPolicy: Always</span><br><span class="line">  volumes:</span><br><span class="line">  - name: myvolume</span><br><span class="line">    persistentVolumeClaim:</span><br><span class="line">      claimName: myclaim</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="5-验证Heketi和GlusterFS集群对接Kubernetes集群："><a href="#5-验证Heketi和GlusterFS集群对接Kubernetes集群：" class="headerlink" title="5. 验证Heketi和GlusterFS集群对接Kubernetes集群："></a>5. 验证Heketi和GlusterFS集群对接Kubernetes集群：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"># 创建StorageClass对接到GlusterFS</span><br><span class="line">kubectl create -f 01-heketi-secret.yaml</span><br><span class="line">secret/heketi-secret created</span><br><span class="line"></span><br><span class="line">kubectl create -f 02-glusterfs-storageclass.yaml</span><br><span class="line">storageclass.storage.k8s.io/glusterfs created</span><br><span class="line"></span><br><span class="line"># 创建PVC使用上面的StorageClass</span><br><span class="line">kubectl create -f 03-persistentvolumeclaim.yaml</span><br><span class="line">persistentvolumeclaim/myclaim created</span><br><span class="line"></span><br><span class="line"># 验证PVC动态供应PV</span><br><span class="line">kubectl get pvc -o wide</span><br><span class="line">NAME      STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">myclaim   Bound     pvc-9d5a1b14-2456-11e9-a588-000c2921eba0   1Gi        RWX            glusterfs      40s</span><br><span class="line"></span><br><span class="line">kubectl get pv -o wide</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM             STORAGECLASS   REASON    AGE</span><br><span class="line">pvc-9d5a1b14-2456-11e9-a588-000c2921eba0   1Gi        RWX            Delete           Bound     default/myclaim   glusterfs                40s</span><br><span class="line"></span><br><span class="line"># 创建测试Pod使用上面的PVC</span><br><span class="line">kubectl create -f 04-pod.yaml</span><br><span class="line">pod/alpine created</span><br><span class="line"></span><br><span class="line">kubectl get pod -o wide</span><br><span class="line">NAME      READY     STATUS    RESTARTS   AGE       IP           NODE</span><br><span class="line">alpine    1/1       Running   0          17s       10.211.1.2   server02</span><br><span class="line"></span><br><span class="line"># 验证PVC是否挂载到Pod中</span><br><span class="line">kubectl exec -it alpine /bin/sh</span><br><span class="line">/ # df -h</span><br><span class="line">Filesystem                Size      Used Available Use% Mounted on</span><br><span class="line">overlay                  17.0G      2.2G     14.8G  13% /</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /dev</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /sys/fs/cgroup</span><br><span class="line">172.16.170.134:vol_b46e22b816b18672ecb5a8c3289b0193</span><br><span class="line">                       1014.0M     42.7M    971.3M   4% /data</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /dev/termination-log</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /etc/resolv.conf</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /etc/hostname</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /etc/hosts</span><br><span class="line">shm                      64.0M         0     64.0M   0% /dev/shm</span><br><span class="line">tmpfs                   911.8M     12.0K    911.8M   0% /run/secrets/kubernetes.io/serviceaccount</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/kcore</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/timer_list</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/timer_stats</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/sched_debug</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /sys/firmware</span><br><span class="line"></span><br><span class="line"># 创建测试文件，验证存储容量的限制</span><br><span class="line">/data # fallocate -l 971M onebox.disk</span><br><span class="line">/data # df -h</span><br><span class="line">Filesystem                Size      Used Available Use% Mounted on</span><br><span class="line">overlay                  17.0G      2.2G     14.8G  13% /</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /dev</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /sys/fs/cgroup</span><br><span class="line">172.16.170.134:vol_b46e22b816b18672ecb5a8c3289b0193</span><br><span class="line">                       1014.0M   1013.7M    348.0K 100% /data</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /dev/termination-log</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /etc/resolv.conf</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /etc/hostname</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /etc/hosts</span><br><span class="line">shm                      64.0M         0     64.0M   0% /dev/shm</span><br><span class="line">tmpfs                   911.8M     12.0K    911.8M   0% /run/secrets/kubernetes.io/serviceaccount</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/kcore</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/timer_list</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/timer_stats</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/sched_debug</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /sys/firmware</span><br><span class="line">/data # fallocate -l 971M onebox.disk</span><br><span class="line">fallocate: fallocate &apos;onebox.disk&apos;: No space left on device</span><br><span class="line"></span><br><span class="line"># 测试文件删除后，验证容量是否恢复</span><br><span class="line">/data # rm -rf onebox.disk</span><br><span class="line">/data # df -h</span><br><span class="line">Filesystem                Size      Used Available Use% Mounted on</span><br><span class="line">overlay                  17.0G      2.2G     14.8G  13% /</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /dev</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /sys/fs/cgroup</span><br><span class="line">172.16.170.134:vol_b46e22b816b18672ecb5a8c3289b0193</span><br><span class="line">                       1014.0M     42.7M    971.3M   4% /data</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /dev/termination-log</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /etc/resolv.conf</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /etc/hostname</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /etc/hosts</span><br><span class="line">shm                      64.0M         0     64.0M   0% /dev/shm</span><br><span class="line">tmpfs                   911.8M     12.0K    911.8M   0% /run/secrets/kubernetes.io/serviceaccount</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/kcore</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/timer_list</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/timer_stats</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/sched_debug</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /sys/firmware</span><br></pre></td></tr></table></figure>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GlusterFS/">GlusterFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Storage/">Storage</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kubernetes_glusterfs_000" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/01/29/kubernetes_glusterfs_000/" class="article-date">
  	<time datetime="2019-01-29T06:09:40.631Z" itemprop="datePublished">2019-01-29</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/29/kubernetes_glusterfs_000/">
        Kubernetes对接GlusterFS（把GlusterFS部署在Kubernetes集群内）
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、环境版本信息"><a href="#一、环境版本信息" class="headerlink" title="一、环境版本信息"></a>一、环境版本信息</h1><h2 id="1-版本信息"><a href="#1-版本信息" class="headerlink" title="1. 版本信息"></a>1. 版本信息</h2><p>CentOS 7.6.1810<br>Kubernetes v1.11.0<br>GlusterFS gluster3u13_centos7<br>Heketi 4</p>
<h2 id="2-服务器信息"><a href="#2-服务器信息" class="headerlink" title="2. 服务器信息"></a>2. 服务器信息</h2><p>server03 <-> 172.16.170.130 <-> kubernetes master<br>server07 <-> 172.16.170.134 <-> kubernetes node<br>server08 <-> 172.16.170.135 <-> kubernetes node<br>server09 <-> 172.16.170.136 <-> kubernetes node</-></-></-></-></-></-></-></-></p>
<h1 id="二、相关注意事项"><a href="#二、相关注意事项" class="headerlink" title="二、相关注意事项"></a>二、相关注意事项</h1><h2 id="1-不要使用链接-https-github-com-gluster-gluster-kubernetes-tree-v1-1-中的部署脚本；"><a href="#1-不要使用链接-https-github-com-gluster-gluster-kubernetes-tree-v1-1-中的部署脚本；" class="headerlink" title="1. 不要使用链接 https://github.com/gluster/gluster-kubernetes/tree/v1.1 中的部署脚本；"></a>1. 不要使用链接 <a href="https://github.com/gluster/gluster-kubernetes/tree/v1.1" target="_blank" rel="noopener">https://github.com/gluster/gluster-kubernetes/tree/v1.1</a> 中的部署脚本；</h2><p>笔者最开始用了，这个过程中还参考了网上很多的帖子，可谓是吃尽了苦头。。。。。。</p>
<h2 id="2-Heketi只有ServiceAccount是不够用的，需要给这个ServiceAccount创建ClusterRoleBinding对象，为其分配比较高的API访问权限；"><a href="#2-Heketi只有ServiceAccount是不够用的，需要给这个ServiceAccount创建ClusterRoleBinding对象，为其分配比较高的API访问权限；" class="headerlink" title="2. Heketi只有ServiceAccount是不够用的，需要给这个ServiceAccount创建ClusterRoleBinding对象，为其分配比较高的API访问权限；"></a>2. Heketi只有ServiceAccount是不够用的，需要给这个ServiceAccount创建ClusterRoleBinding对象，为其分配比较高的API访问权限；</h2><p>比如说笔者这里就简单处理，为其分配cluster-admin权限。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: heketi</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: heketi</span><br><span class="line">  namespace: storage</span><br></pre></td></tr></table></figure></p>
<h2 id="3-所有宿主机都要执行modprobe-dm-thin-pool，否则创建volume会报错；"><a href="#3-所有宿主机都要执行modprobe-dm-thin-pool，否则创建volume会报错；" class="headerlink" title="3. 所有宿主机都要执行modprobe dm_thin_pool，否则创建volume会报错；"></a>3. 所有宿主机都要执行modprobe dm_thin_pool，否则创建volume会报错；</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">modprobe dm_thin_pool</span><br></pre></td></tr></table></figure>
<h2 id="4-GlusterFS和Heketi的Docker镜像不要像社区代码中那样使用latest和dev这样镜像内容会随时更新的标签，建议选择内容固定的标签对应的镜像。"><a href="#4-GlusterFS和Heketi的Docker镜像不要像社区代码中那样使用latest和dev这样镜像内容会随时更新的标签，建议选择内容固定的标签对应的镜像。" class="headerlink" title="4. GlusterFS和Heketi的Docker镜像不要像社区代码中那样使用latest和dev这样镜像内容会随时更新的标签，建议选择内容固定的标签对应的镜像。"></a>4. GlusterFS和Heketi的Docker镜像不要像社区代码中那样使用latest和dev这样镜像内容会随时更新的标签，建议选择内容固定的标签对应的镜像。</h2><p>笔者选择的Docker镜像如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gluster/gluster-centos:gluster3u13_centos7</span><br><span class="line">heketi/heketi:4</span><br></pre></td></tr></table></figure></p>
<h1 id="三、完整的实现过程记录"><a href="#三、完整的实现过程记录" class="headerlink" title="三、完整的实现过程记录"></a>三、完整的实现过程记录</h1><h2 id="1-Kubernetes集群的所有节点上（既包括Master也包括Node）上都执行："><a href="#1-Kubernetes集群的所有节点上（既包括Master也包括Node）上都执行：" class="headerlink" title="1. Kubernetes集群的所有节点上（既包括Master也包括Node）上都执行："></a>1. Kubernetes集群的所有节点上（既包括Master也包括Node）上都执行：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 安装GlusterFS的yum源</span><br><span class="line">yum install -y centos-release-gluster</span><br><span class="line"># 安装挂载GlusterFS Volume所需要的驱动</span><br><span class="line">yum install -y glusterfs glusterfs-fuse</span><br><span class="line"># 开启Linux内核的 device-mapper target(s) 支持</span><br><span class="line">modprobe dm_thin_pool</span><br></pre></td></tr></table></figure>
<h2 id="2-参考社区的YAML，编写部署GlusterFS的YAML："><a href="#2-参考社区的YAML，编写部署GlusterFS的YAML：" class="headerlink" title="2. 参考社区的YAML，编写部署GlusterFS的YAML："></a>2. 参考社区的YAML，编写部署GlusterFS的YAML：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p gluster-kubernetes/deploy/gluster-kubernetes/</span><br><span class="line">cd gluster-kubernetes/deploy/gluster-kubernetes/</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 01-common-namespace.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: storage</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 02-glusterfs-daemonset.yaml</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: glusterfs</span><br><span class="line">  namespace: storage</span><br><span class="line">  labels:</span><br><span class="line">    glusterfs: daemonset</span><br><span class="line">  annotations:</span><br><span class="line">    description: GlusterFS DaemonSet</span><br><span class="line">    tags: glusterfs</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: glusterfs</span><br><span class="line">      labels:</span><br><span class="line">        glusterfs-node: pod</span><br><span class="line">    spec:</span><br><span class="line">      nodeSelector:</span><br><span class="line">        storagenode: glusterfs</span><br><span class="line">      hostNetwork: true</span><br><span class="line">      containers:</span><br><span class="line">      - image: gluster/gluster-centos:gluster3u13_centos7</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        name: glusterfs</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: glusterfs-heketi</span><br><span class="line">          mountPath: &quot;/var/lib/heketi&quot;</span><br><span class="line">        - name: glusterfs-run</span><br><span class="line">          mountPath: &quot;/run&quot;</span><br><span class="line">        - name: glusterfs-lvm</span><br><span class="line">          mountPath: &quot;/run/lvm&quot;</span><br><span class="line">        - name: glusterfs-etc</span><br><span class="line">          mountPath: &quot;/etc/glusterfs&quot;</span><br><span class="line">        - name: glusterfs-logs</span><br><span class="line">          mountPath: &quot;/var/log/glusterfs&quot;</span><br><span class="line">        - name: glusterfs-config</span><br><span class="line">          mountPath: &quot;/var/lib/glusterd&quot;</span><br><span class="line">        - name: glusterfs-dev</span><br><span class="line">          mountPath: &quot;/dev&quot;</span><br><span class="line">        - name: glusterfs-misc</span><br><span class="line">          mountPath: &quot;/var/lib/misc/glusterfsd&quot;</span><br><span class="line">        - name: glusterfs-cgroup</span><br><span class="line">          mountPath: &quot;/sys/fs/cgroup&quot;</span><br><span class="line">          readOnly: true</span><br><span class="line">        - name: glusterfs-ssl</span><br><span class="line">          mountPath: &quot;/etc/ssl&quot;</span><br><span class="line">          readOnly: true</span><br><span class="line">        securityContext:</span><br><span class="line">          capabilities: &#123;&#125;</span><br><span class="line">          privileged: true</span><br><span class="line">        readinessProbe:</span><br><span class="line">          timeoutSeconds: 3</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          exec:</span><br><span class="line">            command:</span><br><span class="line">            - &quot;/bin/bash&quot;</span><br><span class="line">            - &quot;-c&quot;</span><br><span class="line">            - systemctl status glusterd.service</span><br><span class="line">        livenessProbe:</span><br><span class="line">          timeoutSeconds: 3</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          exec:</span><br><span class="line">            command:</span><br><span class="line">            - &quot;/bin/bash&quot;</span><br><span class="line">            - &quot;-c&quot;</span><br><span class="line">            - systemctl status glusterd.service</span><br><span class="line">      volumes:</span><br><span class="line">      - name: glusterfs-heketi</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &quot;/var/lib/heketi&quot;</span><br><span class="line">      - name: glusterfs-run</span><br><span class="line">      - name: glusterfs-lvm</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &quot;/run/lvm&quot;</span><br><span class="line">      - name: glusterfs-etc</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &quot;/etc/glusterfs&quot;</span><br><span class="line">      - name: glusterfs-logs</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &quot;/var/log/glusterfs&quot;</span><br><span class="line">      - name: glusterfs-config</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &quot;/var/lib/glusterd&quot;</span><br><span class="line">      - name: glusterfs-dev</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &quot;/dev&quot;</span><br><span class="line">      - name: glusterfs-misc</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &quot;/var/lib/misc/glusterfsd&quot;</span><br><span class="line">      - name: glusterfs-cgroup</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &quot;/sys/fs/cgroup&quot;</span><br><span class="line">      - name: glusterfs-ssl</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &quot;/etc/ssl&quot;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 03-heketi-serviceaccount.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: heketi</span><br><span class="line">  namespace: storage</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 04-heketi-clusterrolebinding.yaml</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: heketi</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: heketi</span><br><span class="line">  namespace: storage</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 05-heketi-deployment.yaml</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: heketi</span><br><span class="line">  namespace: storage</span><br><span class="line">  labels:</span><br><span class="line">    glusterfs: heketi-deployment</span><br><span class="line">    deploy-heketi: heketi-deployment</span><br><span class="line">  annotations:</span><br><span class="line">    description: Defines how to deploy Heketi</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: heketi</span><br><span class="line">      labels:</span><br><span class="line">        name: deploy-heketi</span><br><span class="line">        glusterfs: heketi-pod</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: heketi</span><br><span class="line">      containers:</span><br><span class="line">      - image: heketi/heketi:4</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        name: heketi</span><br><span class="line">        env:</span><br><span class="line">        - name: HEKETI_EXECUTOR</span><br><span class="line">          value: kubernetes</span><br><span class="line">        - name: HEKETI_FSTAB</span><br><span class="line">          value: &quot;/var/lib/heketi/fstab&quot;</span><br><span class="line">        - name: HEKETI_SNAPSHOT_LIMIT</span><br><span class="line">          value: &apos;14&apos;</span><br><span class="line">        - name: HEKETI_KUBE_GLUSTER_DAEMONSET</span><br><span class="line">          value: &quot;y&quot;</span><br><span class="line">        - name: HEKETI_CLI_SERVER</span><br><span class="line">          value: &quot;http://127.0.0.1:8080&quot;</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: db</span><br><span class="line">          mountPath: &quot;/var/lib/heketi&quot;</span><br><span class="line">        readinessProbe:</span><br><span class="line">          timeoutSeconds: 3</span><br><span class="line">          initialDelaySeconds: 3</span><br><span class="line">          httpGet:</span><br><span class="line">            path: &quot;/hello&quot;</span><br><span class="line">            port: 8080</span><br><span class="line">        livenessProbe:</span><br><span class="line">          timeoutSeconds: 3</span><br><span class="line">          initialDelaySeconds: 30</span><br><span class="line">          httpGet:</span><br><span class="line">            path: &quot;/hello&quot;</span><br><span class="line">            port: 8080</span><br><span class="line">      volumes:</span><br><span class="line">      - name: db</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 06-heketi-service.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: heketi</span><br><span class="line">  namespace: storage</span><br><span class="line">  labels:</span><br><span class="line">    glusterfs: heketi-service</span><br><span class="line">    deploy-heketi: support</span><br><span class="line">  annotations:</span><br><span class="line">    description: Exposes Heketi Service</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    name: deploy-heketi</span><br><span class="line">  ports:</span><br><span class="line">  - name: deploy-heketi</span><br><span class="line">    port: 8080</span><br><span class="line">    targetPort: 8080</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="3-在Kubernetes集群上创建Heketi和GlusterFS集群："><a href="#3-在Kubernetes集群上创建Heketi和GlusterFS集群：" class="headerlink" title="3. 在Kubernetes集群上创建Heketi和GlusterFS集群："></a>3. 在Kubernetes集群上创建Heketi和GlusterFS集群：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cd ../../</span><br><span class="line">kubectl create -f deploy/gluster-kubernetes/</span><br><span class="line"></span><br><span class="line">kubectl get pod -n storage -o wide</span><br><span class="line">NAME                      READY     STATUS    RESTARTS   AGE       IP               NODE</span><br><span class="line">glusterfs-6shxs           1/1       Running   0          1m        172.16.170.135   server08</span><br><span class="line">glusterfs-9vpjg           1/1       Running   0          1m        172.16.170.136   server09</span><br><span class="line">glusterfs-pmq2n           1/1       Running   0          1m        172.16.170.134   server07</span><br><span class="line">heketi-5bdc8f9db5-nmjrw   1/1       Running   0          1m        10.211.1.12      server07</span><br></pre></td></tr></table></figure>
<h2 id="4-使用Heketi初始化GlusterFS集群："><a href="#4-使用Heketi初始化GlusterFS集群：" class="headerlink" title="4. 使用Heketi初始化GlusterFS集群："></a>4. 使用Heketi初始化GlusterFS集群：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; topology.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;clusters&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;nodes&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;node&quot;: &#123;</span><br><span class="line">            &quot;hostnames&quot;: &#123;</span><br><span class="line">              &quot;manage&quot;: [</span><br><span class="line">                &quot;server07&quot;</span><br><span class="line">              ],</span><br><span class="line">              &quot;storage&quot;: [</span><br><span class="line">                &quot;172.16.170.134&quot;</span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;zone&quot;: 1</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;devices&quot;: [</span><br><span class="line">            &quot;/dev/sdb&quot;</span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;node&quot;: &#123;</span><br><span class="line">            &quot;hostnames&quot;: &#123;</span><br><span class="line">              &quot;manage&quot;: [</span><br><span class="line">                &quot;server08&quot;</span><br><span class="line">              ],</span><br><span class="line">              &quot;storage&quot;: [</span><br><span class="line">                &quot;172.16.170.135&quot;</span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;zone&quot;: 1</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;devices&quot;: [</span><br><span class="line">            &quot;/dev/sdb&quot;</span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;node&quot;: &#123;</span><br><span class="line">            &quot;hostnames&quot;: &#123;</span><br><span class="line">              &quot;manage&quot;: [</span><br><span class="line">                &quot;server09&quot;</span><br><span class="line">              ],</span><br><span class="line">              &quot;storage&quot;: [</span><br><span class="line">                &quot;172.16.170.136&quot;</span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;zone&quot;: 1</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;devices&quot;: [</span><br><span class="line">            &quot;/dev/sdb&quot;</span><br><span class="line">          ]</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl cp topology.json heketi-5bdc8f9db5-nmjrw:/ -n storage</span><br><span class="line"></span><br><span class="line">kubectl exec -it heketi-5bdc8f9db5-nmjrw /bin/bash -n storage</span><br><span class="line">[root@heketi-5bdc8f9db5-nmjrw /]# heketi-cli topology load --json=topology.json</span><br><span class="line">Creating cluster ... ID: dde6f33a6cb852e04cb0685e210d3959</span><br><span class="line">	Creating node server07 ... ID: 2818eb53b31234c2cc852b339538d856</span><br><span class="line">		Adding device /dev/sdb ... OK</span><br><span class="line">	Creating node server08 ... ID: d5773e74abd6f3041d07b095a2f4ae99</span><br><span class="line">		Adding device /dev/sdb ... OK</span><br><span class="line">	Creating node server09 ... ID: 4caafd29101b77da4a6cd265dad65140</span><br><span class="line">		Adding device /dev/sdb ... OK</span><br><span class="line"></span><br><span class="line">heketi-cli topology info</span><br><span class="line"></span><br><span class="line">Cluster Id: dde6f33a6cb852e04cb0685e210d3959</span><br><span class="line"></span><br><span class="line">    Volumes:</span><br><span class="line"></span><br><span class="line">    Nodes:</span><br><span class="line"></span><br><span class="line">	Node Id: 2818eb53b31234c2cc852b339538d856</span><br><span class="line">	State: online</span><br><span class="line">	Cluster Id: dde6f33a6cb852e04cb0685e210d3959</span><br><span class="line">	Zone: 1</span><br><span class="line">	Management Hostname: server07</span><br><span class="line">	Storage Hostname: 172.16.170.134</span><br><span class="line">	Devices:</span><br><span class="line">		Id:25bbec25514b711d48d0abef98809310   Name:/dev/sdb            State:online    Size (GiB):19      Used (GiB):0       Free (GiB):19</span><br><span class="line">			Bricks:</span><br><span class="line"></span><br><span class="line">	Node Id: 4caafd29101b77da4a6cd265dad65140</span><br><span class="line">	State: online</span><br><span class="line">	Cluster Id: dde6f33a6cb852e04cb0685e210d3959</span><br><span class="line">	Zone: 1</span><br><span class="line">	Management Hostname: server09</span><br><span class="line">	Storage Hostname: 172.16.170.136</span><br><span class="line">	Devices:</span><br><span class="line">		Id:78385691f170d5ae6aae78448ab7d710   Name:/dev/sdb            State:online    Size (GiB):19      Used (GiB):0       Free (GiB):19</span><br><span class="line">			Bricks:</span><br><span class="line"></span><br><span class="line">	Node Id: d5773e74abd6f3041d07b095a2f4ae99</span><br><span class="line">	State: online</span><br><span class="line">	Cluster Id: dde6f33a6cb852e04cb0685e210d3959</span><br><span class="line">	Zone: 1</span><br><span class="line">	Management Hostname: server08</span><br><span class="line">	Storage Hostname: 172.16.170.135</span><br><span class="line">	Devices:</span><br><span class="line">		Id:2678d65a18f0c21b38288d218d209a3f   Name:/dev/sdb            State:online    Size (GiB):19      Used (GiB):0       Free (GiB):19</span><br><span class="line">			Bricks:</span><br></pre></td></tr></table></figure>
<h2 id="5-编写验证Heketi和GlusterFS集群对接Kubernetes集群的相关YAML："><a href="#5-编写验证Heketi和GlusterFS集群对接Kubernetes集群的相关YAML：" class="headerlink" title="5. 编写验证Heketi和GlusterFS集群对接Kubernetes集群的相关YAML："></a>5. 编写验证Heketi和GlusterFS集群对接Kubernetes集群的相关YAML：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p example/</span><br><span class="line">cd example/</span><br><span class="line"></span><br><span class="line">kubectl get service -n storage</span><br><span class="line">NAME      TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">heketi    ClusterIP   10.96.162.3   &lt;none&gt;        8080/TCP   1h</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 01-glusterfs-storageclass.yaml</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: glusterfs</span><br><span class="line">provisioner: kubernetes.io/glusterfs</span><br><span class="line">parameters:</span><br><span class="line">  resturl: &quot;http://10.96.162.3:8080&quot;</span><br><span class="line">  restauthenabled: &quot;false&quot;</span><br><span class="line">  volumetype: &quot;replicate:3&quot;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 02-persistentvolumeclaim.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: myclaim</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br><span class="line">  storageClassName: glusterfs</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 03-pod.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: alpine</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - image: alpine:3.8</span><br><span class="line">    command:</span><br><span class="line">    - sleep</span><br><span class="line">    - &quot;3600&quot;</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    name: alpine</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: myvolume</span><br><span class="line">      mountPath: &quot;/data&quot;</span><br><span class="line">  restartPolicy: Always</span><br><span class="line">  volumes:</span><br><span class="line">  - name: myvolume</span><br><span class="line">    persistentVolumeClaim:</span><br><span class="line">      claimName: myclaim</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="6-验证Heketi和GlusterFS集群对接Kubernetes集群："><a href="#6-验证Heketi和GlusterFS集群对接Kubernetes集群：" class="headerlink" title="6. 验证Heketi和GlusterFS集群对接Kubernetes集群："></a>6. 验证Heketi和GlusterFS集群对接Kubernetes集群：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"># 创建StorageClass对接到GlusterFS</span><br><span class="line">kubectl create -f 01-glusterfs-storageclass.yaml</span><br><span class="line">storageclass.storage.k8s.io/glusterfs created</span><br><span class="line"></span><br><span class="line"># 创建PVC使用上面的StorageClass</span><br><span class="line">kubectl create -f 02-persistentvolumeclaim.yaml</span><br><span class="line">persistentvolumeclaim/myclaim created</span><br><span class="line"></span><br><span class="line"># 验证PVC动态供应PV</span><br><span class="line">kubectl get pvc -o wide</span><br><span class="line">NAME      STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">myclaim   Bound     pvc-f910f4b2-239d-11e9-b3b6-000c295a4c4c   1Gi        RWX            glusterfs      1m</span><br><span class="line"></span><br><span class="line">kubectl get pv -o wide</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM             STORAGECLASS   REASON    AGE</span><br><span class="line">pvc-f910f4b2-239d-11e9-b3b6-000c295a4c4c   1Gi        RWX            Delete           Bound     default/myclaim   glusterfs                1m</span><br><span class="line"></span><br><span class="line"># 创建测试Pod使用上面的PVC</span><br><span class="line">kubectl create -f 03-pod.yaml</span><br><span class="line">pod/alpine created</span><br><span class="line"></span><br><span class="line">kubectl get pod -o wide</span><br><span class="line">NAME      READY     STATUS    RESTARTS   AGE       IP            NODE</span><br><span class="line">alpine    1/1       Running   0          5s        10.211.2.10   server08</span><br><span class="line"></span><br><span class="line"># 验证PVC是否挂载到Pod中</span><br><span class="line">kubectl exec -it alpine /bin/sh</span><br><span class="line">/ # df -h</span><br><span class="line">Filesystem                Size      Used Available Use% Mounted on</span><br><span class="line">overlay                  17.0G      4.3G     12.7G  25% /</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /dev</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /sys/fs/cgroup</span><br><span class="line">172.16.170.134:vol_7f5ea2a6035e2f7885687db4ef46d2e3</span><br><span class="line">                       1015.3M     42.8M    982.6M   4% /data</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /dev/termination-log</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /etc/resolv.conf</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /etc/hostname</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /etc/hosts</span><br><span class="line">shm                      64.0M         0     64.0M   0% /dev/shm</span><br><span class="line">tmpfs                   909.8M     12.0K    909.8M   0% /run/secrets/kubernetes.io/serviceaccount</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/kcore</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/timer_list</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/timer_stats</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/sched_debug</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /sys/firmware</span><br><span class="line"></span><br><span class="line"># 创建测试文件，验证存储容量的限制</span><br><span class="line">/data # fallocate -l 982M onebox.disk</span><br><span class="line">/data # df -h</span><br><span class="line">Filesystem                Size      Used Available Use% Mounted on</span><br><span class="line">overlay                  17.0G      4.3G     12.7G  25% /</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /dev</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /sys/fs/cgroup</span><br><span class="line">172.16.170.134:vol_7f5ea2a6035e2f7885687db4ef46d2e3</span><br><span class="line">                       1015.3M      1.0G    572.0K 100% /data</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /dev/termination-log</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /etc/resolv.conf</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /etc/hostname</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /etc/hosts</span><br><span class="line">shm                      64.0M         0     64.0M   0% /dev/shm</span><br><span class="line">tmpfs                   909.8M     12.0K    909.8M   0% /run/secrets/kubernetes.io/serviceaccount</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/kcore</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/timer_list</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/timer_stats</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/sched_debug</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /sys/firmware</span><br><span class="line">/data # fallocate -l 982M onebox.disk</span><br><span class="line">fallocate: fallocate &apos;onebox.disk&apos;: No space left on device</span><br><span class="line"></span><br><span class="line"># 测试文件删除后，验证容量是否恢复</span><br><span class="line">/data # rm -rf onebox.disk</span><br><span class="line">/data # df -h</span><br><span class="line">Filesystem                Size      Used Available Use% Mounted on</span><br><span class="line">overlay                  17.0G      4.3G     12.7G  25% /</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /dev</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /sys/fs/cgroup</span><br><span class="line">172.16.170.134:vol_7f5ea2a6035e2f7885687db4ef46d2e3</span><br><span class="line">                       1015.3M     42.8M    982.6M   4% /data</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /dev/termination-log</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /etc/resolv.conf</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /etc/hostname</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /etc/hosts</span><br><span class="line">shm                      64.0M         0     64.0M   0% /dev/shm</span><br><span class="line">tmpfs                   909.8M     12.0K    909.8M   0% /run/secrets/kubernetes.io/serviceaccount</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/kcore</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/timer_list</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/timer_stats</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/sched_debug</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /sys/firmware</span><br></pre></td></tr></table></figure>
<h1 id="四、参考资料"><a href="#四、参考资料" class="headerlink" title="四、参考资料"></a>四、参考资料</h1><p><a href="https://github.com/gluster/gluster-kubernetes/tree/v1.1" target="_blank" rel="noopener">https://github.com/gluster/gluster-kubernetes/tree/v1.1</a><br><a href="https://hub.docker.com/r/gluster/gluster-centos/tags" target="_blank" rel="noopener">https://hub.docker.com/r/gluster/gluster-centos/tags</a><br><a href="https://blog.csdn.net/q1403539144/article/details/86566796" target="_blank" rel="noopener">https://blog.csdn.net/q1403539144/article/details/86566796</a><br><a href="http://www.cnitblog.com/xijia0524/archive/2014/04/19/89466.html" target="_blank" rel="noopener">http://www.cnitblog.com/xijia0524/archive/2014/04/19/89466.html</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GlusterFS/">GlusterFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Storage/">Storage</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kubeadm_ha_clusters" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/01/23/kubeadm_ha_clusters/" class="article-date">
  	<time datetime="2019-01-23T05:52:06.943Z" itemprop="datePublished">2019-01-23</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/23/kubeadm_ha_clusters/">
        使用Kubeadm安装高可用Kubernetes集群（Stacked Control Plane Nodes For Baremetal篇）
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、高可用部署的实现方式介绍"><a href="#一、高可用部署的实现方式介绍" class="headerlink" title="一、高可用部署的实现方式介绍"></a>一、高可用部署的实现方式介绍</h1><p>官方文档介绍了使用Kbeadm设置高可用性Kubernetes集群的两种不同方法：</p>
<h2 id="1-堆叠master的方式（with-stacked-masters）"><a href="#1-堆叠master的方式（with-stacked-masters）" class="headerlink" title="1. 堆叠master的方式（with stacked masters）"></a>1. 堆叠master的方式（with stacked masters）</h2><p>这种方法需要较少的基础设施。控制平面节点和etcd成员位于同一位置。</p>
<h2 id="2-使用外部etcd集群的方式（with-an-external-etcd-cluster）"><a href="#2-使用外部etcd集群的方式（with-an-external-etcd-cluster）" class="headerlink" title="2. 使用外部etcd集群的方式（with an external etcd cluster）"></a>2. 使用外部etcd集群的方式（with an external etcd cluster）</h2><p>这种方法需要更多的基础设施。控制平面节点和etcd成员是分开的。<br>这里重点介绍第一种方式，即堆叠master的方式。官方文档链接详见参考资料。</p>
<h1 id="二、实验环境版本信息"><a href="#二、实验环境版本信息" class="headerlink" title="二、实验环境版本信息"></a>二、实验环境版本信息</h1><p>docker 17.03.1-ce<br>kubeadm v1.11.0<br>kubelet v1.11.0<br>kubectl v1.11.0<br>calico v3.1.3</p>
<h1 id="三、部署架构介绍"><a href="#三、部署架构介绍" class="headerlink" title="三、部署架构介绍"></a>三、部署架构介绍</h1><h2 id="1-Kubernetes-Master（Control-Plane）"><a href="#1-Kubernetes-Master（Control-Plane）" class="headerlink" title="1. Kubernetes Master（Control Plane）"></a>1. Kubernetes Master（Control Plane）</h2><p>172.16.170.128 server01 -&gt; docker kubelet keepalived haproxy etcd kube-apiserver kube-controller-manager kube-scheduler kube-proxy calico-node<br>172.16.170.129 server02 -&gt; docker kubelet keepalived haproxy etcd kube-apiserver kube-controller-manager kube-scheduler kube-proxy calico-node<br>172.16.170.130 server03 -&gt; docker kubelet keepalived haproxy etcd kube-apiserver kube-controller-manager kube-scheduler kube-proxy calico-node</p>
<h2 id="2-Kubernetes-Node"><a href="#2-Kubernetes-Node" class="headerlink" title="2. Kubernetes Node"></a>2. Kubernetes Node</h2><p>172.16.170.134 server07 -&gt; docker kubelet kube-proxy calico-node<br>172.16.170.135 server08 -&gt; docker kubelet kube-proxy calico-node</p>
<h1 id="四、实现过程记录"><a href="#四、实现过程记录" class="headerlink" title="四、实现过程记录"></a>四、实现过程记录</h1><h2 id="1-在Kubernetes-Control-Plane上的所有Node上部署HAProxy做为负载均衡器（由Kubelet管理以静态Pod的方式实现）"><a href="#1-在Kubernetes-Control-Plane上的所有Node上部署HAProxy做为负载均衡器（由Kubelet管理以静态Pod的方式实现）" class="headerlink" title="1. 在Kubernetes Control Plane上的所有Node上部署HAProxy做为负载均衡器（由Kubelet管理以静态Pod的方式实现）"></a>1. 在Kubernetes Control Plane上的所有Node上部署HAProxy做为负载均衡器（由Kubelet管理以静态Pod的方式实现）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /etc/haproxy/</span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">  log 127.0.0.1 local0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  uid 99</span><br><span class="line">  gid 99</span><br><span class="line">global</span><br><span class="line">  log 127.0.0.1 local0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  uid 99</span><br><span class="line">  gid 99</span><br><span class="line">  #daemon</span><br><span class="line">  nbproc 1</span><br><span class="line">  pidfile haproxy.pid</span><br><span class="line">defaults</span><br><span class="line">  mode http</span><br><span class="line">  log 127.0.0.1 local0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  retries 3</span><br><span class="line">  timeout connect 5s</span><br><span class="line">  timeout client 30s</span><br><span class="line">  timeout server 30s</span><br><span class="line">  timeout check 2s</span><br><span class="line"></span><br><span class="line">listen stats</span><br><span class="line">  mode http</span><br><span class="line">  bind 0.0.0.0:9090</span><br><span class="line">  log 127.0.0.1 local0 err</span><br><span class="line">  stats refresh 30s</span><br><span class="line">  stats uri     /haproxy-status</span><br><span class="line">  stats realm   Haproxy\ Statistics</span><br><span class="line">  stats auth    admin:12345678</span><br><span class="line">  stats hide-version</span><br><span class="line">  stats admin if TRUE</span><br><span class="line"></span><br><span class="line">frontend kube-apiserver-https</span><br><span class="line">   mode tcp</span><br><span class="line">   bind :8443</span><br><span class="line">   default_backend kube-apiserver-backend</span><br><span class="line"></span><br><span class="line">backend kube-apiserver-backend</span><br><span class="line">    mode tcp</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server server01 172.16.170.128:6443 weight 3 minconn 100 maxconn 50000 check inter 5000 rise 2 fall 5</span><br><span class="line">    server server02 172.16.170.129:6443 weight 3 minconn 100 maxconn 50000 check inter 5000 rise 2 fall 5</span><br><span class="line">    server server03 172.16.170.130:6443 weight 3 minconn 100 maxconn 50000 check inter 5000 rise 2 fall 5</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/kubernetes/manifests/haproxy.yaml</span><br><span class="line">kind: Pod</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    scheduler.alpha.kubernetes.io/critical-pod: &quot;&quot;</span><br><span class="line">  labels:</span><br><span class="line">    component: haproxy</span><br><span class="line">    tier: control-plane</span><br><span class="line">  name: kube-haproxy</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  hostNetwork: true</span><br><span class="line">  containers:</span><br><span class="line">  - name: kube-haproxy</span><br><span class="line">    image: haproxy:1.7-alpine</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        cpu: 100m</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: haproxy-cfg</span><br><span class="line">      readOnly: true</span><br><span class="line">      mountPath: /usr/local/etc/haproxy/haproxy.cfg</span><br><span class="line">  volumes:</span><br><span class="line">  - name: haproxy-cfg</span><br><span class="line">    hostPath:</span><br><span class="line">      path: /etc/haproxy/haproxy.cfg</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="2-在Kubernetes-Control-Plane的所有Node上部署Keepalived（由Kubelet管理以静态Pod的方式实现）"><a href="#2-在Kubernetes-Control-Plane的所有Node上部署Keepalived（由Kubelet管理以静态Pod的方式实现）" class="headerlink" title="2. 在Kubernetes Control Plane的所有Node上部署Keepalived（由Kubelet管理以静态Pod的方式实现）"></a>2. 在Kubernetes Control Plane的所有Node上部署Keepalived（由Kubelet管理以静态Pod的方式实现）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; /etc/kubernetes/manifests/keepalived.yaml</span><br><span class="line">kind: Pod</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    scheduler.alpha.kubernetes.io/critical-pod: &quot;&quot;</span><br><span class="line">  labels:</span><br><span class="line">    component: keepalived</span><br><span class="line">    tier: control-plane</span><br><span class="line">  name: kube-keepalived</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  hostNetwork: true</span><br><span class="line">  containers:</span><br><span class="line">  - name: kube-keepalived</span><br><span class="line">    image: osixia/keepalived:1.4.5</span><br><span class="line">    env:</span><br><span class="line">    - name: KEEPALIVED_VIRTUAL_IPS</span><br><span class="line">      value: 172.16.170.151</span><br><span class="line">    - name: KEEPALIVED_INTERFACE</span><br><span class="line">      value: ens33</span><br><span class="line">    - name: KEEPALIVED_UNICAST_PEERS</span><br><span class="line">      value: &quot;#PYTHON2BASH:[&apos;172.16.170.128&apos;, &apos;172.16.170.129&apos;, &apos;172.16.170.130&apos;]&quot;</span><br><span class="line">    - name: KEEPALIVED_PASSWORD</span><br><span class="line">      value: docker</span><br><span class="line">    - name: KEEPALIVED_PRIORITY</span><br><span class="line">      value: &quot;100&quot;</span><br><span class="line">    - name: KEEPALIVED_ROUTER_ID</span><br><span class="line">      value: &quot;51&quot;</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        cpu: 100m</span><br><span class="line">    securityContext:</span><br><span class="line">      privileged: true</span><br><span class="line">      capabilities:</span><br><span class="line">        add:</span><br><span class="line">        - NET_ADMIN</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="3-在Kubernetes-Control-Plane-的第一个Node（server01）上操作："><a href="#3-在Kubernetes-Control-Plane-的第一个Node（server01）上操作：" class="headerlink" title="3. 在Kubernetes Control Plane 的第一个Node（server01）上操作："></a>3. 在Kubernetes Control Plane 的第一个Node（server01）上操作：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"># 生成Kubeadm初始化需要使用的配置文件</span><br><span class="line">mkdir -p kubeadm/config/</span><br><span class="line">cat &lt;&lt;EOF &gt; kubeadm/config/kubeadm-config.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.0</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- &quot;172.16.170.151&quot;</span><br><span class="line">api:</span><br><span class="line">  controlPlaneEndpoint: &quot;172.16.170.151:8443&quot;</span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: &quot;https://127.0.0.1:2379,https://172.16.170.128:2379&quot;</span><br><span class="line">      advertise-client-urls: &quot;https://172.16.170.128:2379&quot;</span><br><span class="line">      listen-peer-urls: &quot;https://172.16.170.128:2380&quot;</span><br><span class="line">      initial-advertise-peer-urls: &quot;https://172.16.170.128:2380&quot;</span><br><span class="line">      initial-cluster: &quot;server01=https://172.16.170.128:2380&quot;</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - server01</span><br><span class="line">      - 172.16.170.128</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - server01</span><br><span class="line">      - 172.16.170.128</span><br><span class="line">controllerManagerExtraArgs:</span><br><span class="line">  node-monitor-grace-period: 10s</span><br><span class="line">  pod-eviction-timeout: 10s</span><br><span class="line"></span><br><span class="line">networking:</span><br><span class="line">  podSubnet: 10.211.0.0/16</span><br><span class="line">  serviceSubnet: 10.96.0.0/16</span><br><span class="line"></span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 拉取Kubeadm初始化需要使用的docker镜像</span><br><span class="line">kubeadm config images pull --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line"></span><br><span class="line"># 执行Kubeadm的初始化操作（注意记录输出的Node加入命令）</span><br><span class="line">kubeadm init --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line"></span><br><span class="line"># 配置当前节点上的Kubectl访问权限</span><br><span class="line">rm -rf $HOME/.kube</span><br><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line"># 保存输出中类似于下面的命令，供添加节点功能使用</span><br><span class="line">kubeadm join 172.16.170.151:8443 --token lt0o7j.ayxwcqr8v88spzjj --discovery-token-ca-cert-hash sha256:1ad613cf114281af6eca0afeebae7185ed69218ff92b73ebe248b90cc74353a3</span><br><span class="line"></span><br><span class="line"># 配置server01到server02和server03的ssh免密登录</span><br><span class="line">ssh-keygen</span><br><span class="line">ssh-copy-id -i .ssh/id_rsa.pub root@server02</span><br><span class="line">ssh-copy-id -i .ssh/id_rsa.pub root@server03</span><br><span class="line"></span><br><span class="line"># 验证server01到server02和server03的ssh免密登录</span><br><span class="line">ssh server02</span><br><span class="line">ssh server03</span><br><span class="line"></span><br><span class="line"># 分发pki证书和admin.conf文件</span><br><span class="line">ssh server02 &apos;mkdir -p /etc/kubernetes/pki/etcd/&apos;</span><br><span class="line">ssh server03 &apos;mkdir -p /etc/kubernetes/pki/etcd/&apos;</span><br><span class="line">cat &lt;&lt;EOF &gt; kubeadm/config/scp-config.sh</span><br><span class="line">USER=root</span><br><span class="line">CONTROL_PLANE_IPS=&quot;172.16.170.129 172.16.170.130&quot;</span><br><span class="line">for host in \$&#123;CONTROL_PLANE_IPS&#125;; do</span><br><span class="line">    scp /etc/kubernetes/pki/ca.crt \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/</span><br><span class="line">    scp /etc/kubernetes/pki/ca.key \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/</span><br><span class="line">    scp /etc/kubernetes/pki/sa.key \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/</span><br><span class="line">    scp /etc/kubernetes/pki/sa.pub \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/</span><br><span class="line">    scp /etc/kubernetes/pki/front-proxy-ca.crt \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/</span><br><span class="line">    scp /etc/kubernetes/pki/front-proxy-ca.key \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/</span><br><span class="line">    scp /etc/kubernetes/pki/etcd/ca.crt \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/etcd/</span><br><span class="line">    scp /etc/kubernetes/pki/etcd/ca.key \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/etcd/</span><br><span class="line">    scp /etc/kubernetes/admin.conf \$&#123;USER&#125;@\$host:/etc/kubernetes/</span><br><span class="line">done</span><br><span class="line">EOF</span><br><span class="line">chmod 0755 kubeadm/config/scp-config.sh</span><br><span class="line">./kubeadm/config/scp-config.sh</span><br></pre></td></tr></table></figure>
<h2 id="4-在Kubernetes-Control-Plane-的第二个Node（server02）上操作："><a href="#4-在Kubernetes-Control-Plane-的第二个Node（server02）上操作：" class="headerlink" title="4. 在Kubernetes Control Plane 的第二个Node（server02）上操作："></a>4. 在Kubernetes Control Plane 的第二个Node（server02）上操作：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"># 生成Kubeadm初始化需要使用的配置文件</span><br><span class="line">mkdir -p kubeadm/config/</span><br><span class="line">cat &lt;&lt;EOF &gt; kubeadm/config/kubeadm-config.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.0</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- &quot;172.16.170.151&quot;</span><br><span class="line">api:</span><br><span class="line">  controlPlaneEndpoint: &quot;172.16.170.151:8443&quot;</span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: &quot;https://127.0.0.1:2379,https://172.16.170.129:2379&quot;</span><br><span class="line">      advertise-client-urls: &quot;https://172.16.170.129:2379&quot;</span><br><span class="line">      listen-peer-urls: &quot;https://172.16.170.129:2380&quot;</span><br><span class="line">      initial-advertise-peer-urls: &quot;https://172.16.170.129:2380&quot;</span><br><span class="line">      initial-cluster: &quot;server01=https://172.16.170.128:2380,server02=https://172.16.170.129:2380&quot;</span><br><span class="line">      initial-cluster-state: existing</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - server02</span><br><span class="line">      - 172.16.170.129</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - server02</span><br><span class="line">      - 172.16.170.129</span><br><span class="line">controllerManagerExtraArgs:</span><br><span class="line">  node-monitor-grace-period: 10s</span><br><span class="line">  pod-eviction-timeout: 10s</span><br><span class="line"></span><br><span class="line">networking:</span><br><span class="line">  podSubnet: 10.211.0.0/16</span><br><span class="line">  serviceSubnet: 10.96.0.0/16</span><br><span class="line"></span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 拉取Kubeadm初始化需要使用的docker镜像</span><br><span class="line">kubeadm config images pull --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line"></span><br><span class="line"># 通过Kubeadm phase来启动server02上的Kubelet</span><br><span class="line">kubeadm alpha phase certs all --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">systemctl restart kubelet.service</span><br><span class="line">systemctl status kubelet.service</span><br><span class="line"></span><br><span class="line"># 添加当前Node上的etcd节点到etcd集群中</span><br><span class="line">CP0_IP=172.16.170.128</span><br><span class="line">CP0_HOSTNAME=server01</span><br><span class="line">CP1_IP=172.16.170.129</span><br><span class="line">CP1_HOSTNAME=server02</span><br><span class="line">KUBECONFIG=/etc/kubernetes/admin.conf kubectl exec -n kube-system etcd-$&#123;CP0_HOSTNAME&#125; -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://$&#123;CP0_IP&#125;:2379 member add $&#123;CP1_HOSTNAME&#125; https://$&#123;CP1_IP&#125;:2380</span><br><span class="line">kubeadm alpha phase etcd local --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line"></span><br><span class="line"># 部署Kubernetes Control Plane的相关组件，并且标记当前Node为Master</span><br><span class="line">kubeadm alpha phase kubeconfig all --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config kubeadm/config/kubeadm-config.yaml</span><br></pre></td></tr></table></figure>
<h2 id="4-在Kubernetes-Control-Plane-的第三个Node（server03）上操作："><a href="#4-在Kubernetes-Control-Plane-的第三个Node（server03）上操作：" class="headerlink" title="4. 在Kubernetes Control Plane 的第三个Node（server03）上操作："></a>4. 在Kubernetes Control Plane 的第三个Node（server03）上操作：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"># 生成Kubeadm初始化需要使用的配置文件</span><br><span class="line">mkdir -p kubeadm/config/</span><br><span class="line">cat &lt;&lt;EOF &gt; kubeadm/config/kubeadm-config.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.0</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- &quot;172.16.170.151&quot;</span><br><span class="line">api:</span><br><span class="line">  controlPlaneEndpoint: &quot;172.16.170.151:8443&quot;</span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: &quot;https://127.0.0.1:2379,https://172.16.170.130:2379&quot;</span><br><span class="line">      advertise-client-urls: &quot;https://172.16.170.130:2379&quot;</span><br><span class="line">      listen-peer-urls: &quot;https://172.16.170.130:2380&quot;</span><br><span class="line">      initial-advertise-peer-urls: &quot;https://172.16.170.130:2380&quot;</span><br><span class="line">      initial-cluster: &quot;server01=https://172.16.170.128:2380,server02=https://172.16.170.129:2380,server03=https://172.16.170.130:2380&quot;</span><br><span class="line">      initial-cluster-state: existing</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - server03</span><br><span class="line">      - 172.16.170.130</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - server03</span><br><span class="line">      - 172.16.170.130</span><br><span class="line">controllerManagerExtraArgs:</span><br><span class="line">  node-monitor-grace-period: 10s</span><br><span class="line">  pod-eviction-timeout: 10s</span><br><span class="line"></span><br><span class="line">networking:</span><br><span class="line">  podSubnet: 10.211.0.0/16</span><br><span class="line">  serviceSubnet: 10.96.0.0/16</span><br><span class="line"></span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 拉取Kubeadm初始化需要使用的docker镜像</span><br><span class="line">kubeadm config images pull --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line"></span><br><span class="line"># 通过Kubeadm phase来启动server03上的Kubelet</span><br><span class="line">kubeadm alpha phase certs all --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">systemctl restart kubelet.service</span><br><span class="line">systemctl status kubelet.service</span><br><span class="line"></span><br><span class="line"># 添加当前Node上的etcd节点到etcd集群中</span><br><span class="line">CP0_IP=172.16.170.128</span><br><span class="line">CP0_HOSTNAME=server01</span><br><span class="line">CP2_IP=172.16.170.130</span><br><span class="line">CP2_HOSTNAME=server03</span><br><span class="line">KUBECONFIG=/etc/kubernetes/admin.conf kubectl exec -n kube-system etcd-$&#123;CP0_HOSTNAME&#125; -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://$&#123;CP0_IP&#125;:2379 member add $&#123;CP2_HOSTNAME&#125; https://$&#123;CP2_IP&#125;:2380</span><br><span class="line">kubeadm alpha phase etcd local --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line"></span><br><span class="line"># 部署Kubernetes Control Plane的相关组件，并且标记当前Node为Master</span><br><span class="line">kubeadm alpha phase kubeconfig all --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config kubeadm/config/kubeadm-config.yaml</span><br></pre></td></tr></table></figure>
<h2 id="5-Kubernetes-Control-Plane的另外两个节点分别配置Kubectl访问权限"><a href="#5-Kubernetes-Control-Plane的另外两个节点分别配置Kubectl访问权限" class="headerlink" title="5. Kubernetes Control Plane的另外两个节点分别配置Kubectl访问权限"></a>5. Kubernetes Control Plane的另外两个节点分别配置Kubectl访问权限</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rm -rf $HOME/.kube</span><br><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">chown $(id -u):$(id -g) $HOME/.kube/config</span><br></pre></td></tr></table></figure>
<h2 id="6-高可用部署的Stack结构验证"><a href="#6-高可用部署的Stack结构验证" class="headerlink" title="6. 高可用部署的Stack结构验证"></a>6. 高可用部署的Stack结构验证</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get pod --all-namespaces -o wide</span><br><span class="line">NAMESPACE     NAME                               READY     STATUS    RESTARTS   AGE       IP               NODE</span><br><span class="line">kube-system   calico-node-ff5cv                  2/2       Running   0          47s       172.16.170.130   server03</span><br><span class="line">kube-system   calico-node-hb782                  2/2       Running   0          8m        172.16.170.128   server01</span><br><span class="line">kube-system   calico-node-zpwcp                  2/2       Running   0          4m        172.16.170.129   server02</span><br><span class="line">kube-system   coredns-777d78ff6f-5n8bg           1/1       Running   0          10m       10.211.0.4       server01</span><br><span class="line">kube-system   coredns-777d78ff6f-wfm7d           1/1       Running   0          10m       10.211.0.5       server01</span><br><span class="line">kube-system   etcd-server01                      1/1       Running   0          9m        172.16.170.128   server01</span><br><span class="line">kube-system   etcd-server02                      1/1       Running   0          3m        172.16.170.129   server02</span><br><span class="line">kube-system   etcd-server03                      1/1       Running   0          27s       172.16.170.130   server03</span><br><span class="line">kube-system   kube-apiserver-server01            1/1       Running   0          9m        172.16.170.128   server01</span><br><span class="line">kube-system   kube-apiserver-server02            1/1       Running   0          2m        172.16.170.129   server02</span><br><span class="line">kube-system   kube-apiserver-server03            1/1       Running   0          16s       172.16.170.130   server03</span><br><span class="line">kube-system   kube-controller-manager-server01   1/1       Running   0          9m        172.16.170.128   server01</span><br><span class="line">kube-system   kube-controller-manager-server02   1/1       Running   0          2m        172.16.170.129   server02</span><br><span class="line">kube-system   kube-controller-manager-server03   1/1       Running   0          16s       172.16.170.130   server03</span><br><span class="line">kube-system   kube-haproxy-server01              1/1       Running   0          9m        172.16.170.128   server01</span><br><span class="line">kube-system   kube-haproxy-server02              1/1       Running   0          4m        172.16.170.129   server02</span><br><span class="line">kube-system   kube-haproxy-server03              1/1       Running   0          27s       172.16.170.130   server03</span><br><span class="line">kube-system   kube-keepalived-server01           1/1       Running   0          9m        172.16.170.128   server01</span><br><span class="line">kube-system   kube-keepalived-server02           1/1       Running   0          4m        172.16.170.129   server02</span><br><span class="line">kube-system   kube-keepalived-server03           1/1       Running   0          27s       172.16.170.130   server03</span><br><span class="line">kube-system   kube-proxy-88b55                   1/1       Running   0          4m        172.16.170.129   server02</span><br><span class="line">kube-system   kube-proxy-9n9vv                   1/1       Running   0          9m        172.16.170.128   server01</span><br><span class="line">kube-system   kube-proxy-j7lqz                   1/1       Running   0          47s       172.16.170.130   server03</span><br><span class="line">kube-system   kube-scheduler-server01            1/1       Running   0          9m        172.16.170.128   server01</span><br><span class="line">kube-system   kube-scheduler-server02            1/1       Running   0          2m        172.16.170.129   server02</span><br><span class="line">kube-system   kube-scheduler-server03            1/1       Running   0          16s       172.16.170.130   server03</span><br><span class="line"></span><br><span class="line"># kubectl get node -o wide</span><br><span class="line">NAME       STATUS    ROLES     AGE       VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME</span><br><span class="line">server01   Ready     master    10m       v1.11.0   172.16.170.128   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-862.11.6.el7.x86_64   docker://17.3.1</span><br><span class="line">server02   Ready     master    4m        v1.11.0   172.16.170.129   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-862.11.6.el7.x86_64   docker://17.3.1</span><br><span class="line">server03   Ready     master    1m        v1.11.0   172.16.170.130   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-862.11.6.el7.x86_64   docker://17.3.1</span><br></pre></td></tr></table></figure>
<h2 id="7-为高可用集群添加两个Node"><a href="#7-为高可用集群添加两个Node" class="headerlink" title="7. 为高可用集群添加两个Node"></a>7. 为高可用集群添加两个Node</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"># 在server07上执行添加Node的命令</span><br><span class="line"># kubeadm join 172.16.170.151:8443 --token lt0o7j.ayxwcqr8v88spzjj --discovery-token-ca-cert-hash sha256:1ad613cf114281af6eca0afeebae7185ed69218ff92b73ebe248b90cc74353a3</span><br><span class="line">[preflight] running pre-flight checks</span><br><span class="line">	[WARNING RequiredIPVSKernelModulesAvailable]: the IPVS proxier will not be used, because the following required kernel modules are not loaded: [ip_vs_wrr ip_vs_sh ip_vs ip_vs_rr] or no builtin kernel ipvs support: map[ip_vs_sh:&#123;&#125; nf_conntrack_ipv4:&#123;&#125; ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_wrr:&#123;&#125;]</span><br><span class="line">you can solve this problem with following methods:</span><br><span class="line">1. Run &apos;modprobe -- &apos; to load missing kernel modules;</span><br><span class="line">2. Provide the missing builtin kernel ipvs support</span><br><span class="line"></span><br><span class="line">I0123 16:10:01.668746   17689 kernel_validator.go:81] Validating kernel version</span><br><span class="line">I0123 16:10:01.668820   17689 kernel_validator.go:96] Validating kernel config</span><br><span class="line">[discovery] Trying to connect to API Server &quot;172.16.170.151:8443&quot;</span><br><span class="line">[discovery] Created cluster-info discovery client, requesting info from &quot;https://172.16.170.151:8443&quot;</span><br><span class="line">[discovery] Requesting info from &quot;https://172.16.170.151:8443&quot; again to validate TLS against the pinned public key</span><br><span class="line">[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &quot;172.16.170.151:8443&quot;</span><br><span class="line">[discovery] Successfully established connection with API Server &quot;172.16.170.151:8443&quot;</span><br><span class="line">[kubelet] Downloading configuration for the kubelet from the &quot;kubelet-config-1.11&quot; ConfigMap in the kube-system namespace</span><br><span class="line">[kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">[kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class="line">[preflight] Activating the kubelet service</span><br><span class="line">[tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap...</span><br><span class="line">[patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;server07&quot; as an annotation</span><br><span class="line"></span><br><span class="line">This node has joined the cluster:</span><br><span class="line">* Certificate signing request was sent to master and a response</span><br><span class="line">  was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line"></span><br><span class="line">Run &apos;kubectl get nodes&apos; on the master to see this node join the cluster.</span><br><span class="line"></span><br><span class="line"># 在server08上执行添加Node的命令</span><br><span class="line"># kubeadm join 172.16.170.151:8443 --token lt0o7j.ayxwcqr8v88spzjj --discovery-token-ca-cert-hash sha256:1ad613cf114281af6eca0afeebae7185ed69218ff92b73ebe248b90cc74353a3</span><br><span class="line">[preflight] running pre-flight checks</span><br><span class="line">	[WARNING RequiredIPVSKernelModulesAvailable]: the IPVS proxier will not be used, because the following required kernel modules are not loaded: [ip_vs_rr ip_vs_wrr ip_vs_sh ip_vs] or no builtin kernel ipvs support: map[ip_vs_sh:&#123;&#125; nf_conntrack_ipv4:&#123;&#125; ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_wrr:&#123;&#125;]</span><br><span class="line">you can solve this problem with following methods:</span><br><span class="line">1. Run &apos;modprobe -- &apos; to load missing kernel modules;</span><br><span class="line">2. Provide the missing builtin kernel ipvs support</span><br><span class="line"></span><br><span class="line">I0123 16:10:29.832899   17706 kernel_validator.go:81] Validating kernel version</span><br><span class="line">I0123 16:10:29.833038   17706 kernel_validator.go:96] Validating kernel config</span><br><span class="line">[discovery] Trying to connect to API Server &quot;172.16.170.151:8443&quot;</span><br><span class="line">[discovery] Created cluster-info discovery client, requesting info from &quot;https://172.16.170.151:8443&quot;</span><br><span class="line">[discovery] Requesting info from &quot;https://172.16.170.151:8443&quot; again to validate TLS against the pinned public key</span><br><span class="line">[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &quot;172.16.170.151:8443&quot;</span><br><span class="line">[discovery] Successfully established connection with API Server &quot;172.16.170.151:8443&quot;</span><br><span class="line">[kubelet] Downloading configuration for the kubelet from the &quot;kubelet-config-1.11&quot; ConfigMap in the kube-system namespace</span><br><span class="line">[kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">[kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class="line">[preflight] Activating the kubelet service</span><br><span class="line">[tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap...</span><br><span class="line">[patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;server08&quot; as an annotation</span><br><span class="line"></span><br><span class="line">This node has joined the cluster:</span><br><span class="line">* Certificate signing request was sent to master and a response</span><br><span class="line">  was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line"></span><br><span class="line">Run &apos;kubectl get nodes&apos; on the master to see this node join the cluster.</span><br><span class="line"></span><br><span class="line"># 在任意一个Master节点上执行</span><br><span class="line"># kubectl get node -o wide</span><br><span class="line">NAME       STATUS    ROLES     AGE       VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME</span><br><span class="line">server01   Ready     master    16m       v1.11.0   172.16.170.128   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-862.11.6.el7.x86_64   docker://17.3.1</span><br><span class="line">server02   Ready     master    10m       v1.11.0   172.16.170.129   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-862.11.6.el7.x86_64   docker://17.3.1</span><br><span class="line">server03   Ready     master    7m        v1.11.0   172.16.170.130   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-862.11.6.el7.x86_64   docker://17.3.1</span><br><span class="line">server07   Ready     &lt;none&gt;    40s       v1.11.0   172.16.170.134   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-957.1.3.el7.x86_64    docker://17.3.1</span><br><span class="line">server08   Ready     &lt;none&gt;    12s       v1.11.0   172.16.170.135   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-957.1.3.el7.x86_64    docker://17.3.1</span><br><span class="line"></span><br><span class="line"># 在任意一个Master节点上执行</span><br><span class="line"># kubectl get pod --all-namespaces -o wide</span><br><span class="line">NAMESPACE     NAME                               READY     STATUS    RESTARTS   AGE       IP               NODE</span><br><span class="line">kube-system   calico-node-c8j7r                  2/2       Running   0          1m        172.16.170.134   server07</span><br><span class="line">kube-system   calico-node-chngv                  1/2       Running   0          32s       172.16.170.135   server08</span><br><span class="line">kube-system   calico-node-ff5cv                  2/2       Running   0          7m        172.16.170.130   server03</span><br><span class="line">kube-system   calico-node-hb782                  2/2       Running   0          15m       172.16.170.128   server01</span><br><span class="line">kube-system   calico-node-zpwcp                  2/2       Running   0          11m       172.16.170.129   server02</span><br><span class="line">kube-system   coredns-777d78ff6f-5n8bg           1/1       Running   0          16m       10.211.0.4       server01</span><br><span class="line">kube-system   coredns-777d78ff6f-wfm7d           1/1       Running   0          16m       10.211.0.5       server01</span><br><span class="line">kube-system   etcd-server01                      1/1       Running   0          16m       172.16.170.128   server01</span><br><span class="line">kube-system   etcd-server02                      1/1       Running   0          10m       172.16.170.129   server02</span><br><span class="line">kube-system   etcd-server03                      1/1       Running   0          7m        172.16.170.130   server03</span><br><span class="line">kube-system   kube-apiserver-server01            1/1       Running   0          16m       172.16.170.128   server01</span><br><span class="line">kube-system   kube-apiserver-server02            1/1       Running   0          9m        172.16.170.129   server02</span><br><span class="line">kube-system   kube-apiserver-server03            1/1       Running   0          7m        172.16.170.130   server03</span><br><span class="line">kube-system   kube-controller-manager-server01   1/1       Running   0          16m       172.16.170.128   server01</span><br><span class="line">kube-system   kube-controller-manager-server02   1/1       Running   0          9m        172.16.170.129   server02</span><br><span class="line">kube-system   kube-controller-manager-server03   1/1       Running   0          7m        172.16.170.130   server03</span><br><span class="line">kube-system   kube-haproxy-server01              1/1       Running   0          16m       172.16.170.128   server01</span><br><span class="line">kube-system   kube-haproxy-server02              1/1       Running   0          10m       172.16.170.129   server02</span><br><span class="line">kube-system   kube-haproxy-server03              1/1       Running   0          7m        172.16.170.130   server03</span><br><span class="line">kube-system   kube-keepalived-server01           1/1       Running   0          16m       172.16.170.128   server01</span><br><span class="line">kube-system   kube-keepalived-server02           1/1       Running   0          10m       172.16.170.129   server02</span><br><span class="line">kube-system   kube-keepalived-server03           1/1       Running   0          7m        172.16.170.130   server03</span><br><span class="line">kube-system   kube-proxy-88b55                   1/1       Running   0          11m       172.16.170.129   server02</span><br><span class="line">kube-system   kube-proxy-9n9vv                   1/1       Running   0          16m       172.16.170.128   server01</span><br><span class="line">kube-system   kube-proxy-g8lsj                   1/1       Running   0          1m        172.16.170.134   server07</span><br><span class="line">kube-system   kube-proxy-j7lqz                   1/1       Running   0          7m        172.16.170.130   server03</span><br><span class="line">kube-system   kube-proxy-qdhpj                   1/1       Running   0          32s       172.16.170.135   server08</span><br><span class="line">kube-system   kube-scheduler-server01            1/1       Running   0          16m       172.16.170.128   server01</span><br><span class="line">kube-system   kube-scheduler-server02            1/1       Running   0          9m        172.16.170.129   server02</span><br><span class="line">kube-system   kube-scheduler-server03            1/1       Running   0          7m        172.16.170.130   server03</span><br></pre></td></tr></table></figure>
<h1 id="五、参考资料"><a href="#五、参考资料" class="headerlink" title="五、参考资料"></a>五、参考资料</h1><p><a href="https://v1-11.docs.kubernetes.io/docs/setup/independent/high-availability/" target="_blank" rel="noopener">https://v1-11.docs.kubernetes.io/docs/setup/independent/high-availability/</a><br><a href="https://my.oschina.net/u/3433152/blog/1935402" target="_blank" rel="noopener">https://my.oschina.net/u/3433152/blog/1935402</a><br><a href="https://www.jianshu.com/p/49a48752c1a3?utm_source=oschina-app" target="_blank" rel="noopener">https://www.jianshu.com/p/49a48752c1a3?utm_source=oschina-app</a><br><a href="https://tonybai.com/2017/05/15/setup-a-ha-kubernetes-cluster-based-on-kubeadm-part1/" target="_blank" rel="noopener">https://tonybai.com/2017/05/15/setup-a-ha-kubernetes-cluster-based-on-kubeadm-part1/</a><br><a href="https://tonybai.com/2017/05/15/setup-a-ha-kubernetes-cluster-based-on-kubeadm-part2/" target="_blank" rel="noopener">https://tonybai.com/2017/05/15/setup-a-ha-kubernetes-cluster-based-on-kubeadm-part2/</a><br><a href="https://blog.csdn.net/liu_qingbo/article/details/78383892" target="_blank" rel="noopener">https://blog.csdn.net/liu_qingbo/article/details/78383892</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Highly-Available/">Highly Available</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubeadm/">Kubeadm</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Setup/">Setup</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-docker_haproxy" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/01/21/docker_haproxy/" class="article-date">
  	<time datetime="2019-01-21T08:35:32.159Z" itemprop="datePublished">2019-01-21</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/21/docker_haproxy/">
        Docker HAProxy使用示例
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="1-准备Docker-HAProxy基础环境："><a href="#1-准备Docker-HAProxy基础环境：" class="headerlink" title="1. 准备Docker HAProxy基础环境："></a>1. 准备Docker HAProxy基础环境：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker pull haproxy:1.6.5</span><br><span class="line">mkdir haproxy</span><br><span class="line">cd haproxy</span><br></pre></td></tr></table></figure>
<h2 id="2-编写HAProxy的配置文件："><a href="#2-编写HAProxy的配置文件：" class="headerlink" title="2. 编写HAProxy的配置文件："></a>2. 编写HAProxy的配置文件：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    log 127.0.0.1 local0 # 日志输出配置，所有日志都记录在本机，通过local0输出</span><br><span class="line">    maxconn 4096 # 最大连接数</span><br><span class="line">    tune.ssl.default-dh-param 2048</span><br><span class="line">    chroot /usr/local/sbin # 改变当前工作目录</span><br><span class="line">    daemon # 以后台形式运行HAProxy</span><br><span class="line">    nbproc 4 # 启动4个HAProxy实例</span><br><span class="line">    pidfile /usr/local/sbin/haproxy.pid # pid文件位置</span><br><span class="line">defaults</span><br><span class="line">    log 127.0.0.1 local3 # 日志文件的输出定向</span><br><span class="line">    mode http # &#123; tcp|http|health &#125; 设定启动实例的协议类型</span><br><span class="line">    option dontlognull # 保证HAProxy不记录上级负载均衡发送过来的用于检测状态没有数据的心跳包</span><br><span class="line">    option redispatch # 当serverId对应的服务器挂掉后，强制定向到其他健康的服务器</span><br><span class="line">    retries 2 # 重试两次连接失败就认为服务器不可用，主要通过后面的check检查</span><br><span class="line">    maxconn 2000 # 最大连接数</span><br><span class="line">    balance roundrobin # 负载均衡算法，roundrobin表示轮询，source表示按照IP</span><br><span class="line">    timeout connect 5000ms # 连接超时时间</span><br><span class="line">    timeout client 50000ms # 客户端连接超时时间</span><br><span class="line">    timeout server 50000ms # 服务器端连接超时时间</span><br><span class="line">    errorfile 400 /usr/local/etc/haproxy/errors/400.http</span><br><span class="line">    errorfile 403 /usr/local/etc/haproxy/errors/403.http</span><br><span class="line">    errorfile 408 /usr/local/etc/haproxy/errors/408.http</span><br><span class="line">    errorfile 500 /usr/local/etc/haproxy/errors/500.http</span><br><span class="line">    errorfile 502 /usr/local/etc/haproxy/errors/502.http</span><br><span class="line">    errorfile 503 /usr/local/etc/haproxy/errors/503.http</span><br><span class="line">    errorfile 504 /usr/local/etc/haproxy/errors/504.http</span><br><span class="line">    balance source</span><br><span class="line"></span><br><span class="line">frontend http_frontend</span><br><span class="line">        bind *:80</span><br><span class="line">        mode http</span><br><span class="line"></span><br><span class="line">        default_backend server_harbor_backend</span><br><span class="line"></span><br><span class="line">backend server_backend</span><br><span class="line">        mode http</span><br><span class="line">        ...</span><br><span class="line">        server s1 192.168.86.128:31451</span><br><span class="line">        server s2 192.168.86.129:31451</span><br><span class="line">        server s3 192.168.86.130:31451</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<p>注意事项：宿主机的80端口未被占用；根据需要配置server_backend中的使用标记（…）圈上的部分。</p>
<h2 id="3-使用Docker镜像启动HAProxy容器："><a href="#3-使用Docker镜像启动HAProxy容器：" class="headerlink" title="3. 使用Docker镜像启动HAProxy容器："></a>3. 使用Docker镜像启动HAProxy容器：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 80:80 --name haproxy -v `pwd`/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg haproxy:1.6.5</span><br></pre></td></tr></table></figure>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HAProxy/">HAProxy</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2019 Singh Wang
      </div>
        <div class="footer-right">
          <a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/smackgg/hexo-theme-smackdown" target="_blank">Smackdown</a>
        </div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="/js/main.js"></script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js"></script>


  </div>
</body>
</html>