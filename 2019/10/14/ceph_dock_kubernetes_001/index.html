<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  <title>Kubernetes集群对接Ceph集群：搭建可对接Ceph实验环境的Kubernetes实验环境 | Singh Wang</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="一、实验环境说明1. 环境主旨说明本文旨在帮助读者搭建一个可对接Ceph实验环境的Kubernetes实验环境。关于基本Kubernetes环境的搭建，这里不做讲解，读者请参考网络上的资料，或者本博客的另外一篇文章《使用kubeadm的方式安装Kubernetes集群（一）》，链接地址详见“参考资料”。 2. 环境要点说明升级所有节点的内核为主线版本，包括master节点和所有node节点。所有的">
<meta name="keywords" content="Docker,Kubernetes,Storage,Ceph">
<meta property="og:type" content="article">
<meta property="og:title" content="Kubernetes集群对接Ceph集群：搭建可对接Ceph实验环境的Kubernetes实验环境">
<meta property="og:url" content="http://yoursite.com/2019/10/14/ceph_dock_kubernetes_001/index.html">
<meta property="og:site_name" content="Singh Wang">
<meta property="og:description" content="一、实验环境说明1. 环境主旨说明本文旨在帮助读者搭建一个可对接Ceph实验环境的Kubernetes实验环境。关于基本Kubernetes环境的搭建，这里不做讲解，读者请参考网络上的资料，或者本博客的另外一篇文章《使用kubeadm的方式安装Kubernetes集群（一）》，链接地址详见“参考资料”。 2. 环境要点说明升级所有节点的内核为主线版本，包括master节点和所有node节点。所有的">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2019-12-03T02:22:28.431Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Kubernetes集群对接Ceph集群：搭建可对接Ceph实验环境的Kubernetes实验环境">
<meta name="twitter:description" content="一、实验环境说明1. 环境主旨说明本文旨在帮助读者搭建一个可对接Ceph实验环境的Kubernetes实验环境。关于基本Kubernetes环境的搭建，这里不做讲解，读者请参考网络上的资料，或者本博客的另外一篇文章《使用kubeadm的方式安装Kubernetes集群（一）》，链接地址详见“参考资料”。 2. 环境要点说明升级所有节点的内核为主线版本，包括master节点和所有node节点。所有的">
  
    <link rel="alternative" href="/atom.xml" title="Singh Wang" type="application/atom+xml">
  
  
    <link rel="icon" href="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg">
  
  <link rel="stylesheet" href="/css/style.css">
  
  

  <script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>
  <script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>

  
</head></html>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="https://avatars0.githubusercontent.com/u/17465385?s=400&amp;u=cea56ed1710ec6ebab160c1ce1e068317e05b025&amp;v=4" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Singh Wang</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/archives">所有文章</a></li>
				        
							<li><a href="/categories/容器云技术/">容器云技术</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/singhwang" title="github">github</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/Calico/" style="font-size: 11.11px;">Calico</a> <a href="/tags/Ceph/" style="font-size: 12.22px;">Ceph</a> <a href="/tags/Common/" style="font-size: 10px;">Common</a> <a href="/tags/DNS/" style="font-size: 10px;">DNS</a> <a href="/tags/Dashboard/" style="font-size: 11.11px;">Dashboard</a> <a href="/tags/Docker/" style="font-size: 20px;">Docker</a> <a href="/tags/Dragonfly/" style="font-size: 10px;">Dragonfly</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Fluentd/" style="font-size: 11.11px;">Fluentd</a> <a href="/tags/GlusterFS/" style="font-size: 13.33px;">GlusterFS</a> <a href="/tags/HAProxy/" style="font-size: 11.11px;">HAProxy</a> <a href="/tags/Harbor/" style="font-size: 10px;">Harbor</a> <a href="/tags/Highly-Available/" style="font-size: 12.22px;">Highly Available</a> <a href="/tags/Ingress/" style="font-size: 10px;">Ingress</a> <a href="/tags/Kubeadm/" style="font-size: 12.22px;">Kubeadm</a> <a href="/tags/Kubernetes/" style="font-size: 18.89px;">Kubernetes</a> <a href="/tags/Label/" style="font-size: 10px;">Label</a> <a href="/tags/Logging/" style="font-size: 12.22px;">Logging</a> <a href="/tags/Monitoring/" style="font-size: 14.44px;">Monitoring</a> <a href="/tags/Network/" style="font-size: 14.44px;">Network</a> <a href="/tags/Nginx/" style="font-size: 10px;">Nginx</a> <a href="/tags/Open-Falcon/" style="font-size: 10px;">Open Falcon</a> <a href="/tags/P2P/" style="font-size: 10px;">P2P</a> <a href="/tags/Prometheus/" style="font-size: 12.22px;">Prometheus</a> <a href="/tags/Setup/" style="font-size: 17.78px;">Setup</a> <a href="/tags/Smartping/" style="font-size: 10px;">Smartping</a> <a href="/tags/Source-Build/" style="font-size: 10px;">Source Build</a> <a href="/tags/Storage/" style="font-size: 15.56px;">Storage</a> <a href="/tags/TimeZone/" style="font-size: 10px;">TimeZone</a> <a href="/tags/Trouble-Shooting/" style="font-size: 16.67px;">Trouble Shooting</a>
					</div>
				</section>
				
				
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">Singh Wang</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="https://avatars0.githubusercontent.com/u/17465385?s=400&amp;u=cea56ed1710ec6ebab160c1ce1e068317e05b025&amp;v=4" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author">Singh Wang</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/archives">所有文章</a></li>
		        
					<li><a href="/categories/容器云技术/">容器云技术</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/singhwang" title="github">github</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap"><article id="post-ceph_dock_kubernetes_001" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/10/14/ceph_dock_kubernetes_001/" class="article-date">
  	<time datetime="2019-10-14T03:04:19.009Z" itemprop="datePublished">2019-10-14</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Kubernetes集群对接Ceph集群：搭建可对接Ceph实验环境的Kubernetes实验环境
      
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Ceph/">Ceph</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Storage/">Storage</a></li></ul>
	</div>

        
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


        
        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、实验环境说明"><a href="#一、实验环境说明" class="headerlink" title="一、实验环境说明"></a>一、实验环境说明</h1><h2 id="1-环境主旨说明"><a href="#1-环境主旨说明" class="headerlink" title="1. 环境主旨说明"></a>1. 环境主旨说明</h2><p>本文旨在帮助读者搭建一个可对接Ceph实验环境的Kubernetes实验环境。关于基本Kubernetes环境的搭建，这里不做讲解，读者请参考网络上的资料，或者本博客的另外一篇文章《使用kubeadm的方式安装Kubernetes集群（一）》，链接地址详见“参考资料”。</p>
<h2 id="2-环境要点说明"><a href="#2-环境要点说明" class="headerlink" title="2. 环境要点说明"></a>2. 环境要点说明</h2><p>升级所有节点的内核为主线版本，包括master节点和所有node节点。<br>所有的节点都安装ceph-common组件和python-cephfs组件，包括master节点和所有node节点。</p>
<h1 id="二、实验环境版本信息"><a href="#二、实验环境版本信息" class="headerlink" title="二、实验环境版本信息"></a>二、实验环境版本信息</h1><h2 id="1-操作系统的版本信息"><a href="#1-操作系统的版本信息" class="headerlink" title="1. 操作系统的版本信息"></a>1. 操作系统的版本信息</h2><p>CentOS Linux release 7.7.1908 (Core)</p>
<h2 id="2-核心组件的版本信息"><a href="#2-核心组件的版本信息" class="headerlink" title="2. 核心组件的版本信息"></a>2. 核心组件的版本信息</h2><p>Ceph Luminous 版本 的 ceph-common 和 python-cephfs<br>Kubernetes v1.16.0</p>
<h1 id="三、实验步骤"><a href="#三、实验步骤" class="headerlink" title="三、实验步骤"></a>三、实验步骤</h1><h2 id="1-升级所有节点（所有的master和node节点）的内核为主线版本（当前主线版本为-5-3-6-1-el7-elrepo-x86-64）"><a href="#1-升级所有节点（所有的master和node节点）的内核为主线版本（当前主线版本为-5-3-6-1-el7-elrepo-x86-64）" class="headerlink" title="1. 升级所有节点（所有的master和node节点）的内核为主线版本（当前主线版本为 5.3.6-1.el7.elrepo.x86_64）"></a>1. 升级所有节点（所有的master和node节点）的内核为主线版本（当前主线版本为 5.3.6-1.el7.elrepo.x86_64）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">yum install -y yum-plugin-fastestmirror</span><br><span class="line">cat /etc/redhat-release</span><br><span class="line">cat /etc/os-release</span><br><span class="line">uname -snr</span><br><span class="line"></span><br><span class="line">rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org</span><br><span class="line">rpm -Uvh https://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm</span><br><span class="line">yum repolist</span><br><span class="line"></span><br><span class="line">yum --enablerepo=elrepo-kernel install -y kernel-ml</span><br><span class="line">yum repolist all</span><br><span class="line"></span><br><span class="line">awk -F\&apos; &apos;$1==&quot;menuentry &quot; &#123;print i++ &quot; : &quot; $2&#125;&apos; /etc/grub2.cfg</span><br><span class="line">grub2-set-default 0</span><br><span class="line">grub2-mkconfig -o /boot/grub2/grub.cfg</span><br><span class="line">reboot</span><br><span class="line"></span><br><span class="line">uname -snr</span><br></pre></td></tr></table></figure>
<h2 id="2-所有节点（所有的master和node节点）安装-ceph-luminous-版本-的-ceph-common-和-python-cephfs"><a href="#2-所有节点（所有的master和node节点）安装-ceph-luminous-版本-的-ceph-common-和-python-cephfs" class="headerlink" title="2. 所有节点（所有的master和node节点）安装 ceph luminous 版本 的 ceph-common 和 python-cephfs"></a>2. 所有节点（所有的master和node节点）安装 ceph luminous 版本 的 ceph-common 和 python-cephfs</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt;&gt; /etc/yum.repos.d/ceph.repo</span><br><span class="line">[ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=http://mirrors.163.com/ceph/rpm-luminous/el7/noarch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">priority=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=http://mirrors.163.com/ceph/keys/release.asc</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">yum makecache fast</span><br><span class="line">yum install -y ceph-common python-cephfs</span><br></pre></td></tr></table></figure>
<h2 id="3-为-Kubernetes-集群安装-ceph-rbd-和-ceph-fs-的-对应的-provisioner-服务"><a href="#3-为-Kubernetes-集群安装-ceph-rbd-和-ceph-fs-的-对应的-provisioner-服务" class="headerlink" title="3. 为 Kubernetes 集群安装 ceph rbd 和 ceph fs 的 对应的 provisioner 服务"></a>3. 为 Kubernetes 集群安装 ceph rbd 和 ceph fs 的 对应的 provisioner 服务</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p external-storage/ceph/common/</span><br><span class="line">mkdir -p external-storage/ceph/rbd/</span><br><span class="line">mkdir -p external-storage/ceph/fs/</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; external-storage/ceph/common/01-namespaces.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: storage</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># For ceph rbd</span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; external-storage/ceph/rbd/01-serviceaccount.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: cephrbd-provisioner</span><br><span class="line">  namespace: storage</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; external-storage/ceph/rbd/02-clusterrole.yaml</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: cephrbd-provisioner</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;endpoints&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;secrets&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;, &quot;create&quot;, &quot;update&quot;, &quot;delete&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;persistentvolumes&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;persistentvolumeclaims&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]</span><br><span class="line">  - apiGroups: [&quot;storage.k8s.io&quot;]</span><br><span class="line">    resources: [&quot;storageclasses&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;events&quot;]</span><br><span class="line">    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;services&quot;]</span><br><span class="line">    resourceNames: [&quot;kube-dns&quot;,&quot;coredns&quot;]</span><br><span class="line">    verbs: [&quot;list&quot;, &quot;get&quot;]</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; external-storage/ceph/rbd/03-clusterrolebinding.yaml</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: cephrbd-provisioner</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: cephrbd-provisioner</span><br><span class="line">    namespace: storage</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cephrbd-provisioner</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; external-storage/ceph/rbd/04-deployment.yaml</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: cephrbd-provisioner</span><br><span class="line">  namespace: storage</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: cephrbd-provisioner</span><br><span class="line">  strategy:</span><br><span class="line">    type: Recreate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: cephrbd-provisioner</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">        - name: cephrbd-provisioner</span><br><span class="line">          image: wangx/rbd-provisioner:luminous</span><br><span class="line">          imagePullPolicy: IfNotPresent</span><br><span class="line">          env:</span><br><span class="line">            - name: PROVISIONER_NAME</span><br><span class="line">              value: ceph.com/rbd</span><br><span class="line">      serviceAccount: cephrbd-provisioner</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># For ceph fs</span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; external-storage/ceph/fs/01-serviceaccount.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: cephfs-provisioner</span><br><span class="line">  namespace: storage</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; external-storage/ceph/fs/02-clusterrole.yaml</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: cephfs-provisioner</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;endpoints&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;secrets&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;, &quot;create&quot;, &quot;update&quot;, &quot;delete&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;persistentvolumes&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;persistentvolumeclaims&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]</span><br><span class="line">  - apiGroups: [&quot;storage.k8s.io&quot;]</span><br><span class="line">    resources: [&quot;storageclasses&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;events&quot;]</span><br><span class="line">    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; external-storage/ceph/fs/03-clusterrolebinding.yaml</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: cephfs-provisioner</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: cephfs-provisioner</span><br><span class="line">    namespace: storage</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cephfs-provisioner</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; external-storage/ceph/fs/04-deployment.yaml</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: cephfs-provisioner</span><br><span class="line">  namespace: storage</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: cephfs-provisioner</span><br><span class="line">  strategy:</span><br><span class="line">    type: Recreate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: cephfs-provisioner</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">        - name: cephfs-provisioner</span><br><span class="line">          image: wangx/cephfs-provisioner:luminous</span><br><span class="line">          imagePullPolicy: IfNotPresent</span><br><span class="line">          env:</span><br><span class="line">            - name: PROVISIONER_NAME</span><br><span class="line">              value: ceph.com/cephfs</span><br><span class="line">          command:</span><br><span class="line">            - &quot;/usr/local/bin/cephfs-provisioner&quot;</span><br><span class="line">          args:</span><br><span class="line">            - &quot;-id=cephfs-provisioner-1&quot;</span><br><span class="line">      serviceAccount: cephfs-provisioner</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># crete ceph provisioner&apos;s namespace</span><br><span class="line">kubectl create -f external-storage/ceph/common/</span><br><span class="line"></span><br><span class="line"># create ceph rbd provisioner</span><br><span class="line">kubectl create -f external-storage/ceph/rbd/</span><br><span class="line"></span><br><span class="line"># create ceph fs provisioner</span><br><span class="line">kubectl create -f external-storage/ceph/fs/</span><br></pre></td></tr></table></figure>
<h2 id="4-为-Kubernetes-集群的-ceph-rbd-和-ceph-fs-的-provisioner-服务创建-StroageClass-对象"><a href="#4-为-Kubernetes-集群的-ceph-rbd-和-ceph-fs-的-provisioner-服务创建-StroageClass-对象" class="headerlink" title="4. 为 Kubernetes 集群的 ceph rbd 和 ceph fs 的 provisioner 服务创建 StroageClass 对象"></a>4. 为 Kubernetes 集群的 ceph rbd 和 ceph fs 的 provisioner 服务创建 StroageClass 对象</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"># For ceph rbd </span><br><span class="line">## 注意：这里需要更换两个Secret的key的值为你的环境的。</span><br><span class="line">mkdir -p external-storage/ceph/rbd/storageclass/</span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; external-storage/ceph/rbd/storageclass/01-secrets.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: cephrbd-admin-secret</span><br><span class="line">  namespace: storage</span><br><span class="line">type: &quot;kubernetes.io/cephrbd&quot;</span><br><span class="line">data:</span><br><span class="line">  # ceph auth get-key client.admin | base64</span><br><span class="line">  key: QVFDTmZxRmRDRmtnT3hBQURwY29VdjltbGJqRmIxMTJ2dzlLdEE9PQ==</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: cephrbd-user-secret</span><br><span class="line">  namespace: storage</span><br><span class="line">type: &quot;kubernetes.io/cephrbd&quot;</span><br><span class="line">data:</span><br><span class="line">  # ceph auth add client.kube mon &apos;allow r&apos; osd &apos;allow rwx pool=kube&apos;</span><br><span class="line">  # ceph auth get-key client.kube | base64</span><br><span class="line">  key: QVFBZ3Q2RmRibnBOTXhBQXkwQkJrdmQxQW5adHlWN0syZWIvSEE9PQ==</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; external-storage/ceph/rbd/storageclass/02-storageclass.yaml</span><br><span class="line">kind: StorageClass</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: cephrbd</span><br><span class="line">provisioner: ceph.com/rbd</span><br><span class="line">parameters:</span><br><span class="line">  monitors: ceph-mon.storage.svc.cluster.local:6789 # 这里需要使用Kubernetes内部的DNS配置ceph monitor的地址</span><br><span class="line">  pool: kube</span><br><span class="line">  adminId: admin</span><br><span class="line">  adminSecretNamespace: storage</span><br><span class="line">  adminSecretName: cephrbd-admin-secret</span><br><span class="line">  userId: kube</span><br><span class="line">  userSecretNamespace: storage</span><br><span class="line">  userSecretName: cephrbd-user-secret</span><br><span class="line">  imageFormat: &quot;2&quot;</span><br><span class="line">  imageFeatures: layering</span><br><span class="line">---</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: ceph-mon</span><br><span class="line">  namespace: storage</span><br><span class="line">spec:</span><br><span class="line">  type: ExternalName</span><br><span class="line">  externalName: 192.168.112.131.xip.io # ceph monitor的地址</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl create -f external-storage/ceph/rbd/storageclass/</span><br><span class="line"></span><br><span class="line"># For ceph fs</span><br><span class="line">## 注意：这里需要更换Secret的key的值为你的环境的。</span><br><span class="line">mkdir -p external-storage/ceph/fs/storageclass/</span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; external-storage/ceph/fs/storageclass/01-secrets.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: cephfs-admin-secret</span><br><span class="line">  namespace: storage</span><br><span class="line">type: &quot;kubernetes.io/cephfs&quot;</span><br><span class="line">data:</span><br><span class="line">  # ceph auth get-key client.admin | base64</span><br><span class="line">  key: QVFDTmZxRmRDRmtnT3hBQURwY29VdjltbGJqRmIxMTJ2dzlLdEE9PQ==</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; external-storage/ceph/fs/storageclass/02-storageclass.yaml</span><br><span class="line">kind: StorageClass</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: cephfs</span><br><span class="line">provisioner: ceph.com/cephfs</span><br><span class="line">parameters:</span><br><span class="line">  monitors: 192.168.112.131:6789</span><br><span class="line">  adminId: admin</span><br><span class="line">  adminSecretName: cephfs-admin-secret</span><br><span class="line">  adminSecretNamespace: storage</span><br><span class="line">  claimRoot: /volumes/kubernetes</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl create -f external-storage/ceph/fs/storageclass/</span><br></pre></td></tr></table></figure>
<h2 id="5-验证-Kubernetes-集群的-ceph-rbd-和-ceph-fs-的-provisioner-服务配合StroageClass-对象实现的动态存储供应功能"><a href="#5-验证-Kubernetes-集群的-ceph-rbd-和-ceph-fs-的-provisioner-服务配合StroageClass-对象实现的动态存储供应功能" class="headerlink" title="5. 验证 Kubernetes 集群的 ceph rbd 和 ceph fs 的 provisioner 服务配合StroageClass 对象实现的动态存储供应功能"></a>5. 验证 Kubernetes 集群的 ceph rbd 和 ceph fs 的 provisioner 服务配合StroageClass 对象实现的动态存储供应功能</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br></pre></td><td class="code"><pre><span class="line"># 创建pvc和pod用于验证ceph rbd</span><br><span class="line">mkdir -p external-storage/ceph/rbd/example/</span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; external-storage/ceph/rbd/example/01-claim.yaml</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: claim1</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  storageClassName: cephrbd</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; external-storage/ceph/rbd/example/02-pod.yaml</span><br><span class="line">kind: Pod</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: test-pod-1</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: test-pod-1</span><br><span class="line">      image: nginx:1.7.9</span><br><span class="line">      imagePullPolicy: IfNotPresent</span><br><span class="line">      ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - name: pvc</span><br><span class="line">          mountPath: &quot;/data&quot;</span><br><span class="line">  volumes:</span><br><span class="line">    - name: pvc</span><br><span class="line">      persistentVolumeClaim:</span><br><span class="line">        claimName: claim1</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl create -f external-storage/ceph/rbd/example/</span><br><span class="line"></span><br><span class="line"># 创建pvc和pod用于验证ceph fs</span><br><span class="line">mkdir -p external-storage/ceph/fs/example/</span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; external-storage/ceph/fs/example/01-claim.yaml</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: claim2</span><br><span class="line">spec:</span><br><span class="line">  storageClassName: cephfs</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; external-storage/ceph/fs/example/02-pod.yaml</span><br><span class="line">kind: Pod</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: test-pod-2</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: test-pod-2</span><br><span class="line">      image: nginx:1.7.9</span><br><span class="line">      imagePullPolicy: IfNotPresent</span><br><span class="line">      ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - name: pvc</span><br><span class="line">          mountPath: &quot;/data&quot;</span><br><span class="line">  volumes:</span><br><span class="line">    - name: pvc</span><br><span class="line">      persistentVolumeClaim:</span><br><span class="line">        claimName: claim2</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl create -f external-storage/ceph/fs/example/</span><br><span class="line"></span><br><span class="line"># 验证步骤如下所示：</span><br><span class="line">[root@master ~]# kubectl get pvc</span><br><span class="line">NAME     STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">claim1   Bound    pvc-8bcd7a8a-9c72-4c24-a5ce-dda3f72b459c   1Gi        RWO            cephrbd        5m22s</span><br><span class="line">claim2   Bound    pvc-6aeaf23b-c1c0-4654-8fa9-50656b5b7247   1Gi        RWX            cephfs         3m43s</span><br><span class="line"></span><br><span class="line">[root@master ~]# kubectl get pv</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM            STORAGECLASS   REASON   AGE</span><br><span class="line">pvc-6aeaf23b-c1c0-4654-8fa9-50656b5b7247   1Gi        RWX            Delete           Bound    default/claim2   cephfs                  3m55s</span><br><span class="line">pvc-8bcd7a8a-9c72-4c24-a5ce-dda3f72b459c   1Gi        RWO            Delete           Bound    default/claim1   cephrbd                 5m36s</span><br><span class="line"></span><br><span class="line">[root@master ~]# kubectl get pod -o wide</span><br><span class="line">NAME            READY   STATUS    RESTARTS   AGE     IP               NODE     NOMINATED NODE   READINESS GATES</span><br><span class="line">test-pod-1      1/1     Running   0          6m2s    10.211.196.133   node01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">test-pod-2      1/1     Running   0          4m23s   10.211.140.69    node02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line">## 进入pod下的container中</span><br><span class="line">[root@master ~]# kubectl exec -it test-pod-1 /bin/bash</span><br><span class="line">root@test-pod-1:/# df -h</span><br><span class="line">Filesystem           Size  Used Avail Use% Mounted on</span><br><span class="line">overlay               17G  3.6G   14G  21% /</span><br><span class="line">tmpfs                 64M     0   64M   0% /dev</span><br><span class="line">tmpfs                982M     0  982M   0% /sys/fs/cgroup</span><br><span class="line">/dev/rbd0            976M  2.6M  958M   1% /data</span><br><span class="line">/dev/mapper/cl-root   17G  3.6G   14G  21% /dev/termination-log</span><br><span class="line">/dev/mapper/cl-root   17G  3.6G   14G  21% /etc/resolv.conf</span><br><span class="line">/dev/mapper/cl-root   17G  3.6G   14G  21% /etc/hostname</span><br><span class="line">/dev/mapper/cl-root   17G  3.6G   14G  21% /etc/hosts</span><br><span class="line">shm                   64M     0   64M   0% /dev/shm</span><br><span class="line">/dev/mapper/cl-root   17G  3.6G   14G  21% /var/cache/nginx</span><br><span class="line">tmpfs                982M   12K  982M   1% /run/secrets/kubernetes.io/serviceaccount</span><br><span class="line">tmpfs                982M     0  982M   0% /proc/acpi</span><br><span class="line">tmpfs                 64M     0   64M   0% /proc/kcore</span><br><span class="line">tmpfs                 64M     0   64M   0% /proc/keys</span><br><span class="line">tmpfs                 64M     0   64M   0% /proc/timer_list</span><br><span class="line">tmpfs                 64M     0   64M   0% /proc/sched_debug</span><br><span class="line">tmpfs                982M     0  982M   0% /proc/scsi</span><br><span class="line">tmpfs                982M     0  982M   0% /sys/firmware</span><br><span class="line">root@test-pod-1:/# cd /data/</span><br><span class="line">root@test-pod-1:/data# ls -la</span><br><span class="line">total 20</span><br><span class="line">drwxr-xr-x 3 root root  4096 Oct 14 05:43 .</span><br><span class="line">drwxr-xr-x 1 root root    41 Oct 14 05:39 ..</span><br><span class="line">drwx------ 2 root root 16384 Oct 14 05:39 lost+found</span><br><span class="line">root@test-pod-1:/data# echo &apos;hello ceph rbd.&apos; &gt; readme.md</span><br><span class="line">root@test-pod-1:/data# ls -la</span><br><span class="line">total 24</span><br><span class="line">drwxr-xr-x 3 root root  4096 Oct 14 05:43 .</span><br><span class="line">drwxr-xr-x 1 root root    41 Oct 14 05:39 ..</span><br><span class="line">drwx------ 2 root root 16384 Oct 14 05:39 lost+found</span><br><span class="line">-rw-r--r-- 1 root root    16 Oct 14 05:43 readme.md</span><br><span class="line">root@test-pod-1:/data# cat readme.md</span><br><span class="line">hello ceph rbd.</span><br><span class="line">root@test-pod-1:/data# exit</span><br><span class="line">exit</span><br><span class="line"></span><br><span class="line">[root@master ~]# kubectl exec -it test-pod-2 /bin/bash</span><br><span class="line">root@test-pod-2:/# df -h</span><br><span class="line">Filesystem                                                                                                       Size  Used Avail Use% Mounted on</span><br><span class="line">overlay                                                                                                           17G  3.6G   14G  21% /</span><br><span class="line">tmpfs                                                                                                             64M     0   64M   0% /dev</span><br><span class="line">tmpfs                                                                                                            982M     0  982M   0% /sys/fs/cgroup</span><br><span class="line">192.168.112.131:6789:/volumes/kubernetes/kubernetes/kubernetes-dynamic-pvc-41646ced-ee45-11e9-bfd9-eec9a057c13d   18G     0   18G   0% /data</span><br><span class="line">/dev/mapper/cl-root                                                                                               17G  3.6G   14G  21% /dev/termination-log</span><br><span class="line">/dev/mapper/cl-root                                                                                               17G  3.6G   14G  21% /etc/resolv.conf</span><br><span class="line">/dev/mapper/cl-root                                                                                               17G  3.6G   14G  21% /etc/hostname</span><br><span class="line">/dev/mapper/cl-root                                                                                               17G  3.6G   14G  21% /etc/hosts</span><br><span class="line">shm                                                                                                               64M     0   64M   0% /dev/shm</span><br><span class="line">/dev/mapper/cl-root                                                                                               17G  3.6G   14G  21% /var/cache/nginx</span><br><span class="line">tmpfs                                                                                                            982M   12K  982M   1% /run/secrets/kubernetes.io/serviceaccount</span><br><span class="line">tmpfs                                                                                                            982M     0  982M   0% /proc/acpi</span><br><span class="line">tmpfs                                                                                                             64M     0   64M   0% /proc/kcore</span><br><span class="line">tmpfs                                                                                                             64M     0   64M   0% /proc/keys</span><br><span class="line">tmpfs                                                                                                             64M     0   64M   0% /proc/timer_list</span><br><span class="line">tmpfs                                                                                                             64M     0   64M   0% /proc/sched_debug</span><br><span class="line">tmpfs                                                                                                            982M     0  982M   0% /proc/scsi</span><br><span class="line">tmpfs                                                                                                            982M     0  982M   0% /sys/firmware</span><br><span class="line">root@test-pod-2:/# cd /data/</span><br><span class="line">root@test-pod-2:/data# ls -la</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x 2 root root  0 Oct 14 05:41 .</span><br><span class="line">drwxr-xr-x 1 root root 29 Oct 14 05:41 ..</span><br><span class="line">root@test-pod-2:/data# echo &apos;hello ceph fs.&apos; &gt; readme.md</span><br><span class="line">root@test-pod-2:/data# ls -la</span><br><span class="line">total 1</span><br><span class="line">drwxr-xr-x 2 root root  1 Oct 14 05:44 .</span><br><span class="line">drwxr-xr-x 1 root root 29 Oct 14 05:41 ..</span><br><span class="line">-rw-r--r-- 1 root root 15 Oct 14 05:44 readme.md</span><br><span class="line">root@test-pod-2:/data# cat readme.md</span><br><span class="line">hello ceph fs.</span><br><span class="line">root@test-pod-2:/data# exit</span><br><span class="line">exit</span><br><span class="line"></span><br><span class="line"># 在test-pod-1的宿主机上</span><br><span class="line">[root@node01 ~]# df -h</span><br><span class="line">Filesystem           Size  Used Avail Use% Mounted on</span><br><span class="line">devtmpfs             969M     0  969M   0% /dev</span><br><span class="line">tmpfs                982M     0  982M   0% /dev/shm</span><br><span class="line">tmpfs                982M   23M  959M   3% /run</span><br><span class="line">tmpfs                982M     0  982M   0% /sys/fs/cgroup</span><br><span class="line">/dev/mapper/cl-root   17G  3.6G   14G  21% /</span><br><span class="line">/dev/sda1           1014M  242M  773M  24% /boot</span><br><span class="line">tmpfs                982M   12K  982M   1% /var/lib/kubelet/pods/444da5da-6b0b-4eb9-961c-e67620e2790d/volumes/kubernetes.io~secret/calico-node-token-j29td</span><br><span class="line">tmpfs                982M   12K  982M   1% /var/lib/kubelet/pods/5c6b4dc9-9789-431f-abda-69b791e00852/volumes/kubernetes.io~secret/kube-proxy-token-2xvqz</span><br><span class="line">tmpfs                982M   12K  982M   1% /var/lib/kubelet/pods/117bf0a0-1de4-4233-8f86-58e3b7e222d9/volumes/kubernetes.io~secret/default-token-lvg9j</span><br><span class="line">overlay               17G  3.6G   14G  21% /var/lib/docker/overlay2/c0137c8945f708c9a9af29c435006164f1880e62e12d0944558d76fe826cf79e/merged</span><br><span class="line">overlay               17G  3.6G   14G  21% /var/lib/docker/overlay2/a173aac7b6bd051d9309d31dc98bffc8d8b4601738ad8670981c65e9f206f4d7/merged</span><br><span class="line">shm                   64M     0   64M   0% /var/lib/docker/containers/af5a902877155cc3f0d80506e4af572364e1b02b7527ebc272e36cc3537fdaf0/mounts/shm</span><br><span class="line">shm                   64M     0   64M   0% /var/lib/docker/containers/779e0a0a9e8e46ba53b976b9e0e0acb133778159c001d314d9fb4a689ce4dd51/mounts/shm</span><br><span class="line">overlay               17G  3.6G   14G  21% /var/lib/docker/overlay2/e0f0e09989f3e152c17f2ccf7e15df86a0844f59277804d409126e7508e892f2/merged</span><br><span class="line">overlay               17G  3.6G   14G  21% /var/lib/docker/overlay2/6cf53e19aa7a1cabfcc992b492f0c2d7b3a5b7a41a999e3e17c6e85acd432385/merged</span><br><span class="line">tmpfs                197M     0  197M   0% /run/user/0</span><br><span class="line">overlay               17G  3.6G   14G  21% /var/lib/docker/overlay2/ea9758ae1391c0b51be400d2c716398a6e6a75042c2d266b0fd5fffce49f1856/merged</span><br><span class="line">shm                   64M     0   64M   0% /var/lib/docker/containers/78374c2866c9e59956b46e6cf7745743e5366273eae4a28f584167895eff28c3/mounts/shm</span><br><span class="line">overlay               17G  3.6G   14G  21% /var/lib/docker/overlay2/f75360891104dd17f6035b84fc53e2cee9aa49cf4cb99abbc07ea4e53f3e68fa/merged</span><br><span class="line">tmpfs                982M   12K  982M   1% /var/lib/kubelet/pods/bc337b72-b78d-4f22-9074-0ea1e3c0854b/volumes/kubernetes.io~secret/cephfs-provisioner-token-gnvmf</span><br><span class="line">overlay               17G  3.6G   14G  21% /var/lib/docker/overlay2/5f2bb0260a595ebf2685f606c8fa57ae312b36d9ed0d2ac1c0226f108dcb4f8a/merged</span><br><span class="line">shm                   64M     0   64M   0% /var/lib/docker/containers/2be9a9eab8bf6181da1b00bd6745053236b5b72b4a625244f58664d6cef4056d/mounts/shm</span><br><span class="line">overlay               17G  3.6G   14G  21% /var/lib/docker/overlay2/19c0add3e797d302865131a736187e08b5773ff504cf24baa5ca6dc9e1e2f0bd/merged</span><br><span class="line">tmpfs                982M   12K  982M   1% /var/lib/kubelet/pods/94567637-87d1-44bf-b40c-4cf8d7249455/volumes/kubernetes.io~secret/default-token-lvg9j</span><br><span class="line">/dev/rbd0            976M  2.6M  958M   1% /var/lib/kubelet/plugins/kubernetes.io/rbd/mounts/kube-image-kubernetes-dynamic-pvc-06ed8402-ee45-11e9-81e1-626518d2252b</span><br><span class="line">overlay               17G  3.6G   14G  21% /var/lib/docker/overlay2/0f9b3b248dd6fd5db871c9a8340c3b221099ca48e5e44496b0e8ff87d385053e/merged</span><br><span class="line">shm                   64M     0   64M   0% /var/lib/docker/containers/7fa70409cd56a31427c89a61db4d102261610678a06a550eecfd1c44092c0d02/mounts/shm</span><br><span class="line">overlay               17G  3.6G   14G  21% /var/lib/docker/overlay2/723eda3646d4eabdb62faa89566f0c6d84a14f7f882acee46af96630d96480e4/merged</span><br><span class="line"></span><br><span class="line"># 在test-pod-2的宿主机上</span><br><span class="line">[root@node02 ~]# df -h</span><br><span class="line">Filesystem                                                                                                       Size  Used Avail Use% Mounted on</span><br><span class="line">devtmpfs                                                                                                         969M     0  969M   0% /dev</span><br><span class="line">tmpfs                                                                                                            982M     0  982M   0% /dev/shm</span><br><span class="line">tmpfs                                                                                                            982M   22M  960M   3% /run</span><br><span class="line">tmpfs                                                                                                            982M     0  982M   0% /sys/fs/cgroup</span><br><span class="line">/dev/mapper/cl-root                                                                                               17G  3.6G   14G  21% /</span><br><span class="line">/dev/sda1                                                                                                       1014M  242M  773M  24% /boot</span><br><span class="line">tmpfs                                                                                                            982M   12K  982M   1% /var/lib/kubelet/pods/cde8bcd0-da54-40f8-90e1-a8c53daaca8a/volumes/kubernetes.io~secret/default-token-lvg9j</span><br><span class="line">tmpfs                                                                                                            982M   12K  982M   1% /var/lib/kubelet/pods/27785efc-053d-41c1-b081-d61056715dce/volumes/kubernetes.io~secret/kube-proxy-token-2xvqz</span><br><span class="line">tmpfs                                                                                                            982M   12K  982M   1% /var/lib/kubelet/pods/6af3ba89-fd1d-4ca6-9f1f-d1a9d24aab68/volumes/kubernetes.io~secret/calico-node-token-j29td</span><br><span class="line">overlay                                                                                                           17G  3.6G   14G  21% /var/lib/docker/overlay2/fe5b85b1611ec8ce24761d70dec417d8102f71c13dc3ba0005387999d7f90538/merged</span><br><span class="line">overlay                                                                                                           17G  3.6G   14G  21% /var/lib/docker/overlay2/a2e0efa3dd1bc8d4725acd7ea3123cf2331cabfbd3f58185de44f81dc61f17b2/merged</span><br><span class="line">shm                                                                                                               64M     0   64M   0% /var/lib/docker/containers/a9a00476e189f794aaa8d448f3fd492ed95d7f285ebe398f2561c3369f98f6fc/mounts/shm</span><br><span class="line">shm                                                                                                               64M     0   64M   0% /var/lib/docker/containers/97c5b22f15b951a1d8c88deae0a34163d729cedc2fd21aef2108e657d3dbd2ed/mounts/shm</span><br><span class="line">overlay                                                                                                           17G  3.6G   14G  21% /var/lib/docker/overlay2/d69d00d1bb9d51e3b2a622edfe213b16069886f6a3507b7d22f8cf35aadb67fb/merged</span><br><span class="line">overlay                                                                                                           17G  3.6G   14G  21% /var/lib/docker/overlay2/bc97fe5450c049b42c16ed70907ebf759fd16d8aa0f1f1b597828c400a95fec1/merged</span><br><span class="line">tmpfs                                                                                                            197M     0  197M   0% /run/user/0</span><br><span class="line">overlay                                                                                                           17G  3.6G   14G  21% /var/lib/docker/overlay2/be31ca99791b3a61b34750063296b3f61e0e33f6ca13bd89d09305e29282d93f/merged</span><br><span class="line">shm                                                                                                               64M     0   64M   0% /var/lib/docker/containers/75bf7cd29b86a191287b60b505ca9147aee2590026227b75c108e527c8a42050/mounts/shm</span><br><span class="line">overlay                                                                                                           17G  3.6G   14G  21% /var/lib/docker/overlay2/d424bbdc32abf0a7860ea8cc57138a578d3ac379a2a5a62d467ebdd503f85f12/merged</span><br><span class="line">tmpfs                                                                                                            982M   12K  982M   1% /var/lib/kubelet/pods/63e075b8-1b82-4ead-924b-e8233217c597/volumes/kubernetes.io~secret/cephrbd-provisioner-token-xx8qj</span><br><span class="line">overlay                                                                                                           17G  3.6G   14G  21% /var/lib/docker/overlay2/032279366ff9f083d5f54e8653789f77d30962fb74d9c796912462336ba089a8/merged</span><br><span class="line">shm                                                                                                               64M     0   64M   0% /var/lib/docker/containers/8e3f8bcbbb05781e69759fe678fdba68fa6ec0514972b3a399936c90d754ea1d/mounts/shm</span><br><span class="line">overlay                                                                                                           17G  3.6G   14G  21% /var/lib/docker/overlay2/ca226a39a87d3319f974bdbca242ff6621cafe356a24e9080d65b013f26b30ef/merged</span><br><span class="line">tmpfs                                                                                                            982M   12K  982M   1% /var/lib/kubelet/pods/094d7470-5712-4523-8eb8-9994dbfb2cfe/volumes/kubernetes.io~secret/default-token-lvg9j</span><br><span class="line">192.168.112.131:6789:/volumes/kubernetes/kubernetes/kubernetes-dynamic-pvc-41646ced-ee45-11e9-bfd9-eec9a057c13d   18G     0   18G   0% /var/lib/kubelet/pods/094d7470-5712-4523-8eb8-9994dbfb2cfe/volumes/kubernetes.io~cephfs/pvc-6aeaf23b-c1c0-4654-8fa9-50656b5b7247</span><br><span class="line">overlay                                                                                                           17G  3.6G   14G  21% /var/lib/docker/overlay2/7615fa1f189797ca2b111fa6122ea3a1d8c2f2c009d052a3e06c6a9e9d0b9ba3/merged</span><br><span class="line">shm                                                                                                               64M     0   64M   0% /var/lib/docker/containers/0a76bdb1043188470fa7a2aa3df3ce9640b658064cb41c872025b90ed46f6bcd/mounts/shm</span><br><span class="line">overlay                                                                                                           17G  3.6G   14G  21% /var/lib/docker/overlay2/49ef6810dedcd675d541f06df84336744a7cbb84a876daf96ffab7bbcfa304f7/merged</span><br></pre></td></tr></table></figure>
<h1 id="四、参考资料"><a href="#四、参考资料" class="headerlink" title="四、参考资料"></a>四、参考资料</h1><p><a href="https://github.com/kubernetes-incubator/external-storage/tree/v5.2.0/ceph" target="_blank" rel="noopener">https://github.com/kubernetes-incubator/external-storage/tree/v5.2.0/ceph</a><br><a href="https://www.howtoforge.com/tutorial/how-to-upgrade-kernel-in-centos-7-server/" target="_blank" rel="noopener">https://www.howtoforge.com/tutorial/how-to-upgrade-kernel-in-centos-7-server/</a><br><a href="https://singhwang.github.io/2019/10/03/kubeadm_kubernetes_cluster_000/" target="_blank" rel="noopener">https://singhwang.github.io/2019/10/03/kubeadm_kubernetes_cluster_000/</a></p>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/01/03/binary_kubernetes_cluster_001/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">&lt;</strong>
      <div class="article-nav-title">
        
          使用二进制文件的方式安装 Kubernetes v1.17.0 集群（二）
        
      </div>
    </a>
  
  
    <a href="/2019/10/12/ceph_dock_kubernetes_000/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Kubernetes集群对接Ceph集群：搭建可供Kubernetes对接的Ceph实验环境</div>
      <strong class="article-nav-caption">&gt;</strong>
    </a>
  
</nav>

  
</article>






</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2020 Singh Wang
      </div>
        <div class="footer-right">
          <a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/smackgg/hexo-theme-smackdown" target="_blank">Smackdown</a>
        </div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="/js/main.js"></script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js"></script>


  </div>
</body>
</html>