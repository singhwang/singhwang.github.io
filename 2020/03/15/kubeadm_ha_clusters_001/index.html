<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  <title>使用Kubeadm安装高可用Kubernetes v1.17.0集群（Stacked Control Plane Nodes For Baremetal） | Singh Wang</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="一、高可用部署的实现方式介绍官方文档介绍了使用Kbeadm设置高可用性Kubernetes集群的两种不同方法： 1. 堆叠master的方式（with stacked masters）这种方法需要较少的基础设施。控制平面节点和etcd成员位于同一位置。 2. 使用外部etcd集群的方式（with an external etcd cluster）这种方法需要更多的基础设施。控制平面节点和etcd成">
<meta name="keywords" content="Docker,Kubernetes,Kubeadm,Setup,Highly Available">
<meta property="og:type" content="article">
<meta property="og:title" content="使用Kubeadm安装高可用Kubernetes v1.17.0集群（Stacked Control Plane Nodes For Baremetal）">
<meta property="og:url" content="http://yoursite.com/2020/03/15/kubeadm_ha_clusters_001/index.html">
<meta property="og:site_name" content="Singh Wang">
<meta property="og:description" content="一、高可用部署的实现方式介绍官方文档介绍了使用Kbeadm设置高可用性Kubernetes集群的两种不同方法： 1. 堆叠master的方式（with stacked masters）这种方法需要较少的基础设施。控制平面节点和etcd成员位于同一位置。 2. 使用外部etcd集群的方式（with an external etcd cluster）这种方法需要更多的基础设施。控制平面节点和etcd成">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2020/03/15/kubeadm_ha_clusters_001/stacked_etcd_topology.png">
<meta property="og:updated_time" content="2020-03-16T23:45:46.254Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="使用Kubeadm安装高可用Kubernetes v1.17.0集群（Stacked Control Plane Nodes For Baremetal）">
<meta name="twitter:description" content="一、高可用部署的实现方式介绍官方文档介绍了使用Kbeadm设置高可用性Kubernetes集群的两种不同方法： 1. 堆叠master的方式（with stacked masters）这种方法需要较少的基础设施。控制平面节点和etcd成员位于同一位置。 2. 使用外部etcd集群的方式（with an external etcd cluster）这种方法需要更多的基础设施。控制平面节点和etcd成">
<meta name="twitter:image" content="http://yoursite.com/2020/03/15/kubeadm_ha_clusters_001/stacked_etcd_topology.png">
  
    <link rel="alternative" href="/atom.xml" title="Singh Wang" type="application/atom+xml">
  
  
    <link rel="icon" href="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg">
  
  <link rel="stylesheet" href="/css/style.css">
  
  

  <script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>
  <script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>

  
</head></html>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="https://avatars0.githubusercontent.com/u/17465385?s=400&amp;u=cea56ed1710ec6ebab160c1ce1e068317e05b025&amp;v=4" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Singh Wang</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/archives">所有文章</a></li>
				        
							<li><a href="/categories/容器云技术/">容器云技术</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/singhwang" title="github">github</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/Calico/" style="font-size: 11.11px;">Calico</a> <a href="/tags/Ceph/" style="font-size: 12.22px;">Ceph</a> <a href="/tags/Common/" style="font-size: 10px;">Common</a> <a href="/tags/DNS/" style="font-size: 10px;">DNS</a> <a href="/tags/Dashboard/" style="font-size: 11.11px;">Dashboard</a> <a href="/tags/Docker/" style="font-size: 20px;">Docker</a> <a href="/tags/Dragonfly/" style="font-size: 10px;">Dragonfly</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Fluentd/" style="font-size: 11.11px;">Fluentd</a> <a href="/tags/GlusterFS/" style="font-size: 13.33px;">GlusterFS</a> <a href="/tags/HAProxy/" style="font-size: 10px;">HAProxy</a> <a href="/tags/Harbor/" style="font-size: 10px;">Harbor</a> <a href="/tags/Highly-Available/" style="font-size: 12.22px;">Highly Available</a> <a href="/tags/Ingress/" style="font-size: 10px;">Ingress</a> <a href="/tags/Kubeadm/" style="font-size: 12.22px;">Kubeadm</a> <a href="/tags/Kubernetes/" style="font-size: 18.89px;">Kubernetes</a> <a href="/tags/Label/" style="font-size: 10px;">Label</a> <a href="/tags/Logging/" style="font-size: 12.22px;">Logging</a> <a href="/tags/Monitoring/" style="font-size: 13.33px;">Monitoring</a> <a href="/tags/Network/" style="font-size: 14.44px;">Network</a> <a href="/tags/Open-Falcon/" style="font-size: 10px;">Open Falcon</a> <a href="/tags/P2P/" style="font-size: 10px;">P2P</a> <a href="/tags/Prometheus/" style="font-size: 11.11px;">Prometheus</a> <a href="/tags/Setup/" style="font-size: 17.78px;">Setup</a> <a href="/tags/Smartping/" style="font-size: 10px;">Smartping</a> <a href="/tags/Source-Build/" style="font-size: 10px;">Source Build</a> <a href="/tags/Storage/" style="font-size: 15.56px;">Storage</a> <a href="/tags/TimeZone/" style="font-size: 10px;">TimeZone</a> <a href="/tags/Trouble-Shooting/" style="font-size: 16.67px;">Trouble Shooting</a>
					</div>
				</section>
				
				
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">Singh Wang</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="https://avatars0.githubusercontent.com/u/17465385?s=400&amp;u=cea56ed1710ec6ebab160c1ce1e068317e05b025&amp;v=4" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author">Singh Wang</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/archives">所有文章</a></li>
		        
					<li><a href="/categories/容器云技术/">容器云技术</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/singhwang" title="github">github</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap"><article id="post-kubeadm_ha_clusters_001" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/03/15/kubeadm_ha_clusters_001/" class="article-date">
  	<time datetime="2020-03-15T01:13:30.383Z" itemprop="datePublished">2020-03-15</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      使用Kubeadm安装高可用Kubernetes v1.17.0集群（Stacked Control Plane Nodes For Baremetal）
      
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Highly-Available/">Highly Available</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubeadm/">Kubeadm</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Setup/">Setup</a></li></ul>
	</div>

        
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


        
        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、高可用部署的实现方式介绍"><a href="#一、高可用部署的实现方式介绍" class="headerlink" title="一、高可用部署的实现方式介绍"></a>一、高可用部署的实现方式介绍</h1><p>官方文档介绍了使用Kbeadm设置高可用性Kubernetes集群的两种不同方法：</p>
<h2 id="1-堆叠master的方式（with-stacked-masters）"><a href="#1-堆叠master的方式（with-stacked-masters）" class="headerlink" title="1. 堆叠master的方式（with stacked masters）"></a>1. 堆叠master的方式（with stacked masters）</h2><p>这种方法需要较少的基础设施。控制平面节点和etcd成员位于同一位置。</p>
<h2 id="2-使用外部etcd集群的方式（with-an-external-etcd-cluster）"><a href="#2-使用外部etcd集群的方式（with-an-external-etcd-cluster）" class="headerlink" title="2. 使用外部etcd集群的方式（with an external etcd cluster）"></a>2. 使用外部etcd集群的方式（with an external etcd cluster）</h2><p>这种方法需要更多的基础设施。控制平面节点和etcd成员是分开的。<br>这里重点介绍第一种方式，即堆叠master的方式。官方文档链接详见参考资料。</p>
<h1 id="二、实验环境版本信息"><a href="#二、实验环境版本信息" class="headerlink" title="二、实验环境版本信息"></a>二、实验环境版本信息</h1><h2 id="1-高可用工具的版本（这里记录的是docker镜像的版本）"><a href="#1-高可用工具的版本（这里记录的是docker镜像的版本）" class="headerlink" title="1. 高可用工具的版本（这里记录的是docker镜像的版本）"></a>1. 高可用工具的版本（这里记录的是docker镜像的版本）</h2><p>haproxy:1.7-alpine<br>osixia/keepalived:1.4.5</p>
<h2 id="2-Kubernetes各个组件的版本"><a href="#2-Kubernetes各个组件的版本" class="headerlink" title="2. Kubernetes各个组件的版本"></a>2. Kubernetes各个组件的版本</h2><p>etcd v3.4.3<br>kube-apiserver v1.17.0<br>kube-controller-manager v1.17.0<br>kube-scheduler v1.17.0<br>kubectl v1.17.0<br>coredns 1.6.5</p>
<p>docker 18.09.9<br>kube-proxy v1.17.0<br>kubelet v1.17.0<br>calico v3.11.1 （calico/node:v3.11.1 calico/pod2daemon-flexvol:v3.11.1 calico/cni:v3.11.1 calico/kube-controllers:v3.11.1）</p>
<h1 id="三、部署架构介绍"><a href="#三、部署架构介绍" class="headerlink" title="三、部署架构介绍"></a>三、部署架构介绍</h1><p><img src="/2020/03/15/kubeadm_ha_clusters_001/stacked_etcd_topology.png" alt="stacked_etcd_topology"></p>
<h2 id="1-Kubernetes-Master（Control-Plane）"><a href="#1-Kubernetes-Master（Control-Plane）" class="headerlink" title="1. Kubernetes Master（Control Plane）"></a>1. Kubernetes Master（Control Plane）</h2><p>192.168.112.128 master01 -&gt; docker kubelet keepalived haproxy etcd kube-apiserver kube-controller-manager kube-scheduler kube-proxy calico<br>192.168.112.129 master02 -&gt; docker kubelet keepalived haproxy etcd kube-apiserver kube-controller-manager kube-scheduler kube-proxy calico<br>192.168.112.130 master03 -&gt; docker kubelet keepalived haproxy etcd kube-apiserver kube-controller-manager kube-scheduler kube-proxy calico</p>
<h2 id="2-Kubernetes-Node"><a href="#2-Kubernetes-Node" class="headerlink" title="2. Kubernetes Node"></a>2. Kubernetes Node</h2><p>192.168.112.131 node01 -&gt; docker kubelet kube-proxy calico（calico-node）<br>192.168.112.132 node02 -&gt; docker kubelet kube-proxy calico（calico-node）</p>
<h1 id="四、实现过程记录"><a href="#四、实现过程记录" class="headerlink" title="四、实现过程记录"></a>四、实现过程记录</h1><h2 id="1-在Kubernetes-Control-Plane上的所有Node上部署HAProxy做为负载均衡器（由Kubelet管理以静态Pod的方式实现）"><a href="#1-在Kubernetes-Control-Plane上的所有Node上部署HAProxy做为负载均衡器（由Kubelet管理以静态Pod的方式实现）" class="headerlink" title="1. 在Kubernetes Control Plane上的所有Node上部署HAProxy做为负载均衡器（由Kubelet管理以静态Pod的方式实现）"></a>1. 在Kubernetes Control Plane上的所有Node上部署HAProxy做为负载均衡器（由Kubelet管理以静态Pod的方式实现）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">## 在控制平面的所有Node上执行</span><br><span class="line">mkdir -p /etc/haproxy/</span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">  log 127.0.0.1 local0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  uid 99</span><br><span class="line">  gid 99</span><br><span class="line">  #daemon</span><br><span class="line">  nbproc 1</span><br><span class="line">  pidfile haproxy.pid</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  mode tcp</span><br><span class="line">  log 127.0.0.1 local0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  retries 3</span><br><span class="line">  timeout connect 10s</span><br><span class="line">  timeout client 10m</span><br><span class="line">  timeout server 10m</span><br><span class="line"></span><br><span class="line">listen stats</span><br><span class="line">  mode http</span><br><span class="line">  bind 0.0.0.0:9090</span><br><span class="line">  log 127.0.0.1 local0 err</span><br><span class="line">  stats refresh 30s</span><br><span class="line">  stats uri     /haproxy-status</span><br><span class="line">  stats realm   Haproxy\ Statistics</span><br><span class="line">  stats auth    admin:12345678</span><br><span class="line">  stats hide-version</span><br><span class="line">  stats admin if TRUE</span><br><span class="line"></span><br><span class="line">frontend kube-apiserver-https</span><br><span class="line">   mode tcp</span><br><span class="line">   bind :8443</span><br><span class="line">   default_backend kube-apiserver-backend</span><br><span class="line"></span><br><span class="line">backend kube-apiserver-backend</span><br><span class="line">    mode tcp</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server server01 192.168.112.128:6443 weight 3 minconn 100 maxconn 50000 check inter 5000 rise 2 fall 5</span><br><span class="line">    server server02 192.168.112.129:6443 weight 3 minconn 100 maxconn 50000 check inter 5000 rise 2 fall 5</span><br><span class="line">    server server03 192.168.112.130:6443 weight 3 minconn 100 maxconn 50000 check inter 5000 rise 2 fall 5</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">## 仅在master01上执行</span><br><span class="line">mkdir -p /etc/kubernetes/manifests/</span><br><span class="line"></span><br><span class="line">## 在master01上需要先执行，在master02和master03上需先做完kubeadm join后再执行</span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/kubernetes/manifests/haproxy.yaml</span><br><span class="line">kind: Pod</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    scheduler.alpha.kubernetes.io/critical-pod: &quot;&quot;</span><br><span class="line">  labels:</span><br><span class="line">    component: haproxy</span><br><span class="line">    tier: control-plane</span><br><span class="line">  name: kube-haproxy</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  hostNetwork: true</span><br><span class="line">  containers:</span><br><span class="line">  - name: kube-haproxy</span><br><span class="line">    image: haproxy:1.7-alpine</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        cpu: 100m</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: haproxy-cfg</span><br><span class="line">      readOnly: true</span><br><span class="line">      mountPath: /usr/local/etc/haproxy/haproxy.cfg</span><br><span class="line">  volumes:</span><br><span class="line">  - name: haproxy-cfg</span><br><span class="line">    hostPath:</span><br><span class="line">      path: /etc/haproxy/haproxy.cfg</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="2-在Kubernetes-Control-Plane的所有Node上部署Keepalived（由Kubelet管理以静态Pod的方式实现）"><a href="#2-在Kubernetes-Control-Plane的所有Node上部署Keepalived（由Kubelet管理以静态Pod的方式实现）" class="headerlink" title="2. 在Kubernetes Control Plane的所有Node上部署Keepalived（由Kubelet管理以静态Pod的方式实现）"></a>2. 在Kubernetes Control Plane的所有Node上部署Keepalived（由Kubelet管理以静态Pod的方式实现）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">## 仅在master01上执行</span><br><span class="line">mkdir -p /etc/kubernetes/manifests/</span><br><span class="line"></span><br><span class="line">## 在master01上需要在kubeadm init前先执行，在master02和master03上需先做完kubeadm join后再执行</span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/kubernetes/manifests/keepalived.yaml</span><br><span class="line">kind: Pod</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    scheduler.alpha.kubernetes.io/critical-pod: &quot;&quot;</span><br><span class="line">  labels:</span><br><span class="line">    component: keepalived</span><br><span class="line">    tier: control-plane</span><br><span class="line">  name: kube-keepalived</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  hostNetwork: true</span><br><span class="line">  containers:</span><br><span class="line">  - name: kube-keepalived</span><br><span class="line">    image: osixia/keepalived:1.4.5</span><br><span class="line">    env:</span><br><span class="line">    - name: KEEPALIVED_VIRTUAL_IPS</span><br><span class="line">      value: 192.168.112.136</span><br><span class="line">    - name: KEEPALIVED_INTERFACE</span><br><span class="line">      value: ens33</span><br><span class="line">    - name: KEEPALIVED_UNICAST_PEERS</span><br><span class="line">      value: &quot;#PYTHON2BASH:[&apos;192.168.112.128&apos;, &apos;192.168.112.129&apos;, &apos;192.168.112.130&apos;]&quot;</span><br><span class="line">    - name: KEEPALIVED_PASSWORD</span><br><span class="line">      value: docker</span><br><span class="line">    - name: KEEPALIVED_PRIORITY</span><br><span class="line">      value: &quot;200&quot;</span><br><span class="line">    - name: KEEPALIVED_ROUTER_ID</span><br><span class="line">      value: &quot;51&quot;</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        cpu: 100m</span><br><span class="line">    securityContext:</span><br><span class="line">      privileged: true</span><br><span class="line">      capabilities:</span><br><span class="line">        add:</span><br><span class="line">        - NET_ADMIN</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="3-在Kubernetes-Control-Plane-的第一个Node（master01）上操作："><a href="#3-在Kubernetes-Control-Plane-的第一个Node（master01）上操作：" class="headerlink" title="3. 在Kubernetes Control Plane 的第一个Node（master01）上操作："></a>3. 在Kubernetes Control Plane 的第一个Node（master01）上操作：</h2><p>（1）生成kubeadm配置文件，并拉取相关的docker镜像<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">## 生成Kubeadm初始化需要使用的配置文件</span><br><span class="line">mkdir -p kubeadm/config/</span><br><span class="line">cat &lt;&lt;EOF &gt; kubeadm/config/kubeadm-config.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta2</span><br><span class="line">bootstrapTokens:</span><br><span class="line">- groups:</span><br><span class="line">  - system:bootstrappers:kubeadm:default-node-token</span><br><span class="line">  token: abcdef.0123456789abcdef</span><br><span class="line">  ttl: 24h0m0s</span><br><span class="line">  usages:</span><br><span class="line">  - signing</span><br><span class="line">  - authentication</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">localAPIEndpoint:</span><br><span class="line">  advertiseAddress: 192.168.112.128</span><br><span class="line">  bindPort: 6443</span><br><span class="line">nodeRegistration:</span><br><span class="line">  criSocket: /var/run/dockershim.sock</span><br><span class="line">  name: master01</span><br><span class="line">  taints:</span><br><span class="line">  - effect: NoSchedule</span><br><span class="line">    key: node-role.kubernetes.io/master</span><br><span class="line">---</span><br><span class="line">apiServer:</span><br><span class="line">  timeoutForControlPlane: 4m0s</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta2</span><br><span class="line">certificatesDir: /etc/kubernetes/pki</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">controllerManager: &#123;&#125;</span><br><span class="line">controlPlaneEndpoint: 192.168.112.136:8443</span><br><span class="line">dns:</span><br><span class="line">  type: CoreDNS</span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    dataDir: /var/lib/etcd</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.17.0</span><br><span class="line">networking:</span><br><span class="line">  podSubnet: 10.211.0.0/16</span><br><span class="line">  dnsDomain: cluster.local</span><br><span class="line">  serviceSubnet: 10.96.0.0/16</span><br><span class="line">scheduler: &#123;&#125;</span><br><span class="line"></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">## 拉取Kubeadm初始化需要使用的docker镜像</span><br><span class="line">kubeadm config images pull --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">W0315 10:52:16.188454    5239 validation.go:28] Cannot validate kube-proxy config - no validator is available</span><br><span class="line">W0315 10:52:16.188503    5239 validation.go:28] Cannot validate kubelet config - no validator is available</span><br><span class="line">[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.17.0</span><br><span class="line">[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.17.0</span><br><span class="line">[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.17.0</span><br><span class="line">[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.17.0</span><br><span class="line">[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.4.3-0</span><br><span class="line">[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.6.5</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">。。。。。。</span><br></pre></td></tr></table></figure></p>
<p>（2）初始化集群（千万注意A和B任选一种方法即可，不可以同时使用）</p>
<p>A. 使用自动分发根证书的方式初始化<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"># 执行Kubeadm的初始化操作（自动分发根证书）</span><br><span class="line">kubeadm init --config kubeadm/config/kubeadm-config.yaml --upload-certs</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">W0315 10:53:05.509978    5340 validation.go:28] Cannot validate kube-proxy config - no validator is available</span><br><span class="line">W0315 10:53:05.510016    5340 validation.go:28] Cannot validate kubelet config - no validator is available</span><br><span class="line">[init] Using Kubernetes version: v1.17.0</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[preflight] Pulling images required for setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos;</span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">[kubelet-start] Starting the kubelet</span><br><span class="line">[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;</span><br><span class="line">[certs] Generating &quot;ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver&quot; certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed for DNS names [master01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.112.128 192.168.112.136]</span><br><span class="line">[certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;front-proxy-ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;front-proxy-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd/ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd/server&quot; certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed for DNS names [master01 localhost] and IPs [192.168.112.128 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;etcd/peer&quot; certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed for DNS names [master01 localhost] and IPs [192.168.112.128 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;etcd/healthcheck-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver-etcd-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;sa&quot; key and public key</span><br><span class="line">[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;</span><br><span class="line">[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address</span><br><span class="line">[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file</span><br><span class="line">[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address</span><br><span class="line">[kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file</span><br><span class="line">[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address</span><br><span class="line">[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file</span><br><span class="line">[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address</span><br><span class="line">[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file</span><br><span class="line">[control-plane] Using manifest folder &quot;/etc/kubernetes/manifests&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;</span><br><span class="line">W0315 10:53:08.283605    5340 manifests.go:214] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;</span><br><span class="line">W0315 10:53:08.284727    5340 manifests.go:214] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;</span><br><span class="line">[etcd] Creating static Pod manifest for local etcd in &quot;/etc/kubernetes/manifests&quot;</span><br><span class="line">[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &quot;/etc/kubernetes/manifests&quot;. This can take up to 4m0s</span><br><span class="line">[kubelet-check] Initial timeout of 40s passed.</span><br><span class="line">[apiclient] All control plane components are healthy after 42.019337 seconds</span><br><span class="line">[upload-config] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[kubelet] Creating a ConfigMap &quot;kubelet-config-1.17&quot; in namespace kube-system with the configuration for the kubelets in the cluster</span><br><span class="line">[upload-certs] Storing the certificates in Secret &quot;kubeadm-certs&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[upload-certs] Using certificate key:</span><br><span class="line">16f06d3321fce089cad4b229da9b5d3ef94c08a246943e0f375b977f18bbab8e</span><br><span class="line">[mark-control-plane] Marking the node master01 as control-plane by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot;</span><br><span class="line">[mark-control-plane] Marking the node master01 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line">[bootstrap-token] Using token: abcdef.0123456789abcdef</span><br><span class="line">[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster</span><br><span class="line">[bootstrap-token] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class="line">[kubelet-finalize] Updating &quot;/etc/kubernetes/kubelet.conf&quot; to point to a rotatable kubelet client certificate and key</span><br><span class="line">[addons] Applied essential addon: CoreDNS</span><br><span class="line">[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p $HOME/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">You can now join any number of the control-plane node running the following command on each as root:</span><br><span class="line"></span><br><span class="line">  kubeadm join 192.168.112.136:8443 --token abcdef.0123456789abcdef \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:33a6370b4bb4a9385c1d878e9a7a085ad969d521e4b309b01be797c0d7867d69 \</span><br><span class="line">    --control-plane --certificate-key 16f06d3321fce089cad4b229da9b5d3ef94c08a246943e0f375b977f18bbab8e</span><br><span class="line"></span><br><span class="line">Please note that the certificate-key gives access to cluster sensitive data, keep it secret!</span><br><span class="line">As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use</span><br><span class="line">&quot;kubeadm init phase upload-certs --upload-certs&quot; to reload certs afterward.</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join 192.168.112.136:8443 --token abcdef.0123456789abcdef \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:33a6370b4bb4a9385c1d878e9a7a085ad969d521e4b309b01be797c0d7867d69</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"># 保存输出中类似于下面的命令，供添加节点功能使用</span><br><span class="line">。。。。。。</span><br><span class="line">You can now join any number of the control-plane node running the following command on each as root:</span><br><span class="line"></span><br><span class="line">  kubeadm join 192.168.112.136:8443 --token abcdef.0123456789abcdef \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:33a6370b4bb4a9385c1d878e9a7a085ad969d521e4b309b01be797c0d7867d69 \</span><br><span class="line">    --control-plane --certificate-key 16f06d3321fce089cad4b229da9b5d3ef94c08a246943e0f375b977f18bbab8e</span><br><span class="line"></span><br><span class="line">Please note that the certificate-key gives access to cluster sensitive data, keep it secret!</span><br><span class="line">As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use</span><br><span class="line">&quot;kubeadm init phase upload-certs --upload-certs&quot; to reload certs afterward.</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join 192.168.112.136:8443 --token abcdef.0123456789abcdef \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:33a6370b4bb4a9385c1d878e9a7a085ad969d521e4b309b01be797c0d7867d69</span><br></pre></td></tr></table></figure></p>
<p>B. 使用手动分发根证书的方式初始化<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line"># 执行Kubeadm的初始化操作（手动分发根证书）</span><br><span class="line">kubeadm init --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">W0315 11:37:50.200933    2834 validation.go:28] Cannot validate kubelet config - no validator is available</span><br><span class="line">W0315 11:37:50.201021    2834 validation.go:28] Cannot validate kube-proxy config - no validator is available</span><br><span class="line">[init] Using Kubernetes version: v1.17.0</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[preflight] Pulling images required for setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos;</span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">[kubelet-start] Starting the kubelet</span><br><span class="line">[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;</span><br><span class="line">[certs] Generating &quot;ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver&quot; certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed for DNS names [master01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.112.128 192.168.112.136]</span><br><span class="line">[certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;front-proxy-ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;front-proxy-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd/ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd/server&quot; certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed for DNS names [master01 localhost] and IPs [192.168.112.128 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;etcd/peer&quot; certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed for DNS names [master01 localhost] and IPs [192.168.112.128 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;etcd/healthcheck-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver-etcd-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;sa&quot; key and public key</span><br><span class="line">[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;</span><br><span class="line">[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address</span><br><span class="line">[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file</span><br><span class="line">[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address</span><br><span class="line">[kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file</span><br><span class="line">[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address</span><br><span class="line">[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file</span><br><span class="line">[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address</span><br><span class="line">[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file</span><br><span class="line">[control-plane] Using manifest folder &quot;/etc/kubernetes/manifests&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;</span><br><span class="line">W0315 11:37:52.884008    2834 manifests.go:214] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;</span><br><span class="line">W0315 11:37:52.885218    2834 manifests.go:214] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;</span><br><span class="line">[etcd] Creating static Pod manifest for local etcd in &quot;/etc/kubernetes/manifests&quot;</span><br><span class="line">[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &quot;/etc/kubernetes/manifests&quot;. This can take up to 4m0s</span><br><span class="line">[apiclient] All control plane components are healthy after 36.521431 seconds</span><br><span class="line">[upload-config] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[kubelet] Creating a ConfigMap &quot;kubelet-config-1.17&quot; in namespace kube-system with the configuration for the kubelets in the cluster</span><br><span class="line">[upload-certs] Skipping phase. Please see --upload-certs</span><br><span class="line">[mark-control-plane] Marking the node master01 as control-plane by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot;</span><br><span class="line">[mark-control-plane] Marking the node master01 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line">[bootstrap-token] Using token: abcdef.0123456789abcdef</span><br><span class="line">[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster</span><br><span class="line">[bootstrap-token] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class="line">[kubelet-finalize] Updating &quot;/etc/kubernetes/kubelet.conf&quot; to point to a rotatable kubelet client certificate and key</span><br><span class="line">[addons] Applied essential addon: CoreDNS</span><br><span class="line">[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p $HOME/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">You can now join any number of control-plane nodes by copying certificate authorities</span><br><span class="line">and service account keys on each node and then running the following as root:</span><br><span class="line"></span><br><span class="line">  kubeadm join 192.168.112.136:8443 --token abcdef.0123456789abcdef \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:05945a0dc7d9c5e45e196d8582de19a3df559d1f9f4e4cb52c77d3051db923b4 \</span><br><span class="line">    --control-plane</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join 192.168.112.136:8443 --token abcdef.0123456789abcdef \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:05945a0dc7d9c5e45e196d8582de19a3df559d1f9f4e4cb52c77d3051db923b4</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"># 保存输出中类似于下面的命令，供添加节点功能使用（后续Master节点的加入一定要在手动分发完根证书后再执行第一个命令进行加入）</span><br><span class="line"># 注意：控制Master节点的加入使用第一个命令，Node节点的加入使用第二个命令</span><br><span class="line">。。。。。。</span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">You can now join any number of control-plane nodes by copying certificate authorities</span><br><span class="line">and service account keys on each node and then running the following as root:</span><br><span class="line"></span><br><span class="line">  kubeadm join 192.168.112.136:8443 --token abcdef.0123456789abcdef \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:05945a0dc7d9c5e45e196d8582de19a3df559d1f9f4e4cb52c77d3051db923b4 \</span><br><span class="line">    --control-plane</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join 192.168.112.136:8443 --token abcdef.0123456789abcdef \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:05945a0dc7d9c5e45e196d8582de19a3df559d1f9f4e4cb52c77d3051db923b4</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 配置master01到master02和master03的ssh免密登录</span><br><span class="line">ssh-keygen</span><br><span class="line">ssh-copy-id -i .ssh/id_rsa.pub root@master02</span><br><span class="line">ssh-copy-id -i .ssh/id_rsa.pub root@master03</span><br><span class="line"></span><br><span class="line">## 验证master01到master02和master03的ssh免密登录</span><br><span class="line">ssh master02</span><br><span class="line">ssh master03</span><br><span class="line"></span><br><span class="line">## 分发pki证书和admin.conf文件</span><br><span class="line">cat &lt;&lt;EOF &gt; kubeadm/config/scp-config.sh</span><br><span class="line">USER=root</span><br><span class="line">CONTROL_PLANE_IPS=&quot;192.168.112.129 192.168.112.130&quot;</span><br><span class="line">for host in \$&#123;CONTROL_PLANE_IPS&#125;; do</span><br><span class="line">    ssh \$&#123;USER&#125;@\$host &apos;mkdir -p /etc/kubernetes/pki/etcd/&apos;</span><br><span class="line">    scp /etc/kubernetes/pki/ca.crt \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/</span><br><span class="line">    scp /etc/kubernetes/pki/ca.key \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/</span><br><span class="line">    scp /etc/kubernetes/pki/sa.key \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/</span><br><span class="line">    scp /etc/kubernetes/pki/sa.pub \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/</span><br><span class="line">    scp /etc/kubernetes/pki/front-proxy-ca.crt \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/</span><br><span class="line">    scp /etc/kubernetes/pki/front-proxy-ca.key \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/</span><br><span class="line">    scp /etc/kubernetes/pki/etcd/ca.crt \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/etcd/</span><br><span class="line">    scp /etc/kubernetes/pki/etcd/ca.key \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/etcd/</span><br><span class="line">    scp /etc/kubernetes/admin.conf \$&#123;USER&#125;@\$host:/etc/kubernetes/</span><br><span class="line">done</span><br><span class="line">EOF</span><br><span class="line">chmod 0755 kubeadm/config/scp-config.sh</span><br><span class="line">./kubeadm/config/scp-config.sh</span><br></pre></td></tr></table></figure></p>
<h2 id="4-在Kubernetes-Control-Plane-的第二个Node（master02）上操作：（千万注意A和B任选一种方法即可，不可以同时使用）"><a href="#4-在Kubernetes-Control-Plane-的第二个Node（master02）上操作：（千万注意A和B任选一种方法即可，不可以同时使用）" class="headerlink" title="4. 在Kubernetes Control Plane 的第二个Node（master02）上操作：（千万注意A和B任选一种方法即可，不可以同时使用）"></a>4. 在Kubernetes Control Plane 的第二个Node（master02）上操作：（千万注意A和B任选一种方法即可，不可以同时使用）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"># A. 使用自动分发根证书的方式初始化</span><br><span class="line">kubeadm join 192.168.112.136:8443 --token abcdef.0123456789abcdef \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:33a6370b4bb4a9385c1d878e9a7a085ad969d521e4b309b01be797c0d7867d69 \</span><br><span class="line">    --control-plane --certificate-key 16f06d3321fce089cad4b229da9b5d3ef94c08a246943e0f375b977f18bbab8e</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[preflight] Reading configuration from the cluster...</span><br><span class="line">[preflight] FYI: You can look at this config file with &apos;kubectl -n kube-system get cm kubeadm-config -oyaml&apos;</span><br><span class="line">[preflight] Running pre-flight checks before initializing the new control plane instance</span><br><span class="line">[preflight] Pulling images required for setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos;</span><br><span class="line">[download-certs] Downloading the certificates in Secret &quot;kubeadm-certs&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;</span><br><span class="line">[certs] Generating &quot;front-proxy-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd/peer&quot; certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed for DNS names [master02 localhost] and IPs [192.168.112.129 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;apiserver-etcd-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd/server&quot; certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed for DNS names [master02 localhost] and IPs [192.168.112.129 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;etcd/healthcheck-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver&quot; certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed for DNS names [master02 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.112.129 192.168.112.136]</span><br><span class="line">[certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key</span><br><span class="line">[certs] Valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot;</span><br><span class="line">[certs] Using the existing &quot;sa&quot; key</span><br><span class="line">[kubeconfig] Generating kubeconfig files</span><br><span class="line">[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;</span><br><span class="line">[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address</span><br><span class="line">[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file</span><br><span class="line">[control-plane] Using manifest folder &quot;/etc/kubernetes/manifests&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;</span><br><span class="line">W0315 10:59:52.640333    1546 manifests.go:214] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;</span><br><span class="line">W0315 10:59:52.645116    1546 manifests.go:214] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;</span><br><span class="line">W0315 10:59:52.646387    1546 manifests.go:214] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;</span><br><span class="line">[check-etcd] Checking that the etcd cluster is healthy</span><br><span class="line">[kubelet-start] Downloading configuration for the kubelet from the &quot;kubelet-config-1.17&quot; ConfigMap in the kube-system namespace</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class="line">[kubelet-start] Starting the kubelet</span><br><span class="line">[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...</span><br><span class="line">[etcd] Announced new etcd member joining to the existing etcd cluster</span><br><span class="line">[etcd] Creating static Pod manifest for &quot;etcd&quot;</span><br><span class="line">[etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s</span><br><span class="line">&#123;&quot;level&quot;:&quot;warn&quot;,&quot;ts&quot;:&quot;2020-03-15T11:00:28.875+0800&quot;,&quot;caller&quot;:&quot;clientv3/retry_interceptor.go:61&quot;,&quot;msg&quot;:&quot;retrying of unary invoker failed&quot;,&quot;target&quot;:&quot;passthrough:///https://192.168.112.129:2379&quot;,&quot;attempt&quot;:0,&quot;error&quot;:&quot;rpc error: code = DeadlineExceeded desc = context deadline exceeded&quot;&#125;</span><br><span class="line">[upload-config] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[mark-control-plane] Marking the node master02 as control-plane by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot;</span><br><span class="line">[mark-control-plane] Marking the node master02 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line"></span><br><span class="line">This node has joined the cluster and a new control plane instance was created:</span><br><span class="line"></span><br><span class="line">* Certificate signing request was sent to apiserver and approval was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line">* Control plane (master) label and taint were applied to the new node.</span><br><span class="line">* The Kubernetes control plane instances scaled up.</span><br><span class="line">* A new etcd member was added to the local/stacked etcd cluster.</span><br><span class="line"></span><br><span class="line">To start administering your cluster from this node, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">	mkdir -p $HOME/.kube</span><br><span class="line">	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">	sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">Run &apos;kubectl get nodes&apos; to see this node join the cluster.</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"># B. 使用手动分发根证书的方式初始化</span><br><span class="line">kubeadm join 192.168.112.136:8443 --token abcdef.0123456789abcdef \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:05945a0dc7d9c5e45e196d8582de19a3df559d1f9f4e4cb52c77d3051db923b4 \</span><br><span class="line">    --control-plane</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[preflight] Reading configuration from the cluster...</span><br><span class="line">[preflight] FYI: You can look at this config file with &apos;kubectl -n kube-system get cm kubeadm-config -oyaml&apos;</span><br><span class="line">[preflight] Running pre-flight checks before initializing the new control plane instance</span><br><span class="line">[preflight] Pulling images required for setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos;</span><br><span class="line">[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;</span><br><span class="line">[certs] Generating &quot;apiserver&quot; certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed for DNS names [master02 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.112.129 192.168.112.136]</span><br><span class="line">[certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;front-proxy-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd/server&quot; certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed for DNS names [master02 localhost] and IPs [192.168.112.129 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;etcd/healthcheck-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd/peer&quot; certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed for DNS names [master02 localhost] and IPs [192.168.112.129 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;apiserver-etcd-client&quot; certificate and key</span><br><span class="line">[certs] Valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot;</span><br><span class="line">[certs] Using the existing &quot;sa&quot; key</span><br><span class="line">[kubeconfig] Generating kubeconfig files</span><br><span class="line">[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;</span><br><span class="line">[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address</span><br><span class="line">[kubeconfig] Using existing kubeconfig file: &quot;/etc/kubernetes/admin.conf&quot;</span><br><span class="line">[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file</span><br><span class="line">[control-plane] Using manifest folder &quot;/etc/kubernetes/manifests&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;</span><br><span class="line">W0315 11:48:00.712980    2760 manifests.go:214] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;</span><br><span class="line">W0315 11:48:00.717833    2760 manifests.go:214] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;</span><br><span class="line">W0315 11:48:00.718658    2760 manifests.go:214] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;</span><br><span class="line">[check-etcd] Checking that the etcd cluster is healthy</span><br><span class="line">[kubelet-start] Downloading configuration for the kubelet from the &quot;kubelet-config-1.17&quot; ConfigMap in the kube-system namespace</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class="line">[kubelet-start] Starting the kubelet</span><br><span class="line">[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...</span><br><span class="line">[etcd] Announced new etcd member joining to the existing etcd cluster</span><br><span class="line">[etcd] Creating static Pod manifest for &quot;etcd&quot;</span><br><span class="line">[etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s</span><br><span class="line">&#123;&quot;level&quot;:&quot;warn&quot;,&quot;ts&quot;:&quot;2020-03-15T11:48:38.856+0800&quot;,&quot;caller&quot;:&quot;clientv3/retry_interceptor.go:61&quot;,&quot;msg&quot;:&quot;retrying of unary invoker failed&quot;,&quot;target&quot;:&quot;passthrough:///https://192.168.112.129:2379&quot;,&quot;attempt&quot;:0,&quot;error&quot;:&quot;rpc error: code = DeadlineExceeded desc = context deadline exceeded&quot;&#125;</span><br><span class="line">[upload-config] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[kubelet-check] Initial timeout of 40s passed.</span><br><span class="line">[mark-control-plane] Marking the node master02 as control-plane by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot;</span><br><span class="line">[mark-control-plane] Marking the node master02 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line"></span><br><span class="line">This node has joined the cluster and a new control plane instance was created:</span><br><span class="line"></span><br><span class="line">* Certificate signing request was sent to apiserver and approval was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line">* Control plane (master) label and taint were applied to the new node.</span><br><span class="line">* The Kubernetes control plane instances scaled up.</span><br><span class="line">* A new etcd member was added to the local/stacked etcd cluster.</span><br><span class="line"></span><br><span class="line">To start administering your cluster from this node, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">	mkdir -p $HOME/.kube</span><br><span class="line">	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">	sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">Run &apos;kubectl get nodes&apos; to see this node join the cluster.</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>
<h2 id="5-在Kubernetes-Control-Plane-的第三个Node（master03）上操作：（千万注意A和B任选一种方法即可，不可以同时使用）"><a href="#5-在Kubernetes-Control-Plane-的第三个Node（master03）上操作：（千万注意A和B任选一种方法即可，不可以同时使用）" class="headerlink" title="5. 在Kubernetes Control Plane 的第三个Node（master03）上操作：（千万注意A和B任选一种方法即可，不可以同时使用）"></a>5. 在Kubernetes Control Plane 的第三个Node（master03）上操作：（千万注意A和B任选一种方法即可，不可以同时使用）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line"># A. 使用自动分发根证书的方式初始化</span><br><span class="line">kubeadm join 192.168.112.136:8443 --token abcdef.0123456789abcdef \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:33a6370b4bb4a9385c1d878e9a7a085ad969d521e4b309b01be797c0d7867d69 \</span><br><span class="line">    --control-plane --certificate-key 16f06d3321fce089cad4b229da9b5d3ef94c08a246943e0f375b977f18bbab8e</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[preflight] Reading configuration from the cluster...</span><br><span class="line">[preflight] FYI: You can look at this config file with &apos;kubectl -n kube-system get cm kubeadm-config -oyaml&apos;</span><br><span class="line">[preflight] Running pre-flight checks before initializing the new control plane instance</span><br><span class="line">[preflight] Pulling images required for setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos;</span><br><span class="line">[download-certs] Downloading the certificates in Secret &quot;kubeadm-certs&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;</span><br><span class="line">[certs] Generating &quot;front-proxy-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd/server&quot; certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed for DNS names [master03 localhost] and IPs [192.168.112.130 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;etcd/peer&quot; certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed for DNS names [master03 localhost] and IPs [192.168.112.130 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;etcd/healthcheck-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver-etcd-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver&quot; certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed for DNS names [master03 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.112.130 192.168.112.136]</span><br><span class="line">[certs] Valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot;</span><br><span class="line">[certs] Using the existing &quot;sa&quot; key</span><br><span class="line">[kubeconfig] Generating kubeconfig files</span><br><span class="line">[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;</span><br><span class="line">[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address</span><br><span class="line">[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file</span><br><span class="line">[control-plane] Using manifest folder &quot;/etc/kubernetes/manifests&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;</span><br><span class="line">W0315 11:02:05.176831    1648 manifests.go:214] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;</span><br><span class="line">W0315 11:02:05.182344    1648 manifests.go:214] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;</span><br><span class="line">W0315 11:02:05.183197    1648 manifests.go:214] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;</span><br><span class="line">[check-etcd] Checking that the etcd cluster is healthy</span><br><span class="line">[kubelet-start] Downloading configuration for the kubelet from the &quot;kubelet-config-1.17&quot; ConfigMap in the kube-system namespace</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class="line">[kubelet-start] Starting the kubelet</span><br><span class="line">[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...</span><br><span class="line">[etcd] Announced new etcd member joining to the existing etcd cluster</span><br><span class="line">[etcd] Creating static Pod manifest for &quot;etcd&quot;</span><br><span class="line">[etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s</span><br><span class="line">&#123;&quot;level&quot;:&quot;warn&quot;,&quot;ts&quot;:&quot;2020-03-15T11:02:32.084+0800&quot;,&quot;caller&quot;:&quot;clientv3/retry_interceptor.go:61&quot;,&quot;msg&quot;:&quot;retrying of unary invoker failed&quot;,&quot;target&quot;:&quot;passthrough:///https://192.168.112.130:2379&quot;,&quot;attempt&quot;:0,&quot;error&quot;:&quot;rpc error: code = DeadlineExceeded desc = context deadline exceeded&quot;&#125;</span><br><span class="line">[upload-config] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[mark-control-plane] Marking the node master03 as control-plane by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot;</span><br><span class="line">[mark-control-plane] Marking the node master03 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line"></span><br><span class="line">This node has joined the cluster and a new control plane instance was created:</span><br><span class="line"></span><br><span class="line">* Certificate signing request was sent to apiserver and approval was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line">* Control plane (master) label and taint were applied to the new node.</span><br><span class="line">* The Kubernetes control plane instances scaled up.</span><br><span class="line">* A new etcd member was added to the local/stacked etcd cluster.</span><br><span class="line"></span><br><span class="line">To start administering your cluster from this node, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">	mkdir -p $HOME/.kube</span><br><span class="line">	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">	sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">Run &apos;kubectl get nodes&apos; to see this node join the cluster.</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"># B. 使用手动分发根证书的方式初始化</span><br><span class="line">kubeadm join 192.168.112.136:8443 --token abcdef.0123456789abcdef \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:05945a0dc7d9c5e45e196d8582de19a3df559d1f9f4e4cb52c77d3051db923b4 \</span><br><span class="line">    --control-plane</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[preflight] Reading configuration from the cluster...</span><br><span class="line">[preflight] FYI: You can look at this config file with &apos;kubectl -n kube-system get cm kubeadm-config -oyaml&apos;</span><br><span class="line">[preflight] Running pre-flight checks before initializing the new control plane instance</span><br><span class="line">[preflight] Pulling images required for setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos;</span><br><span class="line">[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;</span><br><span class="line">[certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver&quot; certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed for DNS names [master03 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.112.130 192.168.112.136]</span><br><span class="line">[certs] Generating &quot;etcd/peer&quot; certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed for DNS names [master03 localhost] and IPs [192.168.112.130 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;apiserver-etcd-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd/server&quot; certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed for DNS names [master03 localhost] and IPs [192.168.112.130 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;etcd/healthcheck-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;front-proxy-client&quot; certificate and key</span><br><span class="line">[certs] Valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot;</span><br><span class="line">[certs] Using the existing &quot;sa&quot; key</span><br><span class="line">[kubeconfig] Generating kubeconfig files</span><br><span class="line">[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;</span><br><span class="line">[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address</span><br><span class="line">[kubeconfig] Using existing kubeconfig file: &quot;/etc/kubernetes/admin.conf&quot;</span><br><span class="line">[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file</span><br><span class="line">[control-plane] Using manifest folder &quot;/etc/kubernetes/manifests&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;</span><br><span class="line">W0315 11:49:29.220424    2807 manifests.go:214] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;</span><br><span class="line">W0315 11:49:29.225217    2807 manifests.go:214] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;</span><br><span class="line">W0315 11:49:29.226261    2807 manifests.go:214] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;</span><br><span class="line">[check-etcd] Checking that the etcd cluster is healthy</span><br><span class="line">[kubelet-start] Downloading configuration for the kubelet from the &quot;kubelet-config-1.17&quot; ConfigMap in the kube-system namespace</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class="line">[kubelet-start] Starting the kubelet</span><br><span class="line">[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...</span><br><span class="line">[etcd] Announced new etcd member joining to the existing etcd cluster</span><br><span class="line">[etcd] Creating static Pod manifest for &quot;etcd&quot;</span><br><span class="line">[etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s</span><br><span class="line">&#123;&quot;level&quot;:&quot;warn&quot;,&quot;ts&quot;:&quot;2020-03-15T11:49:56.765+0800&quot;,&quot;caller&quot;:&quot;clientv3/retry_interceptor.go:61&quot;,&quot;msg&quot;:&quot;retrying of unary invoker failed&quot;,&quot;target&quot;:&quot;passthrough:///https://192.168.112.130:2379&quot;,&quot;attempt&quot;:0,&quot;error&quot;:&quot;rpc error: code = DeadlineExceeded desc = context deadline exceeded&quot;&#125;</span><br><span class="line">[upload-config] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[mark-control-plane] Marking the node master03 as control-plane by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot;</span><br><span class="line">[mark-control-plane] Marking the node master03 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line"></span><br><span class="line">This node has joined the cluster and a new control plane instance was created:</span><br><span class="line"></span><br><span class="line">* Certificate signing request was sent to apiserver and approval was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line">* Control plane (master) label and taint were applied to the new node.</span><br><span class="line">* The Kubernetes control plane instances scaled up.</span><br><span class="line">* A new etcd member was added to the local/stacked etcd cluster.</span><br><span class="line"></span><br><span class="line">To start administering your cluster from this node, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">	mkdir -p $HOME/.kube</span><br><span class="line">	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">	sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">Run &apos;kubectl get nodes&apos; to see this node join the cluster.</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>
<h2 id="6-Kubernetes-Control-Plane的三个节点上分别配置Kubectl访问权限"><a href="#6-Kubernetes-Control-Plane的三个节点上分别配置Kubectl访问权限" class="headerlink" title="6. Kubernetes Control Plane的三个节点上分别配置Kubectl访问权限"></a>6. Kubernetes Control Plane的三个节点上分别配置Kubectl访问权限</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 在master01、master02和master03上分别执行</span><br><span class="line">rm -rf $HOME/.kube</span><br><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">chown $(id -u):$(id -g) $HOME/.kube/config</span><br></pre></td></tr></table></figure>
<h2 id="7-高可用部署的Stack结构验证"><a href="#7-高可用部署的Stack结构验证" class="headerlink" title="7. 高可用部署的Stack结构验证"></a>7. 高可用部署的Stack结构验证</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"># 在master01、master02和master03中的任意一个master执行都可以</span><br><span class="line">kubectl get pod --all-namespaces -o wide</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE     IP                NODE       NOMINATED NODE   READINESS GATES</span><br><span class="line">kube-system   calico-kube-controllers-648f4868b8-gszmn   1/1     Running   0          2m36s   10.211.235.1      master03   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   calico-node-dk4s6                          1/1     Running   0          2m36s   192.168.112.128   master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   calico-node-lhj5p                          1/1     Running   0          2m36s   192.168.112.129   master02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   calico-node-tscpz                          1/1     Running   0          2m36s   192.168.112.130   master03   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   coredns-7f9c544f75-9w4kn                   1/1     Running   0          12m     10.211.59.193     master02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   coredns-7f9c544f75-xvsbn                   1/1     Running   0          12m     10.211.59.194     master02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   etcd-master01                              1/1     Running   0          12m     192.168.112.128   master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   etcd-master02                              1/1     Running   0          5m58s   192.168.112.129   master02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   etcd-master03                              1/1     Running   0          3m46s   192.168.112.130   master03   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-apiserver-master01                    1/1     Running   0          12m     192.168.112.128   master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-apiserver-master02                    1/1     Running   0          5m59s   192.168.112.129   master02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-apiserver-master03                    1/1     Running   0          3m46s   192.168.112.130   master03   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-controller-manager-master01           1/1     Running   1          12m     192.168.112.128   master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-controller-manager-master02           1/1     Running   0          5m59s   192.168.112.129   master02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-controller-manager-master03           1/1     Running   0          3m46s   192.168.112.130   master03   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-haproxy-master01                      1/1     Running   0          12m     192.168.112.128   master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-keepalived-master01                   1/1     Running   0          12m     192.168.112.128   master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-proxy-6fw8x                           1/1     Running   0          12m     192.168.112.128   master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-proxy-7hkv7                           1/1     Running   0          6m      192.168.112.129   master02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-proxy-9trwk                           1/1     Running   0          3m47s   192.168.112.130   master03   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-scheduler-master01                    1/1     Running   1          12m     192.168.112.128   master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-scheduler-master02                    1/1     Running   0          5m59s   192.168.112.129   master02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-scheduler-master03                    1/1     Running   0          3m46s   192.168.112.130   master03   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"># 在master01、master02和master03中的任意一个master执行都可以</span><br><span class="line">kubectl get node -o wide</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">NAME       STATUS   ROLES    AGE     VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME</span><br><span class="line">master01   Ready    master   13m     v1.17.0   192.168.112.128   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://18.9.9</span><br><span class="line">master02   Ready    master   6m40s   v1.17.0   192.168.112.129   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://18.9.9</span><br><span class="line">master03   Ready    master   4m27s   v1.17.0   192.168.112.130   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://18.9.9</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>
<h2 id="8-确认etcd的健康状况"><a href="#8-确认etcd的健康状况" class="headerlink" title="8. 确认etcd的健康状况"></a>8. 确认etcd的健康状况</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># master01、master02和master03上分别执行，这里以master01为例</span><br><span class="line">kubectl exec -it etcd-master01 /bin/sh -n kube-system</span><br><span class="line"></span><br><span class="line">etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key member list</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">ade36780a0899522, started, master01, https://192.168.112.128:2380, https://192.168.112.128:2379, false</span><br><span class="line">b4a6061544dbd63b, started, master03, https://192.168.112.130:2380, https://192.168.112.130:2379, false</span><br><span class="line">ecaa91fc374ff6f0, started, master02, https://192.168.112.129:2380, https://192.168.112.129:2379, false</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key endpoint health</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">https://127.0.0.1:2379 is healthy: successfully committed proposal: took = 9.338525ms</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key endpoint status</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">https://127.0.0.1:2379, ade36780a0899522, 3.4.3, 2.6 MB, false, false, 21, 53251, 53251,</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>
<h2 id="9-为高可用集群添加两个Node"><a href="#9-为高可用集群添加两个Node" class="headerlink" title="9. 为高可用集群添加两个Node"></a>9. 为高可用集群添加两个Node</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"># node01上执行（无论是自动分发根证书的方式还是手动分发证书的方式，在这里都没有区别）</span><br><span class="line">kubeadm join 192.168.112.136:8443 --token abcdef.0123456789abcdef \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:33a6370b4bb4a9385c1d878e9a7a085ad969d521e4b309b01be797c0d7867d69</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">W0315 11:12:27.853703    9587 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[preflight] Reading configuration from the cluster...</span><br><span class="line">[preflight] FYI: You can look at this config file with &apos;kubectl -n kube-system get cm kubeadm-config -oyaml&apos;</span><br><span class="line">[kubelet-start] Downloading configuration for the kubelet from the &quot;kubelet-config-1.17&quot; ConfigMap in the kube-system namespace</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class="line">[kubelet-start] Starting the kubelet</span><br><span class="line">[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...</span><br><span class="line"></span><br><span class="line">This node has joined the cluster:</span><br><span class="line">* Certificate signing request was sent to apiserver and a response was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line"></span><br><span class="line">Run &apos;kubectl get nodes&apos; on the control-plane to see this node join the cluster.</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"># node02上执行（无论是自动分发根证书的方式还是手动分发证书的方式，在这里都没有区别）</span><br><span class="line">kubeadm join 192.168.112.136:8443 --token abcdef.0123456789abcdef \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:33a6370b4bb4a9385c1d878e9a7a085ad969d521e4b309b01be797c0d7867d69</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">W0315 11:13:18.680949    9561 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[preflight] Reading configuration from the cluster...</span><br><span class="line">[preflight] FYI: You can look at this config file with &apos;kubectl -n kube-system get cm kubeadm-config -oyaml&apos;</span><br><span class="line">[kubelet-start] Downloading configuration for the kubelet from the &quot;kubelet-config-1.17&quot; ConfigMap in the kube-system namespace</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class="line">[kubelet-start] Starting the kubelet</span><br><span class="line">[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...</span><br><span class="line"></span><br><span class="line">This node has joined the cluster:</span><br><span class="line">* Certificate signing request was sent to apiserver and a response was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line"></span><br><span class="line">Run &apos;kubectl get nodes&apos; on the control-plane to see this node join the cluster.</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"># 在master01、master02和master03中的任意一个master执行都可以</span><br><span class="line">kubectl get node -o wide</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">NAME       STATUS   ROLES    AGE     VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME</span><br><span class="line">master01   Ready    master   23m     v1.17.0   192.168.112.128   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://18.9.9</span><br><span class="line">master02   Ready    master   17m     v1.17.0   192.168.112.129   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://18.9.9</span><br><span class="line">master03   Ready    master   15m     v1.17.0   192.168.112.130   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://18.9.9</span><br><span class="line">node01     Ready    &lt;none&gt;   4m59s   v1.17.0   192.168.112.131   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://18.9.9</span><br><span class="line">node02     Ready    &lt;none&gt;   4m8s    v1.17.0   192.168.112.132   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://18.9.9</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"># 在master01、master02和master03中的任意一个master执行都可以</span><br><span class="line">kubectl get pod --all-namespaces -o wide</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE     IP                NODE       NOMINATED NODE   READINESS GATES</span><br><span class="line">kube-system   calico-kube-controllers-648f4868b8-gszmn   1/1     Running   0          14m     10.211.235.1      master03   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   calico-node-dk4s6                          1/1     Running   0          14m     192.168.112.128   master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   calico-node-lhj5p                          1/1     Running   0          14m     192.168.112.129   master02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   calico-node-lkl66                          1/1     Running   0          4m43s   192.168.112.132   node02     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   calico-node-ncjc4                          1/1     Running   0          5m34s   192.168.112.131   node01     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   calico-node-tscpz                          1/1     Running   0          14m     192.168.112.130   master03   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   coredns-7f9c544f75-9w4kn                   1/1     Running   0          24m     10.211.59.193     master02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   coredns-7f9c544f75-xvsbn                   1/1     Running   0          24m     10.211.59.194     master02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   etcd-master01                              1/1     Running   0          24m     192.168.112.128   master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   etcd-master02                              1/1     Running   0          18m     192.168.112.129   master02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   etcd-master03                              1/1     Running   0          15m     192.168.112.130   master03   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-apiserver-master01                    1/1     Running   0          24m     192.168.112.128   master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-apiserver-master02                    1/1     Running   0          18m     192.168.112.129   master02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-apiserver-master03                    1/1     Running   0          15m     192.168.112.130   master03   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-controller-manager-master01           1/1     Running   1          24m     192.168.112.128   master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-controller-manager-master02           1/1     Running   0          18m     192.168.112.129   master02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-controller-manager-master03           1/1     Running   0          15m     192.168.112.130   master03   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-haproxy-master01                      1/1     Running   0          24m     192.168.112.128   master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-keepalived-master01                   1/1     Running   0          24m     192.168.112.128   master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-proxy-6fw8x                           1/1     Running   0          24m     192.168.112.128   master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-proxy-7hkv7                           1/1     Running   0          18m     192.168.112.129   master02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-proxy-96cz5                           1/1     Running   0          5m34s   192.168.112.131   node01     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-proxy-9trwk                           1/1     Running   0          15m     192.168.112.130   master03   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-proxy-pwslt                           1/1     Running   0          4m43s   192.168.112.132   node02     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-scheduler-master01                    1/1     Running   1          24m     192.168.112.128   master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-scheduler-master02                    1/1     Running   0          18m     192.168.112.129   master02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-scheduler-master03                    1/1     Running   0          15m     192.168.112.130   master03   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>
<h1 id="五、关于所有节点（Master和Node）的重置"><a href="#五、关于所有节点（Master和Node）的重置" class="headerlink" title="五、关于所有节点（Master和Node）的重置"></a>五、关于所有节点（Master和Node）的重置</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubeadm reset</span><br><span class="line">rm -rf /etc/kubernetes/ /var/lib/etcd/ /etc/cni/ $HOME/.kube/</span><br><span class="line">reboot</span><br></pre></td></tr></table></figure>
<h1 id="六、参考资料"><a href="#六、参考资料" class="headerlink" title="六、参考资料"></a>六、参考资料</h1><h2 id="1-官方资料（官方最新版本v1-17）"><a href="#1-官方资料（官方最新版本v1-17）" class="headerlink" title="1. 官方资料（官方最新版本v1.17）"></a>1. 官方资料（官方最新版本v1.17）</h2><p><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/" target="_blank" rel="noopener">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/</a><br><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/" target="_blank" rel="noopener">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/</a></p>
<h2 id="2-第三方资料（因Kubernetes-从v1-15开始到v1-17，kubeadm的安装方式和二进制安装方式基本相同，故v1-15的资料可以供v1-17参考）"><a href="#2-第三方资料（因Kubernetes-从v1-15开始到v1-17，kubeadm的安装方式和二进制安装方式基本相同，故v1-15的资料可以供v1-17参考）" class="headerlink" title="2. 第三方资料（因Kubernetes 从v1.15开始到v1.17，kubeadm的安装方式和二进制安装方式基本相同，故v1.15的资料可以供v1.17参考）"></a>2. 第三方资料（因Kubernetes 从v1.15开始到v1.17，kubeadm的安装方式和二进制安装方式基本相同，故v1.15的资料可以供v1.17参考）</h2><p><a href="https://www.cnblogs.com/lingfenglian/p/11753590.html" target="_blank" rel="noopener">https://www.cnblogs.com/lingfenglian/p/11753590.html</a><br><a href="https://blog.51cto.com/fengwan/2426528?source=dra" target="_blank" rel="noopener">https://blog.51cto.com/fengwan/2426528?source=dra</a><br><a href="https://my.oschina.net/beyondken/blog/1935402" target="_blank" rel="noopener">https://my.oschina.net/beyondken/blog/1935402</a><br><a href="https://www.cnblogs.com/shenlinken/p/9968274.html" target="_blank" rel="noopener">https://www.cnblogs.com/shenlinken/p/9968274.html</a> </p>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/03/15/binary_kubernetes_ha_cluster_000/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">&lt;</strong>
      <div class="article-nav-title">
        
          使用二进制文件安装高可用Kubernetes v1.17.0集群（Stacked Control Plane Nodes For Baremetal）
        
      </div>
    </a>
  
  
    <a href="/2020/03/04/kubernetes_harbor_000/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Kubernetes Harbor 的安装、配置和使用</div>
      <strong class="article-nav-caption">&gt;</strong>
    </a>
  
</nav>

  
</article>






</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2020 Singh Wang
      </div>
        <div class="footer-right">
          <a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/smackgg/hexo-theme-smackdown" target="_blank">Smackdown</a>
        </div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="/js/main.js"></script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js"></script>


  </div>
</body>
</html>