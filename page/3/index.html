<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  <title>Singh Wang</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Singh Wang">
<meta property="og:url" content="http://yoursite.com/page/3/index.html">
<meta property="og:site_name" content="Singh Wang">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Singh Wang">
  
    <link rel="alternative" href="/atom.xml" title="Singh Wang" type="application/atom+xml">
  
  
    <link rel="icon" href="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg">
  
  <link rel="stylesheet" href="/css/style.css">
  
  

  <script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>
  <script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>

  
</head></html>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="https://avatars0.githubusercontent.com/u/17465385?s=400&amp;u=cea56ed1710ec6ebab160c1ce1e068317e05b025&amp;v=4" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Singh Wang</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/archives">所有文章</a></li>
				        
							<li><a href="/categories/容器云技术/">容器云技术</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/singhwang" title="github">github</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/Calico/" style="font-size: 11.43px;">Calico</a> <a href="/tags/Ceph/" style="font-size: 10px;">Ceph</a> <a href="/tags/Common/" style="font-size: 10px;">Common</a> <a href="/tags/DNS/" style="font-size: 10px;">DNS</a> <a href="/tags/Docker/" style="font-size: 20px;">Docker</a> <a href="/tags/Dragonfly/" style="font-size: 10px;">Dragonfly</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Fluentd/" style="font-size: 11.43px;">Fluentd</a> <a href="/tags/GlusterFS/" style="font-size: 14.29px;">GlusterFS</a> <a href="/tags/HAProxy/" style="font-size: 10px;">HAProxy</a> <a href="/tags/Highly-Available/" style="font-size: 10px;">Highly Available</a> <a href="/tags/Ingress/" style="font-size: 10px;">Ingress</a> <a href="/tags/Kubeadm/" style="font-size: 11.43px;">Kubeadm</a> <a href="/tags/Kubernetes/" style="font-size: 18.57px;">Kubernetes</a> <a href="/tags/Label/" style="font-size: 10px;">Label</a> <a href="/tags/Logging/" style="font-size: 12.86px;">Logging</a> <a href="/tags/Monitoring/" style="font-size: 14.29px;">Monitoring</a> <a href="/tags/Network/" style="font-size: 15.71px;">Network</a> <a href="/tags/Open-Falcon/" style="font-size: 10px;">Open Falcon</a> <a href="/tags/P2P/" style="font-size: 10px;">P2P</a> <a href="/tags/Prometheus/" style="font-size: 11.43px;">Prometheus</a> <a href="/tags/Setup/" style="font-size: 12.86px;">Setup</a> <a href="/tags/Smartping/" style="font-size: 10px;">Smartping</a> <a href="/tags/Source-Build/" style="font-size: 10px;">Source Build</a> <a href="/tags/Storage/" style="font-size: 15.71px;">Storage</a> <a href="/tags/TimeZone/" style="font-size: 10px;">TimeZone</a> <a href="/tags/Trouble-Shooting/" style="font-size: 17.14px;">Trouble Shooting</a>
					</div>
				</section>
				
				
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">Singh Wang</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="https://avatars0.githubusercontent.com/u/17465385?s=400&amp;u=cea56ed1710ec6ebab160c1ce1e068317e05b025&amp;v=4" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author">Singh Wang</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/archives">所有文章</a></li>
		        
					<li><a href="/categories/容器云技术/">容器云技术</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/singhwang" title="github">github</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap">
  
    <article id="post-fluentd_trouble_shooting_001" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/fluentd_trouble_shooting_001/" class="article-date">
  	<time datetime="2019-05-27T04:32:32.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/fluentd_trouble_shooting_001/">
        Fluentd不使用运行时的时区向Elasticsearch写入日志记录，怎么解决？
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、环境版本信息"><a href="#一、环境版本信息" class="headerlink" title="一、环境版本信息"></a>一、环境版本信息</h1><p>Fluentd 1.1.0<br>Elasticsearch 6.3.0<br>fluent-plugin-elasticsearch 2.4.1</p>
<h1 id="二、报错现象描述"><a href="#二、报错现象描述" class="headerlink" title="二、报错现象描述"></a>二、报错现象描述</h1><p>以时区设置为Asia/Shanghai为例：<br>虽然Fluentd所在的运行时已经设置正确，但是上报到Elasticsearch对应索引中的日志记录，与实际相比还是相差八个小时。更直接的现象是，你发现明明应该归档在今天对应的索引中的日志，却归档在了昨天对应的索引中。再有就是你会发现，无论你怎么修改运行时的时区，Fluentd向Elasticsearch写入日志记录都始终使用UTC时区。</p>
<h1 id="三、解决方法"><a href="#三、解决方法" class="headerlink" title="三、解决方法"></a>三、解决方法</h1><p>把utc_index的值设置为false，允许Fluentd使用本地运行时设置的时区。</p>
<p>官方文档的原文描述如下：（详见参考资料部分的链接）<br>By default, the records inserted into index logstash-YYMMDD with UTC (Coordinated Universal Time). This option allows to use local time if you describe utc_index to false.</p>
<p>配置修改示例如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&lt;match **&gt;</span><br><span class="line">  @id elasticsearch</span><br><span class="line">  @type elasticsearch</span><br><span class="line">  @log_level info</span><br><span class="line">  include_tag_key true</span><br><span class="line">  host elasticsearch-ingest.logging</span><br><span class="line">  port 9200</span><br><span class="line">  logstash_format true</span><br><span class="line">  utc_index false</span><br><span class="line">  slow_flush_log_threshold 60.0</span><br><span class="line">  &lt;buffer&gt;</span><br><span class="line">    @type file</span><br><span class="line">    path /var/log/fluentd-buffers/kubernetes.system.buffer</span><br><span class="line">    flush_mode interval</span><br><span class="line">    retry_type exponential_backoff</span><br><span class="line">    flush_thread_count 2</span><br><span class="line">    flush_interval 5s</span><br><span class="line">    retry_forever</span><br><span class="line">    retry_max_interval 30</span><br><span class="line">    chunk_limit_size 6M</span><br><span class="line">    queue_limit_length 256</span><br><span class="line">    overflow_action block</span><br><span class="line">  &lt;/buffer&gt;</span><br><span class="line">&lt;/match&gt;</span><br></pre></td></tr></table></figure></p>
<h1 id="四、参考资料"><a href="#四、参考资料" class="headerlink" title="四、参考资料"></a>四、参考资料</h1><p><a href="https://github.com/uken/fluent-plugin-elasticsearch/tree/v2.4.1#utc_index" target="_blank" rel="noopener">https://github.com/uken/fluent-plugin-elasticsearch/tree/v2.4.1#utc_index</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Fluentd/">Fluentd</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Trouble-Shooting/">Trouble Shooting</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-fluentd_trouble_shooting_000" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/fluentd_trouble_shooting_000/" class="article-date">
  	<time datetime="2019-05-27T04:32:26.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/fluentd_trouble_shooting_000/">
        Fluentd报错批处理索引队列满了，该怎么解决？
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、环境版本信息"><a href="#一、环境版本信息" class="headerlink" title="一、环境版本信息"></a>一、环境版本信息</h1><p>Fluentd 1.1.0<br>Elasticsearch 6.3.0</p>
<h1 id="二、报错信息"><a href="#二、报错信息" class="headerlink" title="二、报错信息"></a>二、报错信息</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">2018-12-20 03:50:41 +0000 [info]: [elasticsearch] Connection opened to Elasticsearch cluster =&gt; &#123;:host=&gt;&quot;elasticsearch.logging&quot;, :port=&gt;9200, :scheme=&gt;&quot;http&quot;&#125;</span><br><span class="line">2018-12-20 20:28:23 +0000 [warn]: [elasticsearch] failed to flush the buffer. retry_time=0 next_retry_seconds=2018-12-20 20:28:24 +0000 chunk=&quot;57d79f767802e630777d6da9f12a1fd7&quot; error_class=Fluent::Plugin::ElasticsearchErrorHandler::BulkIndexQueueFull error=&quot;Bulk index queue is full, retrying&quot;</span><br><span class="line">  2018-12-20 20:28:23 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluent-plugin-elasticsearch-2.4.1/lib/fluent/plugin/elasticsearch_error_handler.rb:90:in `block in handle_error&apos;</span><br><span class="line">  2018-12-20 20:28:23 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluent-plugin-elasticsearch-2.4.1/lib/fluent/plugin/elasticsearch_error_handler.rb:85:in `each_key&apos;</span><br><span class="line">  2018-12-20 20:28:23 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluent-plugin-elasticsearch-2.4.1/lib/fluent/plugin/elasticsearch_error_handler.rb:85:in `handle_error&apos;</span><br><span class="line">  2018-12-20 20:28:23 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluent-plugin-elasticsearch-2.4.1/lib/fluent/plugin/out_elasticsearch.rb:434:in `send_bulk&apos;</span><br><span class="line">  2018-12-20 20:28:23 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluent-plugin-elasticsearch-2.4.1/lib/fluent/plugin/out_elasticsearch.rb:418:in `write&apos;</span><br><span class="line">  2018-12-20 20:28:23 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin/output.rb:1094:in `try_flush&apos;</span><br><span class="line">  2018-12-20 20:28:23 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin/output.rb:1319:in `flush_thread_run&apos;</span><br><span class="line">  2018-12-20 20:28:23 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin/output.rb:439:in `block (2 levels) in start&apos;</span><br><span class="line">  2018-12-20 20:28:23 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin_helper/thread.rb:78:in `block in thread_create&apos;</span><br><span class="line">2018-12-20 20:28:24 +0000 [warn]: [elasticsearch] failed to flush the buffer. retry_time=0 next_retry_seconds=2018-12-20 20:28:24 +0000 chunk=&quot;57d79f767802e630777d6da9f12a1fd7&quot; error_class=Fluent::Plugin::ElasticsearchErrorHandler::BulkIndexQueueFull error=&quot;Bulk index queue is full, retrying&quot;</span><br><span class="line">  2018-12-20 20:28:24 +0000 [warn]: suppressed same stacktrace</span><br><span class="line">2018-12-20 20:28:25 +0000 [warn]: [elasticsearch] failed to flush the buffer. retry_time=1 next_retry_seconds=2018-12-20 20:28:25 +0000 chunk=&quot;57d79f767802e630777d6da9f12a1fd7&quot; error_class=Fluent::Plugin::ElasticsearchErrorHandler::BulkIndexQueueFull error=&quot;Bulk index queue is full, retrying&quot;</span><br><span class="line">  2018-12-20 20:28:25 +0000 [warn]: suppressed same stacktrace</span><br><span class="line">2018-12-20 20:28:26 +0000 [warn]: [elasticsearch] failed to flush the buffer. retry_time=2 next_retry_seconds=2018-12-20 20:28:27 +0000 chunk=&quot;57d79f78b9fce084be8b1f544a046a09&quot; error_class=Fluent::Plugin::ElasticsearchErrorHandler::BulkIndexQueueFull error=&quot;Bulk index queue is full, retrying&quot;</span><br><span class="line">  2018-12-20 20:28:26 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluent-plugin-elasticsearch-2.4.1/lib/fluent/plugin/elasticsearch_error_handler.rb:90:in `block in handle_error&apos;</span><br><span class="line">  2018-12-20 20:28:26 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluent-plugin-elasticsearch-2.4.1/lib/fluent/plugin/elasticsearch_error_handler.rb:85:in `each_key&apos;</span><br><span class="line">  2018-12-20 20:28:26 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluent-plugin-elasticsearch-2.4.1/lib/fluent/plugin/elasticsearch_error_handler.rb:85:in `handle_error&apos;</span><br><span class="line">  2018-12-20 20:28:26 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluent-plugin-elasticsearch-2.4.1/lib/fluent/plugin/out_elasticsearch.rb:434:in `send_bulk&apos;</span><br><span class="line">  2018-12-20 20:28:26 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluent-plugin-elasticsearch-2.4.1/lib/fluent/plugin/out_elasticsearch.rb:418:in `write&apos;</span><br><span class="line">  2018-12-20 20:28:26 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin/output.rb:1094:in `try_flush&apos;</span><br><span class="line">  2018-12-20 20:28:26 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin/output.rb:1319:in `flush_thread_run&apos;</span><br><span class="line">  2018-12-20 20:28:26 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin/output.rb:439:in `block (2 levels) in start&apos;</span><br><span class="line">  2018-12-20 20:28:26 +0000 [warn]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin_helper/thread.rb:78:in `block in thread_create&apos;</span><br><span class="line">2018-12-20 20:28:30 +0000 [warn]: [elasticsearch] failed to flush the buffer. retry_time=3 next_retry_seconds=2018-12-20 20:28:30 +0000 chunk=&quot;57d79f78b9fce084be8b1f544a046a09&quot; error_class=Fluent::Plugin::ElasticsearchErrorHandler::BulkIndexQueueFull error=&quot;Bulk index queue is full, retrying&quot;</span><br><span class="line">  2018-12-20 20:28:30 +0000 [warn]: suppressed same stacktrace</span><br><span class="line">2018-12-20 20:28:30 +0000 [warn]: [elasticsearch] failed to flush the buffer. retry_time=4 next_retry_seconds=2018-12-20 20:28:39 +0000 chunk=&quot;57d79f767802e630777d6da9f12a1fd7&quot; error_class=Fluent::Plugin::ElasticsearchErrorHandler::BulkIndexQueueFull error=&quot;Bulk index queue is full, retrying&quot;</span><br><span class="line">  2018-12-20 20:28:30 +0000 [warn]: suppressed same stacktrace</span><br><span class="line">2018-12-20 20:28:45 +0000 [warn]: [elasticsearch] failed to flush the buffer. retry_time=5 next_retry_seconds=2018-12-20 20:28:44 +0000 chunk=&quot;57d79f767802e630777d6da9f12a1fd7&quot; error_class=Fluent::Plugin::ElasticsearchErrorHandler::BulkIndexQueueFull error=&quot;Bulk index queue is full, retrying&quot;</span><br><span class="line">  2018-12-20 20:28:45 +0000 [warn]: suppressed same stacktrace</span><br><span class="line">2018-12-20 20:28:45 +0000 [warn]: [elasticsearch] failed to flush the buffer. retry_time=6 next_retry_seconds=2018-12-20 20:29:18 +0000 chunk=&quot;57d79f78b9fce084be8b1f544a046a09&quot; error_class=Fluent::Plugin::ElasticsearchErrorHandler::BulkIndexQueueFull error=&quot;Bulk index queue is full, retrying&quot;</span><br><span class="line">  2018-12-20 20:28:45 +0000 [warn]: suppressed same stacktrace</span><br><span class="line">2018-12-20 20:29:13 +0000 [warn]: [elasticsearch] retry succeeded. chunk_id=&quot;57d79f78b9fce084be8b1f544a046a09&quot;</span><br></pre></td></tr></table></figure>
<h1 id="三、解决方法"><a href="#三、解决方法" class="headerlink" title="三、解决方法"></a>三、解决方法</h1><p>在配置文件elasticsearch.yml中，增加如下配置：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">thread_pool:</span><br><span class="line">  index:</span><br><span class="line">    queue_size: $&#123;INDEX_QUEUE_SIZE:200&#125;</span><br><span class="line">  write:</span><br><span class="line">    queue_size: $&#123;WRITE_QUEUE_SIZE:200&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>在pod、deployment或者statefulset等pod相关的yaml中，增加如下环境变量的配置：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">        env:</span><br><span class="line">        - name: INDEX_QUEUE_SIZE</span><br><span class="line">          value: &quot;1000&quot;</span><br><span class="line">        - name: WRITE_QUEUE_SIZE</span><br><span class="line">          value: &quot;1000&quot;</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>如何查看配置的修改是否生效？即查看Elasticsearch的线程池配置。（注意：不通版本的Elasticsearch查看方式有所不同，详见参考资料中的链接文档）<br>提示：推荐使用 Kibana 的 Dev Tools进行查询，这种方式最为方便。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GET _cat/thread_pool/index?v&amp;h=id,node_name,name,active,queue,rejected,completed,queue_size</span><br><span class="line">GET _cat/thread_pool/write?v&amp;h=id,node_name,name,active,queue,rejected,completed,queue_size</span><br></pre></td></tr></table></figure></p>
<h1 id="四、参考资料"><a href="#四、参考资料" class="headerlink" title="四、参考资料"></a>四、参考资料</h1><p><a href="https://blog.csdn.net/opensure/article/details/51491815" target="_blank" rel="noopener">https://blog.csdn.net/opensure/article/details/51491815</a><br><a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.4/modules-threadpool.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/2.4/modules-threadpool.html</a><br><a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.3/modules-threadpool.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/6.3/modules-threadpool.html</a><br><a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.3/cat-thread-pool.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/6.3/cat-thread-pool.html</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Fluentd/">Fluentd</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Trouble-Shooting/">Trouble Shooting</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-elasticsearch_resources" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/elasticsearch_resources/" class="article-date">
  	<time datetime="2019-05-27T04:32:20.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/elasticsearch_resources/">
        Elasticsearch 经典资料汇总
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="中文资料"><a href="#中文资料" class="headerlink" title="中文资料"></a>中文资料</h1><p><a href="https://es.xiaoleilu.com" target="_blank" rel="noopener">https://es.xiaoleilu.com</a><br><a href="https://github.com/elasticsearch-cn/elasticsearch-definitive-guide" target="_blank" rel="noopener">https://github.com/elasticsearch-cn/elasticsearch-definitive-guide</a><br><a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/index.html" target="_blank" rel="noopener">https://www.elastic.co/guide/cn/elasticsearch/guide/current/index.html</a></p>
<h1 id="Elasticsearch-Client-Golang-SDK"><a href="#Elasticsearch-Client-Golang-SDK" class="headerlink" title="Elasticsearch Client Golang SDK"></a>Elasticsearch Client Golang SDK</h1><p><a href="https://github.com/olivere/elastic/tree/v6.2.11" target="_blank" rel="noopener">https://github.com/olivere/elastic/tree/v6.2.11</a></p>
<h1 id="深度分页问题"><a href="#深度分页问题" class="headerlink" title="深度分页问题"></a>深度分页问题</h1><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.3/search-request-search-after.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/6.3/search-request-search-after.html</a><br><a href="https://blog.csdn.net/u011228889/article/details/79760167" target="_blank" rel="noopener">https://blog.csdn.net/u011228889/article/details/79760167</a><br><a href="https://blog.csdn.net/WangPing1223/article/details/79148244" target="_blank" rel="noopener">https://blog.csdn.net/WangPing1223/article/details/79148244</a></p>
<h1 id="Kubernetes上非常好用的Elasticsearch"><a href="#Kubernetes上非常好用的Elasticsearch" class="headerlink" title="Kubernetes上非常好用的Elasticsearch"></a>Kubernetes上非常好用的Elasticsearch</h1><p><a href="https://github.com/pires/kubernetes-elasticsearch-cluster/tree/6.3.0" target="_blank" rel="noopener">https://github.com/pires/kubernetes-elasticsearch-cluster/tree/6.3.0</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Elasticsearch/">Elasticsearch</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Logging/">Logging</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-docker_trouble_shooting_000" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/docker_trouble_shooting_000/" class="article-date">
  	<time datetime="2019-05-27T04:32:12.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/docker_trouble_shooting_000/">
        删除Docker容器或者重启Docker Daemon时，遇到device or resource busy，该怎么办？
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>前段时间，在内部的线上环境修复Kubernetes集群时，需要重启Docker Daemon，可以悲剧发生了，一重启就报错：device or resource busy。Google了好久，终于找到了类似的案例和解决方法。此时才知道，原来删除Docker容器，也可能会出现这个问题。这里简单记录一下，原创详见“参考资料”。</p>
<p>原文给出的脚本：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">for i in $(curl -s --unix /var/run/docker.sock http://localhost/info | jq -r .DockerRootDir) /var/lib/docker /run /var/run; do</span><br><span class="line">    for m in $(tac /proc/mounts | awk &apos;&#123;print $2&#125;&apos; | grep ^$&#123;i&#125;/); do</span><br><span class="line">        umount $m || true</span><br><span class="line">    done</span><br><span class="line">done</span><br></pre></td></tr></table></figure></p>
<p>在我们内部的环境中，DockerRootDir是固定的，就在/var/lib/docker，为了避免安装jq依赖，调整为下面的脚本：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">for i in /var/lib/docker /run /var/run; do</span><br><span class="line">    for m in $(tac /proc/mounts | awk &apos;&#123;print $2&#125;&apos; | grep ^$&#123;i&#125;/); do</span><br><span class="line">        umount $m || true</span><br><span class="line">    done</span><br><span class="line">done</span><br></pre></td></tr></table></figure></p>
<p>参考资料：<br><a href="http://niusmallnan.com/2016/12/27/docker-device-resource-busy/" target="_blank" rel="noopener">http://niusmallnan.com/2016/12/27/docker-device-resource-busy/</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Trouble-Shooting/">Trouble Shooting</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-docker_haproxy" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/docker_haproxy/" class="article-date">
  	<time datetime="2019-05-27T04:32:04.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/docker_haproxy/">
        Docker HAProxy使用示例
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="1-准备Docker-HAProxy基础环境："><a href="#1-准备Docker-HAProxy基础环境：" class="headerlink" title="1. 准备Docker HAProxy基础环境："></a>1. 准备Docker HAProxy基础环境：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker pull haproxy:1.6.5</span><br><span class="line">mkdir haproxy</span><br><span class="line">cd haproxy</span><br></pre></td></tr></table></figure>
<h2 id="2-编写HAProxy的配置文件："><a href="#2-编写HAProxy的配置文件：" class="headerlink" title="2. 编写HAProxy的配置文件："></a>2. 编写HAProxy的配置文件：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    log 127.0.0.1 local0 # 日志输出配置，所有日志都记录在本机，通过local0输出</span><br><span class="line">    maxconn 4096 # 最大连接数</span><br><span class="line">    tune.ssl.default-dh-param 2048</span><br><span class="line">    chroot /usr/local/sbin # 改变当前工作目录</span><br><span class="line">    daemon # 以后台形式运行HAProxy</span><br><span class="line">    nbproc 4 # 启动4个HAProxy实例</span><br><span class="line">    pidfile /usr/local/sbin/haproxy.pid # pid文件位置</span><br><span class="line">defaults</span><br><span class="line">    log 127.0.0.1 local3 # 日志文件的输出定向</span><br><span class="line">    mode http # &#123; tcp|http|health &#125; 设定启动实例的协议类型</span><br><span class="line">    option dontlognull # 保证HAProxy不记录上级负载均衡发送过来的用于检测状态没有数据的心跳包</span><br><span class="line">    option redispatch # 当serverId对应的服务器挂掉后，强制定向到其他健康的服务器</span><br><span class="line">    retries 2 # 重试两次连接失败就认为服务器不可用，主要通过后面的check检查</span><br><span class="line">    maxconn 2000 # 最大连接数</span><br><span class="line">    balance roundrobin # 负载均衡算法，roundrobin表示轮询，source表示按照IP</span><br><span class="line">    timeout connect 5000ms # 连接超时时间</span><br><span class="line">    timeout client 50000ms # 客户端连接超时时间</span><br><span class="line">    timeout server 50000ms # 服务器端连接超时时间</span><br><span class="line">    errorfile 400 /usr/local/etc/haproxy/errors/400.http</span><br><span class="line">    errorfile 403 /usr/local/etc/haproxy/errors/403.http</span><br><span class="line">    errorfile 408 /usr/local/etc/haproxy/errors/408.http</span><br><span class="line">    errorfile 500 /usr/local/etc/haproxy/errors/500.http</span><br><span class="line">    errorfile 502 /usr/local/etc/haproxy/errors/502.http</span><br><span class="line">    errorfile 503 /usr/local/etc/haproxy/errors/503.http</span><br><span class="line">    errorfile 504 /usr/local/etc/haproxy/errors/504.http</span><br><span class="line">    balance source</span><br><span class="line"></span><br><span class="line">frontend http_frontend</span><br><span class="line">        bind *:80</span><br><span class="line">        mode http</span><br><span class="line"></span><br><span class="line">        default_backend server_harbor_backend</span><br><span class="line"></span><br><span class="line">backend server_backend</span><br><span class="line">        mode http</span><br><span class="line">        ...</span><br><span class="line">        server s1 192.168.86.128:31451</span><br><span class="line">        server s2 192.168.86.129:31451</span><br><span class="line">        server s3 192.168.86.130:31451</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<p>注意事项：宿主机的80端口未被占用；根据需要配置server_backend中的使用标记（…）圈上的部分。</p>
<h2 id="3-使用Docker镜像启动HAProxy容器："><a href="#3-使用Docker镜像启动HAProxy容器：" class="headerlink" title="3. 使用Docker镜像启动HAProxy容器："></a>3. 使用Docker镜像启动HAProxy容器：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 80:80 --name haproxy -v `pwd`/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg haproxy:1.6.5</span><br></pre></td></tr></table></figure>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HAProxy/">HAProxy</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-dns_query" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/dns_query/" class="article-date">
  	<time datetime="2019-05-27T04:31:57.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/dns_query/">
        DNS正解和反解查询示例
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="DNS正解和反解查询示例"><a href="#DNS正解和反解查询示例" class="headerlink" title="DNS正解和反解查询示例"></a>DNS正解和反解查询示例</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nslookup www.baidu.com 8.8.8.8 -port=53</span><br><span class="line">nslookup -querytype=ptr 120.114.100.20 8.8.8.8 -port=53</span><br><span class="line"></span><br><span class="line">dig @8.8.8.8 -p 53 www.baidu.com</span><br><span class="line">dig -p 53 -x 120.114.100.20 @8.8.8.8</span><br></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DNS/">DNS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Network/">Network</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-common-label" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/common-label/" class="article-date">
  	<time datetime="2019-05-27T04:31:49.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/common-label/">
        容器标签使用小记
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="docker-cli使用举例"><a href="#docker-cli使用举例" class="headerlink" title="docker cli使用举例"></a>docker cli使用举例</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker ps -a --filter &quot;label=org.hyperledger.fabric.chaincode.id.name=test-chaincode&quot; --filter &quot;label=org.hyperledger.fabric.chaincode.id.version=v0.0.1&quot;</span><br><span class="line"></span><br><span class="line">或者</span><br><span class="line"></span><br><span class="line">docker ps -a -f &quot;label=org.hyperledger.fabric.chaincode.id.name=test-chaincode&quot; -f &quot;label=org.hyperledger.fabric.chaincode.id.version=v0.0.1&quot;</span><br></pre></td></tr></table></figure>
<p>如下图所示：</p>
<p><img src="/2019/05/27/common-label/docker_label.png" alt="docker_label"></p>
<h1 id="docker-client-go编码示例"><a href="#docker-client-go编码示例" class="headerlink" title="docker client go编码示例"></a>docker client go编码示例</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">import (</span><br><span class="line">	&quot;github.com/docker/docker/api/types&quot;</span><br><span class="line"></span><br><span class="line">	&quot;github.com/docker/docker/api/types/filters&quot;</span><br><span class="line">)</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">		flagArgs := []string&#123;</span><br><span class="line">			fmt.Sprintf(&quot;label=%s=%s&quot;, ChainCodeNameLabelKey, contractName), // &quot;label=org.hyperledger.fabric.chaincode.id.name=wx-hy001&quot;</span><br><span class="line">			fmt.Sprintf(&quot;label=%s=%s&quot;, ChainCodeVersionLabelKey, contractVersion), // &quot;label=org.hyperledger.fabric.chaincode.id.version=1.2.0&quot;</span><br><span class="line">		&#125;</span><br><span class="line">		args := filters.NewArgs()</span><br><span class="line">		for i := range flagArgs &#123;</span><br><span class="line">			args, err = filters.ParseFlag(flagArgs[i], args)</span><br><span class="line">			if err != nil &#123;</span><br><span class="line">				glog.Error(err)</span><br><span class="line">				return contractChainCodes, err</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		containerList, err := cli.ContainerList(types.ContainerListOptions&#123;</span><br><span class="line">			Filters: args,</span><br><span class="line">		&#125;)</span><br><span class="line">		if err != nil &#123;</span><br><span class="line">			glog.Error(err)</span><br><span class="line">			return contractChainCodes, err</span><br><span class="line">		&#125;</span><br><span class="line">		for j := 0; j &lt; len(containerList); j++ &#123;</span><br><span class="line">			container := containerList[j]</span><br><span class="line">			containerId := container.ID</span><br><span class="line">			containerName := container.Names[0][1 : ]</span><br><span class="line">			contractChainCode := module.ContractChainCode&#123;</span><br><span class="line">				ContainerId: containerId,</span><br><span class="line">				ContainerName: containerName,</span><br><span class="line">				ContainerHost: hostIP,</span><br><span class="line">			&#125;</span><br><span class="line">			contractChainCodes = append(contractChainCodes, contractChainCode)</span><br><span class="line">		&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>如下图所示：</p>
<p><img src="/2019/05/27/common-label/docker_label_go.png" alt="docker_label"></p>
<h1 id="kubernetes-cli使用举例"><a href="#kubernetes-cli使用举例" class="headerlink" title="kubernetes cli使用举例"></a>kubernetes cli使用举例</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod --all-namespaces -o wide -l baas.yonghui.cn/network-name=test-network</span><br><span class="line"></span><br><span class="line">或者</span><br><span class="line"></span><br><span class="line">kubectl get pod --all-namespaces -o wide -l baas.yonghui.cn/network-name=test-network,baas.yonghui.cn/network-component=peer</span><br></pre></td></tr></table></figure>
<p>如下图所示：</p>
<p><img src="/2019/05/27/common-label/kubernetes_label.png" alt="kubernetes_label"></p>
<h1 id="kubernetes-client-go编码示例"><a href="#kubernetes-client-go编码示例" class="headerlink" title="kubernetes client go编码示例"></a>kubernetes client go编码示例</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">import (</span><br><span class="line">	module &quot;yonghui.cn/blockchain/yhbkas/module/network&quot;</span><br><span class="line">	pkglabels &quot;k8s.io/apimachinery/pkg/labels&quot;</span><br><span class="line">)</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">	networkNameKey := string(sc.NetworkNameKey)</span><br><span class="line">	labels := map[string]string&#123;</span><br><span class="line">		networkNameKey: networkName,</span><br><span class="line">	&#125;</span><br><span class="line">	selector := pkglabels.FormatLabels(labels)</span><br><span class="line">	opts := metav1.ListOptions&#123;</span><br><span class="line">		LabelSelector: selector,</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	podList, err := clientSet.CoreV1().Pods(metav1.NamespaceAll).List(opts)</span><br><span class="line">	if err != nil &#123;</span><br><span class="line">		return module.GetNetworkComponentsResponse&#123;</span><br><span class="line">			CommonResponse: module.CommonResponse&#123;</span><br><span class="line">				Status:  http.StatusInternalServerError,</span><br><span class="line">				Message: &quot;GetNetworkComponents unsuccessfully.&quot;,</span><br><span class="line">			&#125;,</span><br><span class="line">			NetworkComponents: networkComponents,</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	for i := 0; i &lt; len(podList.Items); i++  &#123;</span><br><span class="line">		pod := podList.Items[i]</span><br><span class="line">		podContainers := make([]module.PodContainer, 0)</span><br><span class="line">		containers := pod.Spec.Containers</span><br><span class="line">		for j := 0; j &lt; len(containers); j++ &#123;</span><br><span class="line">			container := containers[j]</span><br><span class="line">			podContainer := module.PodContainer&#123;</span><br><span class="line">				Name: container.Name,</span><br><span class="line">			&#125;</span><br><span class="line">			podContainers = append(podContainers, podContainer)</span><br><span class="line">		&#125;</span><br><span class="line">		labels := pod.Labels</span><br><span class="line">		componentTypeKey := string(sc.NetworkComponentTypeKey)</span><br><span class="line">		componentTypeValue := labels[componentTypeKey]</span><br><span class="line">		networkComponent := module.NetworkComponent&#123;</span><br><span class="line">			Id: string(pod.UID),</span><br><span class="line">			Name: pod.Name,</span><br><span class="line">			Namespace: pod.Namespace,</span><br><span class="line">			Containers: podContainers,</span><br><span class="line">			Role: componentTypeValue,</span><br><span class="line">		&#125;</span><br><span class="line">		networkComponents = append(networkComponents, networkComponent)</span><br><span class="line">	&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>如下图所示：</p>
<p><img src="/2019/05/27/common-label/kubernetes_label_go.png" alt="kubernetes_label"></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Common/">Common</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Label/">Label</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kubeadm_ha_clusters" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/kubeadm_ha_clusters/" class="article-date">
  	<time datetime="2019-05-27T04:31:36.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/kubeadm_ha_clusters/">
        使用Kubeadm安装高可用Kubernetes集群（Stacked Control Plane Nodes For Baremetal篇）
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、高可用部署的实现方式介绍"><a href="#一、高可用部署的实现方式介绍" class="headerlink" title="一、高可用部署的实现方式介绍"></a>一、高可用部署的实现方式介绍</h1><p>官方文档介绍了使用Kbeadm设置高可用性Kubernetes集群的两种不同方法：</p>
<h2 id="1-堆叠master的方式（with-stacked-masters）"><a href="#1-堆叠master的方式（with-stacked-masters）" class="headerlink" title="1. 堆叠master的方式（with stacked masters）"></a>1. 堆叠master的方式（with stacked masters）</h2><p>这种方法需要较少的基础设施。控制平面节点和etcd成员位于同一位置。</p>
<h2 id="2-使用外部etcd集群的方式（with-an-external-etcd-cluster）"><a href="#2-使用外部etcd集群的方式（with-an-external-etcd-cluster）" class="headerlink" title="2. 使用外部etcd集群的方式（with an external etcd cluster）"></a>2. 使用外部etcd集群的方式（with an external etcd cluster）</h2><p>这种方法需要更多的基础设施。控制平面节点和etcd成员是分开的。<br>这里重点介绍第一种方式，即堆叠master的方式。官方文档链接详见参考资料。</p>
<h1 id="二、实验环境版本信息"><a href="#二、实验环境版本信息" class="headerlink" title="二、实验环境版本信息"></a>二、实验环境版本信息</h1><p>docker 17.03.1-ce<br>kubeadm v1.11.0<br>kubelet v1.11.0<br>kubectl v1.11.0<br>calico v3.1.3</p>
<h1 id="三、部署架构介绍"><a href="#三、部署架构介绍" class="headerlink" title="三、部署架构介绍"></a>三、部署架构介绍</h1><h2 id="1-Kubernetes-Master（Control-Plane）"><a href="#1-Kubernetes-Master（Control-Plane）" class="headerlink" title="1. Kubernetes Master（Control Plane）"></a>1. Kubernetes Master（Control Plane）</h2><p>172.16.170.128 server01 -&gt; docker kubelet keepalived haproxy etcd kube-apiserver kube-controller-manager kube-scheduler kube-proxy calico-node<br>172.16.170.129 server02 -&gt; docker kubelet keepalived haproxy etcd kube-apiserver kube-controller-manager kube-scheduler kube-proxy calico-node<br>172.16.170.130 server03 -&gt; docker kubelet keepalived haproxy etcd kube-apiserver kube-controller-manager kube-scheduler kube-proxy calico-node</p>
<h2 id="2-Kubernetes-Node"><a href="#2-Kubernetes-Node" class="headerlink" title="2. Kubernetes Node"></a>2. Kubernetes Node</h2><p>172.16.170.134 server07 -&gt; docker kubelet kube-proxy calico-node<br>172.16.170.135 server08 -&gt; docker kubelet kube-proxy calico-node</p>
<h1 id="四、实现过程记录"><a href="#四、实现过程记录" class="headerlink" title="四、实现过程记录"></a>四、实现过程记录</h1><h2 id="1-在Kubernetes-Control-Plane上的所有Node上部署HAProxy做为负载均衡器（由Kubelet管理以静态Pod的方式实现）"><a href="#1-在Kubernetes-Control-Plane上的所有Node上部署HAProxy做为负载均衡器（由Kubelet管理以静态Pod的方式实现）" class="headerlink" title="1. 在Kubernetes Control Plane上的所有Node上部署HAProxy做为负载均衡器（由Kubelet管理以静态Pod的方式实现）"></a>1. 在Kubernetes Control Plane上的所有Node上部署HAProxy做为负载均衡器（由Kubelet管理以静态Pod的方式实现）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /etc/haproxy/</span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">  log 127.0.0.1 local0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  uid 99</span><br><span class="line">  gid 99</span><br><span class="line">global</span><br><span class="line">  log 127.0.0.1 local0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  uid 99</span><br><span class="line">  gid 99</span><br><span class="line">  #daemon</span><br><span class="line">  nbproc 1</span><br><span class="line">  pidfile haproxy.pid</span><br><span class="line">defaults</span><br><span class="line">  mode http</span><br><span class="line">  log 127.0.0.1 local0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  retries 3</span><br><span class="line">  timeout connect 5s</span><br><span class="line">  timeout client 30s</span><br><span class="line">  timeout server 30s</span><br><span class="line">  timeout check 2s</span><br><span class="line"></span><br><span class="line">listen stats</span><br><span class="line">  mode http</span><br><span class="line">  bind 0.0.0.0:9090</span><br><span class="line">  log 127.0.0.1 local0 err</span><br><span class="line">  stats refresh 30s</span><br><span class="line">  stats uri     /haproxy-status</span><br><span class="line">  stats realm   Haproxy\ Statistics</span><br><span class="line">  stats auth    admin:12345678</span><br><span class="line">  stats hide-version</span><br><span class="line">  stats admin if TRUE</span><br><span class="line"></span><br><span class="line">frontend kube-apiserver-https</span><br><span class="line">   mode tcp</span><br><span class="line">   bind :8443</span><br><span class="line">   default_backend kube-apiserver-backend</span><br><span class="line"></span><br><span class="line">backend kube-apiserver-backend</span><br><span class="line">    mode tcp</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server server01 172.16.170.128:6443 weight 3 minconn 100 maxconn 50000 check inter 5000 rise 2 fall 5</span><br><span class="line">    server server02 172.16.170.129:6443 weight 3 minconn 100 maxconn 50000 check inter 5000 rise 2 fall 5</span><br><span class="line">    server server03 172.16.170.130:6443 weight 3 minconn 100 maxconn 50000 check inter 5000 rise 2 fall 5</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/kubernetes/manifests/haproxy.yaml</span><br><span class="line">kind: Pod</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    scheduler.alpha.kubernetes.io/critical-pod: &quot;&quot;</span><br><span class="line">  labels:</span><br><span class="line">    component: haproxy</span><br><span class="line">    tier: control-plane</span><br><span class="line">  name: kube-haproxy</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  hostNetwork: true</span><br><span class="line">  containers:</span><br><span class="line">  - name: kube-haproxy</span><br><span class="line">    image: haproxy:1.7-alpine</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        cpu: 100m</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: haproxy-cfg</span><br><span class="line">      readOnly: true</span><br><span class="line">      mountPath: /usr/local/etc/haproxy/haproxy.cfg</span><br><span class="line">  volumes:</span><br><span class="line">  - name: haproxy-cfg</span><br><span class="line">    hostPath:</span><br><span class="line">      path: /etc/haproxy/haproxy.cfg</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="2-在Kubernetes-Control-Plane的所有Node上部署Keepalived（由Kubelet管理以静态Pod的方式实现）"><a href="#2-在Kubernetes-Control-Plane的所有Node上部署Keepalived（由Kubelet管理以静态Pod的方式实现）" class="headerlink" title="2. 在Kubernetes Control Plane的所有Node上部署Keepalived（由Kubelet管理以静态Pod的方式实现）"></a>2. 在Kubernetes Control Plane的所有Node上部署Keepalived（由Kubelet管理以静态Pod的方式实现）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; /etc/kubernetes/manifests/keepalived.yaml</span><br><span class="line">kind: Pod</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    scheduler.alpha.kubernetes.io/critical-pod: &quot;&quot;</span><br><span class="line">  labels:</span><br><span class="line">    component: keepalived</span><br><span class="line">    tier: control-plane</span><br><span class="line">  name: kube-keepalived</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  hostNetwork: true</span><br><span class="line">  containers:</span><br><span class="line">  - name: kube-keepalived</span><br><span class="line">    image: osixia/keepalived:1.4.5</span><br><span class="line">    env:</span><br><span class="line">    - name: KEEPALIVED_VIRTUAL_IPS</span><br><span class="line">      value: 172.16.170.151</span><br><span class="line">    - name: KEEPALIVED_INTERFACE</span><br><span class="line">      value: ens33</span><br><span class="line">    - name: KEEPALIVED_UNICAST_PEERS</span><br><span class="line">      value: &quot;#PYTHON2BASH:[&apos;172.16.170.128&apos;, &apos;172.16.170.129&apos;, &apos;172.16.170.130&apos;]&quot;</span><br><span class="line">    - name: KEEPALIVED_PASSWORD</span><br><span class="line">      value: docker</span><br><span class="line">    - name: KEEPALIVED_PRIORITY</span><br><span class="line">      value: &quot;100&quot;</span><br><span class="line">    - name: KEEPALIVED_ROUTER_ID</span><br><span class="line">      value: &quot;51&quot;</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        cpu: 100m</span><br><span class="line">    securityContext:</span><br><span class="line">      privileged: true</span><br><span class="line">      capabilities:</span><br><span class="line">        add:</span><br><span class="line">        - NET_ADMIN</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="3-在Kubernetes-Control-Plane-的第一个Node（server01）上操作："><a href="#3-在Kubernetes-Control-Plane-的第一个Node（server01）上操作：" class="headerlink" title="3. 在Kubernetes Control Plane 的第一个Node（server01）上操作："></a>3. 在Kubernetes Control Plane 的第一个Node（server01）上操作：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"># 生成Kubeadm初始化需要使用的配置文件</span><br><span class="line">mkdir -p kubeadm/config/</span><br><span class="line">cat &lt;&lt;EOF &gt; kubeadm/config/kubeadm-config.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.0</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- &quot;172.16.170.151&quot;</span><br><span class="line">api:</span><br><span class="line">  controlPlaneEndpoint: &quot;172.16.170.151:8443&quot;</span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: &quot;https://127.0.0.1:2379,https://172.16.170.128:2379&quot;</span><br><span class="line">      advertise-client-urls: &quot;https://172.16.170.128:2379&quot;</span><br><span class="line">      listen-peer-urls: &quot;https://172.16.170.128:2380&quot;</span><br><span class="line">      initial-advertise-peer-urls: &quot;https://172.16.170.128:2380&quot;</span><br><span class="line">      initial-cluster: &quot;server01=https://172.16.170.128:2380&quot;</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - server01</span><br><span class="line">      - 172.16.170.128</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - server01</span><br><span class="line">      - 172.16.170.128</span><br><span class="line">controllerManagerExtraArgs:</span><br><span class="line">  node-monitor-grace-period: 10s</span><br><span class="line">  pod-eviction-timeout: 10s</span><br><span class="line"></span><br><span class="line">networking:</span><br><span class="line">  podSubnet: 10.211.0.0/16</span><br><span class="line">  serviceSubnet: 10.96.0.0/16</span><br><span class="line"></span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 拉取Kubeadm初始化需要使用的docker镜像</span><br><span class="line">kubeadm config images pull --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line"></span><br><span class="line"># 执行Kubeadm的初始化操作（注意记录输出的Node加入命令）</span><br><span class="line">kubeadm init --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line"></span><br><span class="line"># 配置当前节点上的Kubectl访问权限</span><br><span class="line">rm -rf $HOME/.kube</span><br><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line"># 保存输出中类似于下面的命令，供添加节点功能使用</span><br><span class="line">kubeadm join 172.16.170.151:8443 --token lt0o7j.ayxwcqr8v88spzjj --discovery-token-ca-cert-hash sha256:1ad613cf114281af6eca0afeebae7185ed69218ff92b73ebe248b90cc74353a3</span><br><span class="line"></span><br><span class="line"># 配置server01到server02和server03的ssh免密登录</span><br><span class="line">ssh-keygen</span><br><span class="line">ssh-copy-id -i .ssh/id_rsa.pub root@server02</span><br><span class="line">ssh-copy-id -i .ssh/id_rsa.pub root@server03</span><br><span class="line"></span><br><span class="line"># 验证server01到server02和server03的ssh免密登录</span><br><span class="line">ssh server02</span><br><span class="line">ssh server03</span><br><span class="line"></span><br><span class="line"># 分发pki证书和admin.conf文件</span><br><span class="line">ssh server02 &apos;mkdir -p /etc/kubernetes/pki/etcd/&apos;</span><br><span class="line">ssh server03 &apos;mkdir -p /etc/kubernetes/pki/etcd/&apos;</span><br><span class="line">cat &lt;&lt;EOF &gt; kubeadm/config/scp-config.sh</span><br><span class="line">USER=root</span><br><span class="line">CONTROL_PLANE_IPS=&quot;172.16.170.129 172.16.170.130&quot;</span><br><span class="line">for host in \$&#123;CONTROL_PLANE_IPS&#125;; do</span><br><span class="line">    scp /etc/kubernetes/pki/ca.crt \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/</span><br><span class="line">    scp /etc/kubernetes/pki/ca.key \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/</span><br><span class="line">    scp /etc/kubernetes/pki/sa.key \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/</span><br><span class="line">    scp /etc/kubernetes/pki/sa.pub \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/</span><br><span class="line">    scp /etc/kubernetes/pki/front-proxy-ca.crt \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/</span><br><span class="line">    scp /etc/kubernetes/pki/front-proxy-ca.key \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/</span><br><span class="line">    scp /etc/kubernetes/pki/etcd/ca.crt \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/etcd/</span><br><span class="line">    scp /etc/kubernetes/pki/etcd/ca.key \$&#123;USER&#125;@\$host:/etc/kubernetes/pki/etcd/</span><br><span class="line">    scp /etc/kubernetes/admin.conf \$&#123;USER&#125;@\$host:/etc/kubernetes/</span><br><span class="line">done</span><br><span class="line">EOF</span><br><span class="line">chmod 0755 kubeadm/config/scp-config.sh</span><br><span class="line">./kubeadm/config/scp-config.sh</span><br></pre></td></tr></table></figure>
<h2 id="4-在Kubernetes-Control-Plane-的第二个Node（server02）上操作："><a href="#4-在Kubernetes-Control-Plane-的第二个Node（server02）上操作：" class="headerlink" title="4. 在Kubernetes Control Plane 的第二个Node（server02）上操作："></a>4. 在Kubernetes Control Plane 的第二个Node（server02）上操作：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"># 生成Kubeadm初始化需要使用的配置文件</span><br><span class="line">mkdir -p kubeadm/config/</span><br><span class="line">cat &lt;&lt;EOF &gt; kubeadm/config/kubeadm-config.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.0</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- &quot;172.16.170.151&quot;</span><br><span class="line">api:</span><br><span class="line">  controlPlaneEndpoint: &quot;172.16.170.151:8443&quot;</span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: &quot;https://127.0.0.1:2379,https://172.16.170.129:2379&quot;</span><br><span class="line">      advertise-client-urls: &quot;https://172.16.170.129:2379&quot;</span><br><span class="line">      listen-peer-urls: &quot;https://172.16.170.129:2380&quot;</span><br><span class="line">      initial-advertise-peer-urls: &quot;https://172.16.170.129:2380&quot;</span><br><span class="line">      initial-cluster: &quot;server01=https://172.16.170.128:2380,server02=https://172.16.170.129:2380&quot;</span><br><span class="line">      initial-cluster-state: existing</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - server02</span><br><span class="line">      - 172.16.170.129</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - server02</span><br><span class="line">      - 172.16.170.129</span><br><span class="line">controllerManagerExtraArgs:</span><br><span class="line">  node-monitor-grace-period: 10s</span><br><span class="line">  pod-eviction-timeout: 10s</span><br><span class="line"></span><br><span class="line">networking:</span><br><span class="line">  podSubnet: 10.211.0.0/16</span><br><span class="line">  serviceSubnet: 10.96.0.0/16</span><br><span class="line"></span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 拉取Kubeadm初始化需要使用的docker镜像</span><br><span class="line">kubeadm config images pull --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line"></span><br><span class="line"># 通过Kubeadm phase来启动server02上的Kubelet</span><br><span class="line">kubeadm alpha phase certs all --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">systemctl restart kubelet.service</span><br><span class="line">systemctl status kubelet.service</span><br><span class="line"></span><br><span class="line"># 添加当前Node上的etcd节点到etcd集群中</span><br><span class="line">CP0_IP=172.16.170.128</span><br><span class="line">CP0_HOSTNAME=server01</span><br><span class="line">CP1_IP=172.16.170.129</span><br><span class="line">CP1_HOSTNAME=server02</span><br><span class="line">KUBECONFIG=/etc/kubernetes/admin.conf kubectl exec -n kube-system etcd-$&#123;CP0_HOSTNAME&#125; -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://$&#123;CP0_IP&#125;:2379 member add $&#123;CP1_HOSTNAME&#125; https://$&#123;CP1_IP&#125;:2380</span><br><span class="line">kubeadm alpha phase etcd local --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line"></span><br><span class="line"># 部署Kubernetes Control Plane的相关组件，并且标记当前Node为Master</span><br><span class="line">kubeadm alpha phase kubeconfig all --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config kubeadm/config/kubeadm-config.yaml</span><br></pre></td></tr></table></figure>
<h2 id="4-在Kubernetes-Control-Plane-的第三个Node（server03）上操作："><a href="#4-在Kubernetes-Control-Plane-的第三个Node（server03）上操作：" class="headerlink" title="4. 在Kubernetes Control Plane 的第三个Node（server03）上操作："></a>4. 在Kubernetes Control Plane 的第三个Node（server03）上操作：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"># 生成Kubeadm初始化需要使用的配置文件</span><br><span class="line">mkdir -p kubeadm/config/</span><br><span class="line">cat &lt;&lt;EOF &gt; kubeadm/config/kubeadm-config.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.0</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- &quot;172.16.170.151&quot;</span><br><span class="line">api:</span><br><span class="line">  controlPlaneEndpoint: &quot;172.16.170.151:8443&quot;</span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: &quot;https://127.0.0.1:2379,https://172.16.170.130:2379&quot;</span><br><span class="line">      advertise-client-urls: &quot;https://172.16.170.130:2379&quot;</span><br><span class="line">      listen-peer-urls: &quot;https://172.16.170.130:2380&quot;</span><br><span class="line">      initial-advertise-peer-urls: &quot;https://172.16.170.130:2380&quot;</span><br><span class="line">      initial-cluster: &quot;server01=https://172.16.170.128:2380,server02=https://172.16.170.129:2380,server03=https://172.16.170.130:2380&quot;</span><br><span class="line">      initial-cluster-state: existing</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - server03</span><br><span class="line">      - 172.16.170.130</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - server03</span><br><span class="line">      - 172.16.170.130</span><br><span class="line">controllerManagerExtraArgs:</span><br><span class="line">  node-monitor-grace-period: 10s</span><br><span class="line">  pod-eviction-timeout: 10s</span><br><span class="line"></span><br><span class="line">networking:</span><br><span class="line">  podSubnet: 10.211.0.0/16</span><br><span class="line">  serviceSubnet: 10.96.0.0/16</span><br><span class="line"></span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 拉取Kubeadm初始化需要使用的docker镜像</span><br><span class="line">kubeadm config images pull --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line"></span><br><span class="line"># 通过Kubeadm phase来启动server03上的Kubelet</span><br><span class="line">kubeadm alpha phase certs all --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">systemctl restart kubelet.service</span><br><span class="line">systemctl status kubelet.service</span><br><span class="line"></span><br><span class="line"># 添加当前Node上的etcd节点到etcd集群中</span><br><span class="line">CP0_IP=172.16.170.128</span><br><span class="line">CP0_HOSTNAME=server01</span><br><span class="line">CP2_IP=172.16.170.130</span><br><span class="line">CP2_HOSTNAME=server03</span><br><span class="line">KUBECONFIG=/etc/kubernetes/admin.conf kubectl exec -n kube-system etcd-$&#123;CP0_HOSTNAME&#125; -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://$&#123;CP0_IP&#125;:2379 member add $&#123;CP2_HOSTNAME&#125; https://$&#123;CP2_IP&#125;:2380</span><br><span class="line">kubeadm alpha phase etcd local --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line"></span><br><span class="line"># 部署Kubernetes Control Plane的相关组件，并且标记当前Node为Master</span><br><span class="line">kubeadm alpha phase kubeconfig all --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config kubeadm/config/kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config kubeadm/config/kubeadm-config.yaml</span><br></pre></td></tr></table></figure>
<h2 id="5-Kubernetes-Control-Plane的另外两个节点分别配置Kubectl访问权限"><a href="#5-Kubernetes-Control-Plane的另外两个节点分别配置Kubectl访问权限" class="headerlink" title="5. Kubernetes Control Plane的另外两个节点分别配置Kubectl访问权限"></a>5. Kubernetes Control Plane的另外两个节点分别配置Kubectl访问权限</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rm -rf $HOME/.kube</span><br><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">chown $(id -u):$(id -g) $HOME/.kube/config</span><br></pre></td></tr></table></figure>
<h2 id="6-高可用部署的Stack结构验证"><a href="#6-高可用部署的Stack结构验证" class="headerlink" title="6. 高可用部署的Stack结构验证"></a>6. 高可用部署的Stack结构验证</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get pod --all-namespaces -o wide</span><br><span class="line">NAMESPACE     NAME                               READY     STATUS    RESTARTS   AGE       IP               NODE</span><br><span class="line">kube-system   calico-node-ff5cv                  2/2       Running   0          47s       172.16.170.130   server03</span><br><span class="line">kube-system   calico-node-hb782                  2/2       Running   0          8m        172.16.170.128   server01</span><br><span class="line">kube-system   calico-node-zpwcp                  2/2       Running   0          4m        172.16.170.129   server02</span><br><span class="line">kube-system   coredns-777d78ff6f-5n8bg           1/1       Running   0          10m       10.211.0.4       server01</span><br><span class="line">kube-system   coredns-777d78ff6f-wfm7d           1/1       Running   0          10m       10.211.0.5       server01</span><br><span class="line">kube-system   etcd-server01                      1/1       Running   0          9m        172.16.170.128   server01</span><br><span class="line">kube-system   etcd-server02                      1/1       Running   0          3m        172.16.170.129   server02</span><br><span class="line">kube-system   etcd-server03                      1/1       Running   0          27s       172.16.170.130   server03</span><br><span class="line">kube-system   kube-apiserver-server01            1/1       Running   0          9m        172.16.170.128   server01</span><br><span class="line">kube-system   kube-apiserver-server02            1/1       Running   0          2m        172.16.170.129   server02</span><br><span class="line">kube-system   kube-apiserver-server03            1/1       Running   0          16s       172.16.170.130   server03</span><br><span class="line">kube-system   kube-controller-manager-server01   1/1       Running   0          9m        172.16.170.128   server01</span><br><span class="line">kube-system   kube-controller-manager-server02   1/1       Running   0          2m        172.16.170.129   server02</span><br><span class="line">kube-system   kube-controller-manager-server03   1/1       Running   0          16s       172.16.170.130   server03</span><br><span class="line">kube-system   kube-haproxy-server01              1/1       Running   0          9m        172.16.170.128   server01</span><br><span class="line">kube-system   kube-haproxy-server02              1/1       Running   0          4m        172.16.170.129   server02</span><br><span class="line">kube-system   kube-haproxy-server03              1/1       Running   0          27s       172.16.170.130   server03</span><br><span class="line">kube-system   kube-keepalived-server01           1/1       Running   0          9m        172.16.170.128   server01</span><br><span class="line">kube-system   kube-keepalived-server02           1/1       Running   0          4m        172.16.170.129   server02</span><br><span class="line">kube-system   kube-keepalived-server03           1/1       Running   0          27s       172.16.170.130   server03</span><br><span class="line">kube-system   kube-proxy-88b55                   1/1       Running   0          4m        172.16.170.129   server02</span><br><span class="line">kube-system   kube-proxy-9n9vv                   1/1       Running   0          9m        172.16.170.128   server01</span><br><span class="line">kube-system   kube-proxy-j7lqz                   1/1       Running   0          47s       172.16.170.130   server03</span><br><span class="line">kube-system   kube-scheduler-server01            1/1       Running   0          9m        172.16.170.128   server01</span><br><span class="line">kube-system   kube-scheduler-server02            1/1       Running   0          2m        172.16.170.129   server02</span><br><span class="line">kube-system   kube-scheduler-server03            1/1       Running   0          16s       172.16.170.130   server03</span><br><span class="line"></span><br><span class="line"># kubectl get node -o wide</span><br><span class="line">NAME       STATUS    ROLES     AGE       VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME</span><br><span class="line">server01   Ready     master    10m       v1.11.0   172.16.170.128   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-862.11.6.el7.x86_64   docker://17.3.1</span><br><span class="line">server02   Ready     master    4m        v1.11.0   172.16.170.129   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-862.11.6.el7.x86_64   docker://17.3.1</span><br><span class="line">server03   Ready     master    1m        v1.11.0   172.16.170.130   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-862.11.6.el7.x86_64   docker://17.3.1</span><br></pre></td></tr></table></figure>
<h2 id="7-为高可用集群添加两个Node"><a href="#7-为高可用集群添加两个Node" class="headerlink" title="7. 为高可用集群添加两个Node"></a>7. 为高可用集群添加两个Node</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"># 在server07上执行添加Node的命令</span><br><span class="line"># kubeadm join 172.16.170.151:8443 --token lt0o7j.ayxwcqr8v88spzjj --discovery-token-ca-cert-hash sha256:1ad613cf114281af6eca0afeebae7185ed69218ff92b73ebe248b90cc74353a3</span><br><span class="line">[preflight] running pre-flight checks</span><br><span class="line">	[WARNING RequiredIPVSKernelModulesAvailable]: the IPVS proxier will not be used, because the following required kernel modules are not loaded: [ip_vs_wrr ip_vs_sh ip_vs ip_vs_rr] or no builtin kernel ipvs support: map[ip_vs_sh:&#123;&#125; nf_conntrack_ipv4:&#123;&#125; ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_wrr:&#123;&#125;]</span><br><span class="line">you can solve this problem with following methods:</span><br><span class="line">1. Run &apos;modprobe -- &apos; to load missing kernel modules;</span><br><span class="line">2. Provide the missing builtin kernel ipvs support</span><br><span class="line"></span><br><span class="line">I0123 16:10:01.668746   17689 kernel_validator.go:81] Validating kernel version</span><br><span class="line">I0123 16:10:01.668820   17689 kernel_validator.go:96] Validating kernel config</span><br><span class="line">[discovery] Trying to connect to API Server &quot;172.16.170.151:8443&quot;</span><br><span class="line">[discovery] Created cluster-info discovery client, requesting info from &quot;https://172.16.170.151:8443&quot;</span><br><span class="line">[discovery] Requesting info from &quot;https://172.16.170.151:8443&quot; again to validate TLS against the pinned public key</span><br><span class="line">[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &quot;172.16.170.151:8443&quot;</span><br><span class="line">[discovery] Successfully established connection with API Server &quot;172.16.170.151:8443&quot;</span><br><span class="line">[kubelet] Downloading configuration for the kubelet from the &quot;kubelet-config-1.11&quot; ConfigMap in the kube-system namespace</span><br><span class="line">[kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">[kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class="line">[preflight] Activating the kubelet service</span><br><span class="line">[tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap...</span><br><span class="line">[patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;server07&quot; as an annotation</span><br><span class="line"></span><br><span class="line">This node has joined the cluster:</span><br><span class="line">* Certificate signing request was sent to master and a response</span><br><span class="line">  was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line"></span><br><span class="line">Run &apos;kubectl get nodes&apos; on the master to see this node join the cluster.</span><br><span class="line"></span><br><span class="line"># 在server08上执行添加Node的命令</span><br><span class="line"># kubeadm join 172.16.170.151:8443 --token lt0o7j.ayxwcqr8v88spzjj --discovery-token-ca-cert-hash sha256:1ad613cf114281af6eca0afeebae7185ed69218ff92b73ebe248b90cc74353a3</span><br><span class="line">[preflight] running pre-flight checks</span><br><span class="line">	[WARNING RequiredIPVSKernelModulesAvailable]: the IPVS proxier will not be used, because the following required kernel modules are not loaded: [ip_vs_rr ip_vs_wrr ip_vs_sh ip_vs] or no builtin kernel ipvs support: map[ip_vs_sh:&#123;&#125; nf_conntrack_ipv4:&#123;&#125; ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_wrr:&#123;&#125;]</span><br><span class="line">you can solve this problem with following methods:</span><br><span class="line">1. Run &apos;modprobe -- &apos; to load missing kernel modules;</span><br><span class="line">2. Provide the missing builtin kernel ipvs support</span><br><span class="line"></span><br><span class="line">I0123 16:10:29.832899   17706 kernel_validator.go:81] Validating kernel version</span><br><span class="line">I0123 16:10:29.833038   17706 kernel_validator.go:96] Validating kernel config</span><br><span class="line">[discovery] Trying to connect to API Server &quot;172.16.170.151:8443&quot;</span><br><span class="line">[discovery] Created cluster-info discovery client, requesting info from &quot;https://172.16.170.151:8443&quot;</span><br><span class="line">[discovery] Requesting info from &quot;https://172.16.170.151:8443&quot; again to validate TLS against the pinned public key</span><br><span class="line">[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &quot;172.16.170.151:8443&quot;</span><br><span class="line">[discovery] Successfully established connection with API Server &quot;172.16.170.151:8443&quot;</span><br><span class="line">[kubelet] Downloading configuration for the kubelet from the &quot;kubelet-config-1.11&quot; ConfigMap in the kube-system namespace</span><br><span class="line">[kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">[kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class="line">[preflight] Activating the kubelet service</span><br><span class="line">[tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap...</span><br><span class="line">[patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;server08&quot; as an annotation</span><br><span class="line"></span><br><span class="line">This node has joined the cluster:</span><br><span class="line">* Certificate signing request was sent to master and a response</span><br><span class="line">  was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line"></span><br><span class="line">Run &apos;kubectl get nodes&apos; on the master to see this node join the cluster.</span><br><span class="line"></span><br><span class="line"># 在任意一个Master节点上执行</span><br><span class="line"># kubectl get node -o wide</span><br><span class="line">NAME       STATUS    ROLES     AGE       VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME</span><br><span class="line">server01   Ready     master    16m       v1.11.0   172.16.170.128   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-862.11.6.el7.x86_64   docker://17.3.1</span><br><span class="line">server02   Ready     master    10m       v1.11.0   172.16.170.129   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-862.11.6.el7.x86_64   docker://17.3.1</span><br><span class="line">server03   Ready     master    7m        v1.11.0   172.16.170.130   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-862.11.6.el7.x86_64   docker://17.3.1</span><br><span class="line">server07   Ready     &lt;none&gt;    40s       v1.11.0   172.16.170.134   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-957.1.3.el7.x86_64    docker://17.3.1</span><br><span class="line">server08   Ready     &lt;none&gt;    12s       v1.11.0   172.16.170.135   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-957.1.3.el7.x86_64    docker://17.3.1</span><br><span class="line"></span><br><span class="line"># 在任意一个Master节点上执行</span><br><span class="line"># kubectl get pod --all-namespaces -o wide</span><br><span class="line">NAMESPACE     NAME                               READY     STATUS    RESTARTS   AGE       IP               NODE</span><br><span class="line">kube-system   calico-node-c8j7r                  2/2       Running   0          1m        172.16.170.134   server07</span><br><span class="line">kube-system   calico-node-chngv                  1/2       Running   0          32s       172.16.170.135   server08</span><br><span class="line">kube-system   calico-node-ff5cv                  2/2       Running   0          7m        172.16.170.130   server03</span><br><span class="line">kube-system   calico-node-hb782                  2/2       Running   0          15m       172.16.170.128   server01</span><br><span class="line">kube-system   calico-node-zpwcp                  2/2       Running   0          11m       172.16.170.129   server02</span><br><span class="line">kube-system   coredns-777d78ff6f-5n8bg           1/1       Running   0          16m       10.211.0.4       server01</span><br><span class="line">kube-system   coredns-777d78ff6f-wfm7d           1/1       Running   0          16m       10.211.0.5       server01</span><br><span class="line">kube-system   etcd-server01                      1/1       Running   0          16m       172.16.170.128   server01</span><br><span class="line">kube-system   etcd-server02                      1/1       Running   0          10m       172.16.170.129   server02</span><br><span class="line">kube-system   etcd-server03                      1/1       Running   0          7m        172.16.170.130   server03</span><br><span class="line">kube-system   kube-apiserver-server01            1/1       Running   0          16m       172.16.170.128   server01</span><br><span class="line">kube-system   kube-apiserver-server02            1/1       Running   0          9m        172.16.170.129   server02</span><br><span class="line">kube-system   kube-apiserver-server03            1/1       Running   0          7m        172.16.170.130   server03</span><br><span class="line">kube-system   kube-controller-manager-server01   1/1       Running   0          16m       172.16.170.128   server01</span><br><span class="line">kube-system   kube-controller-manager-server02   1/1       Running   0          9m        172.16.170.129   server02</span><br><span class="line">kube-system   kube-controller-manager-server03   1/1       Running   0          7m        172.16.170.130   server03</span><br><span class="line">kube-system   kube-haproxy-server01              1/1       Running   0          16m       172.16.170.128   server01</span><br><span class="line">kube-system   kube-haproxy-server02              1/1       Running   0          10m       172.16.170.129   server02</span><br><span class="line">kube-system   kube-haproxy-server03              1/1       Running   0          7m        172.16.170.130   server03</span><br><span class="line">kube-system   kube-keepalived-server01           1/1       Running   0          16m       172.16.170.128   server01</span><br><span class="line">kube-system   kube-keepalived-server02           1/1       Running   0          10m       172.16.170.129   server02</span><br><span class="line">kube-system   kube-keepalived-server03           1/1       Running   0          7m        172.16.170.130   server03</span><br><span class="line">kube-system   kube-proxy-88b55                   1/1       Running   0          11m       172.16.170.129   server02</span><br><span class="line">kube-system   kube-proxy-9n9vv                   1/1       Running   0          16m       172.16.170.128   server01</span><br><span class="line">kube-system   kube-proxy-g8lsj                   1/1       Running   0          1m        172.16.170.134   server07</span><br><span class="line">kube-system   kube-proxy-j7lqz                   1/1       Running   0          7m        172.16.170.130   server03</span><br><span class="line">kube-system   kube-proxy-qdhpj                   1/1       Running   0          32s       172.16.170.135   server08</span><br><span class="line">kube-system   kube-scheduler-server01            1/1       Running   0          16m       172.16.170.128   server01</span><br><span class="line">kube-system   kube-scheduler-server02            1/1       Running   0          9m        172.16.170.129   server02</span><br><span class="line">kube-system   kube-scheduler-server03            1/1       Running   0          7m        172.16.170.130   server03</span><br></pre></td></tr></table></figure>
<h1 id="五、参考资料"><a href="#五、参考资料" class="headerlink" title="五、参考资料"></a>五、参考资料</h1><p><a href="https://v1-11.docs.kubernetes.io/docs/setup/independent/high-availability/" target="_blank" rel="noopener">https://v1-11.docs.kubernetes.io/docs/setup/independent/high-availability/</a><br><a href="https://my.oschina.net/u/3433152/blog/1935402" target="_blank" rel="noopener">https://my.oschina.net/u/3433152/blog/1935402</a><br><a href="https://www.jianshu.com/p/49a48752c1a3?utm_source=oschina-app" target="_blank" rel="noopener">https://www.jianshu.com/p/49a48752c1a3?utm_source=oschina-app</a><br><a href="https://tonybai.com/2017/05/15/setup-a-ha-kubernetes-cluster-based-on-kubeadm-part1/" target="_blank" rel="noopener">https://tonybai.com/2017/05/15/setup-a-ha-kubernetes-cluster-based-on-kubeadm-part1/</a><br><a href="https://tonybai.com/2017/05/15/setup-a-ha-kubernetes-cluster-based-on-kubeadm-part2/" target="_blank" rel="noopener">https://tonybai.com/2017/05/15/setup-a-ha-kubernetes-cluster-based-on-kubeadm-part2/</a><br><a href="https://blog.csdn.net/liu_qingbo/article/details/78383892" target="_blank" rel="noopener">https://blog.csdn.net/liu_qingbo/article/details/78383892</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Highly-Available/">Highly Available</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubeadm/">Kubeadm</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Setup/">Setup</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kubeadm_node_param" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/01/23/kubeadm_node_param/" class="article-date">
  	<time datetime="2019-01-23T05:53:01.000Z" itemprop="datePublished">2019-01-23</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/23/kubeadm_node_param/">
        Kubeadm安装Kubernetes之宿主机的相关参数修改
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Kubeadm安装Kubernetes之宿主机的相关参数修改"><a href="#Kubeadm安装Kubernetes之宿主机的相关参数修改" class="headerlink" title="Kubeadm安装Kubernetes之宿主机的相关参数修改"></a>Kubeadm安装Kubernetes之宿主机的相关参数修改</h1><h2 id="1-ulimit-n-参数修改永久生效"><a href="#1-ulimit-n-参数修改永久生效" class="headerlink" title="1. ulimit -n 参数修改永久生效"></a>1. ulimit -n 参数修改永久生效</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt;  /etc/security/limits.d/k8s.conf</span><br><span class="line">* hard nofile 65536</span><br><span class="line">* soft nofile 65536</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<p>参考资料：<br><a href="https://www.aliyun.com/jiaocheng/208647.html" target="_blank" rel="noopener">https://www.aliyun.com/jiaocheng/208647.html</a><br><a href="https://www.aliyun.com/jiaocheng/124954.html" target="_blank" rel="noopener">https://www.aliyun.com/jiaocheng/124954.html</a><br><a href="https://www.cnblogs.com/xuexiaohun/articles/6233430.html" target="_blank" rel="noopener">https://www.cnblogs.com/xuexiaohun/articles/6233430.html</a><br><a href="https://my.oschina.net/jxcdwangtao/blog/1621106" target="_blank" rel="noopener">https://my.oschina.net/jxcdwangtao/blog/1621106</a></p>
<h2 id="2-通过sysctl修改的相关内核参数："><a href="#2-通过sysctl修改的相关内核参数：" class="headerlink" title="2. 通过sysctl修改的相关内核参数："></a>2. 通过sysctl修改的相关内核参数：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line"></span><br><span class="line">vm.swappiness = 0</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_timestamps = 1</span><br><span class="line">net.ipv4.tcp_tw_recycle = 1 </span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_fin_timeout = 30</span><br><span class="line">net.ipv4.tcp_tw_resue = 1</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sysctl --system</span><br></pre></td></tr></table></figure>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubeadm/">Kubeadm</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Setup/">Setup</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-timezone" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/01/07/timezone/" class="article-date">
  	<time datetime="2019-01-07T01:49:10.000Z" itemprop="datePublished">2019-01-07</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/07/timezone/">
        Docker容器的时区设置
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Docker容器的时区设置，一直以来都是个简单而且重要的问题。下面笔者以CentOS 7.5.1804作为容器宿主机为例，分别介绍如何在原生Docker容器和Kubernetes的Pod中正确设置时区。</p>
<h1 id="Node设置正确时区（以CentOS-7-5-1804-为例）"><a href="#Node设置正确时区（以CentOS-7-5-1804-为例）" class="headerlink" title="Node设置正确时区（以CentOS 7.5.1804 为例）"></a>Node设置正确时区（以CentOS 7.5.1804 为例）</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">timedatectl list-timezones</span><br><span class="line">timedatectl set-timezone Asia/Shanghai</span><br><span class="line">timedatectl status | grep &apos;Time zone&apos;</span><br></pre></td></tr></table></figure>
<p>提示：这里相当于为Docker Container和Kubernetes Pod的宿主机设置正确的时区，需要优先配置完成。</p>
<h1 id="Docker-Container-与-Node同步时区设置"><a href="#Docker-Container-与-Node同步时区设置" class="headerlink" title="Docker Container 与 Node同步时区设置"></a>Docker Container 与 Node同步时区设置</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it -v /etc/localtime:/etc/localtime --rm alpine:3.8 /bin/sh</span><br></pre></td></tr></table></figure>
<p>提示：上述命令注意-v参数的部分。<br>总结：经测试验证，alpine:3.8、centos:7.5.1804和ubuntu:16.04处理方式相同。</p>
<h1 id="Kubernetes-Pod-与-Node同步时区设置"><a href="#Kubernetes-Pod-与-Node同步时区设置" class="headerlink" title="Kubernetes Pod 与 Node同步时区设置"></a>Kubernetes Pod 与 Node同步时区设置</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: alpine</span><br><span class="line">  labels:</span><br><span class="line">    app: alpine</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - image: alpine:3.8</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    name: alpine</span><br><span class="line">    command:</span><br><span class="line">      - sleep</span><br><span class="line">      - &quot;3600&quot;</span><br><span class="line">    volumeMounts:</span><br><span class="line">     - name: tz-config</span><br><span class="line">       mountPath: /etc/localtime</span><br><span class="line">       readOnly: true</span><br><span class="line">  restartPolicy: Always</span><br><span class="line">  volumes:</span><br><span class="line">    - name: tz-config</span><br><span class="line">      hostPath:</span><br><span class="line">        path: /etc/localtime</span><br></pre></td></tr></table></figure>
<p>提示：上述YAML注意volumes和volumeMounts部分。<br>总结：经测试验证，alpine:3.8、centos:7.5.1804和ubuntu:16.04处理方式相同。</p>
<h1 id="Docker-Container-和-Kubernetes-Pod-通用-与-Node同步时区设置"><a href="#Docker-Container-和-Kubernetes-Pod-通用-与-Node同步时区设置" class="headerlink" title="Docker Container 和 Kubernetes Pod 通用 与 Node同步时区设置"></a>Docker Container 和 Kubernetes Pod 通用 与 Node同步时区设置</h1><p>该方式在需要构建Docker Image的时候，即修改Docker Image的默认时区设置。</p>
<h2 id="Alpine-Linux-3-8-Dockerfile-Example"><a href="#Alpine-Linux-3-8-Dockerfile-Example" class="headerlink" title="Alpine Linux 3.8 Dockerfile Example"></a>Alpine Linux 3.8 Dockerfile Example</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">FROM alpine:3.8</span><br><span class="line"></span><br><span class="line">MAINTAINER wangxin_0611@126.com</span><br><span class="line"></span><br><span class="line">RUN apk --no-cache add tzdata  &amp;&amp; \</span><br><span class="line">    ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br></pre></td></tr></table></figure>
<h2 id="Ubuntu-Linux-16-04-Dockerfile-Example"><a href="#Ubuntu-Linux-16-04-Dockerfile-Example" class="headerlink" title="Ubuntu Linux 16.04 Dockerfile Example"></a>Ubuntu Linux 16.04 Dockerfile Example</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">FROM ubuntu:16.04</span><br><span class="line"></span><br><span class="line">MAINTAINER wangxin_0611@126.com</span><br><span class="line"></span><br><span class="line">RUN apt-get update &amp;&amp; \</span><br><span class="line">    apt-get install -y tzdata &amp;&amp; \</span><br><span class="line">    ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br></pre></td></tr></table></figure>
<h2 id="CenOS-Linux-7-5-1804-Dockerfile-Example"><a href="#CenOS-Linux-7-5-1804-Dockerfile-Example" class="headerlink" title="CenOS Linux 7.5.1804 Dockerfile Example"></a>CenOS Linux 7.5.1804 Dockerfile Example</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM centos:7.5.1804</span><br><span class="line"></span><br><span class="line">MAINTAINER wangxin_0611@126.com</span><br><span class="line"></span><br><span class="line">RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br></pre></td></tr></table></figure>
<p>参考资料：<br><a href="https://blog.csdn.net/qq_34924407/article/details/82057080" target="_blank" rel="noopener">https://blog.csdn.net/qq_34924407/article/details/82057080</a><br><a href="https://blog.csdn.net/u013201439/article/details/79436413" target="_blank" rel="noopener">https://blog.csdn.net/u013201439/article/details/79436413</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Logging/">Logging</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/TimeZone/">TimeZone</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/2/">&laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span>
    </nav>
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2019 Singh Wang
      </div>
        <div class="footer-right">
          <a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/smackgg/hexo-theme-smackdown" target="_blank">Smackdown</a>
        </div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="/js/main.js"></script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js"></script>


  </div>
</body>
</html>