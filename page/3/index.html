<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  <title>Singh Wang</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Singh Wang">
<meta property="og:url" content="http://yoursite.com/page/3/index.html">
<meta property="og:site_name" content="Singh Wang">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Singh Wang">
  
    <link rel="alternative" href="/atom.xml" title="Singh Wang" type="application/atom+xml">
  
  
    <link rel="icon" href="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg">
  
  <link rel="stylesheet" href="/css/style.css">
  
  

  <script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>
  <script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>

  
</head></html>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="https://avatars0.githubusercontent.com/u/17465385?s=400&amp;u=cea56ed1710ec6ebab160c1ce1e068317e05b025&amp;v=4" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Singh Wang</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/archives">所有文章</a></li>
				        
							<li><a href="/categories/容器云技术/">容器云技术</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/singhwang" title="github">github</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/Calico/" style="font-size: 11.25px;">Calico</a> <a href="/tags/Ceph/" style="font-size: 12.5px;">Ceph</a> <a href="/tags/Common/" style="font-size: 10px;">Common</a> <a href="/tags/DNS/" style="font-size: 10px;">DNS</a> <a href="/tags/Dashboard/" style="font-size: 11.25px;">Dashboard</a> <a href="/tags/Docker/" style="font-size: 20px;">Docker</a> <a href="/tags/Dragonfly/" style="font-size: 10px;">Dragonfly</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Fluentd/" style="font-size: 11.25px;">Fluentd</a> <a href="/tags/GlusterFS/" style="font-size: 13.75px;">GlusterFS</a> <a href="/tags/HAProxy/" style="font-size: 10px;">HAProxy</a> <a href="/tags/Highly-Available/" style="font-size: 10px;">Highly Available</a> <a href="/tags/Ingress/" style="font-size: 10px;">Ingress</a> <a href="/tags/Kubeadm/" style="font-size: 11.25px;">Kubeadm</a> <a href="/tags/Kubernetes/" style="font-size: 18.75px;">Kubernetes</a> <a href="/tags/Label/" style="font-size: 10px;">Label</a> <a href="/tags/Logging/" style="font-size: 12.5px;">Logging</a> <a href="/tags/Monitoring/" style="font-size: 13.75px;">Monitoring</a> <a href="/tags/Network/" style="font-size: 15px;">Network</a> <a href="/tags/Open-Falcon/" style="font-size: 10px;">Open Falcon</a> <a href="/tags/P2P/" style="font-size: 10px;">P2P</a> <a href="/tags/Prometheus/" style="font-size: 11.25px;">Prometheus</a> <a href="/tags/Setup/" style="font-size: 17.5px;">Setup</a> <a href="/tags/Smartping/" style="font-size: 10px;">Smartping</a> <a href="/tags/Source-Build/" style="font-size: 10px;">Source Build</a> <a href="/tags/Storage/" style="font-size: 16.25px;">Storage</a> <a href="/tags/TimeZone/" style="font-size: 10px;">TimeZone</a> <a href="/tags/Trouble-Shooting/" style="font-size: 17.5px;">Trouble Shooting</a>
					</div>
				</section>
				
				
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">Singh Wang</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="https://avatars0.githubusercontent.com/u/17465385?s=400&amp;u=cea56ed1710ec6ebab160c1ce1e068317e05b025&amp;v=4" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author">Singh Wang</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/archives">所有文章</a></li>
		        
					<li><a href="/categories/容器云技术/">容器云技术</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/singhwang" title="github">github</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap">
  
    <article id="post-kubernetes_calico_000" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/kubernetes_calico_000/" class="article-date">
  	<time datetime="2019-05-27T04:34:27.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/kubernetes_calico_000/">
        Calico的网路通信过程跟踪（IPIP CrossSubnet 模式）
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、环境版本信息"><a href="#一、环境版本信息" class="headerlink" title="一、环境版本信息"></a>一、环境版本信息</h1><p>Kubernetes v1.11.0<br>Calico v3.1.3<br>Calicoctl v3.1.3</p>
<h1 id="二、核心概念解惑"><a href="#二、核心概念解惑" class="headerlink" title="二、核心概念解惑"></a>二、核心概念解惑</h1><h2 id="1-BGP协议是路由器与路由器之间的通信协议，建立在TCP上。路由器之间可以通过BGP协议交换彼此的路由信息。"><a href="#1-BGP协议是路由器与路由器之间的通信协议，建立在TCP上。路由器之间可以通过BGP协议交换彼此的路由信息。" class="headerlink" title="1. BGP协议是路由器与路由器之间的通信协议，建立在TCP上。路由器之间可以通过BGP协议交换彼此的路由信息。"></a>1. BGP协议是路由器与路由器之间的通信协议，建立在TCP上。路由器之间可以通过BGP协议交换彼此的路由信息。</h2><p>A路由器 <----> BGP协议 <----> B路由器 <----> BGP协议 <----> C路由器</----></----></----></----></p>
<h2 id="2-宿主机上运行了很多的Pod，这些Pod的IP地址和通信要怎么处理？"><a href="#2-宿主机上运行了很多的Pod，这些Pod的IP地址和通信要怎么处理？" class="headerlink" title="2. 宿主机上运行了很多的Pod，这些Pod的IP地址和通信要怎么处理？"></a>2. 宿主机上运行了很多的Pod，这些Pod的IP地址和通信要怎么处理？</h2><p>即把宿主机变成一台路由器。宿主机变身路由器后，现实的网络是怎么联通的，Pod之间就怎么联通，技术都是现成的，并且都已经支撑起连接全地球的互联网了。</p>
<h1 id="三、网络通信过程跟踪"><a href="#三、网络通信过程跟踪" class="headerlink" title="三、网络通信过程跟踪"></a>三、网络通信过程跟踪</h1><h2 id="1-以Daemonset形式，在default下部署一组Pod，用于测试Pod的跨主机网络通信："><a href="#1-以Daemonset形式，在default下部署一组Pod，用于测试Pod的跨主机网络通信：" class="headerlink" title="1. 以Daemonset形式，在default下部署一组Pod，用于测试Pod的跨主机网络通信："></a>1. 以Daemonset形式，在default下部署一组Pod，用于测试Pod的跨主机网络通信：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get node -o wide</span><br><span class="line">NAME       STATUS    ROLES     AGE       VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME</span><br><span class="line">server01   Ready     master    128d      v1.11.0   172.16.170.128   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-862.11.6.el7.x86_64   docker://17.3.1</span><br><span class="line">server02   Ready     &lt;none&gt;    128d      v1.11.0   172.16.170.129   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-514.el7.x86_64        docker://17.3.1</span><br><span class="line">server03   Ready     &lt;none&gt;    128d      v1.11.0   172.16.170.130   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-514.el7.x86_64        docker://17.3.1</span><br><span class="line"></span><br><span class="line"># kubectl get pod --all-namespaces -o wide</span><br><span class="line">NAMESPACE     NAME                               READY     STATUS    RESTARTS   AGE       IP               NODE</span><br><span class="line">default       network-5z4qp                      1/1       Running   0          12s       10.211.0.6       server01</span><br><span class="line">default       network-6k87z                      1/1       Running   0          12s       10.211.2.4       server03</span><br><span class="line">default       network-mngxw                      1/1       Running   0          12s       10.211.1.4       server02</span><br><span class="line">kube-system   calico-node-644wq                  2/2       Running   0          128d      172.16.170.128   server01</span><br><span class="line">kube-system   calico-node-hdkf6                  2/2       Running   0          128d      172.16.170.130   server03</span><br><span class="line">kube-system   calico-node-wltgp                  2/2       Running   0          128d      172.16.170.129   server02</span><br><span class="line">kube-system   coredns-777d78ff6f-6sjt5           1/1       Running   0          128d      10.211.0.3       server01</span><br><span class="line">kube-system   coredns-777d78ff6f-mr977           1/1       Running   0          128d      10.211.0.2       server01</span><br><span class="line">kube-system   etcd-server01                      1/1       Running   0          128d      172.16.170.128   server01</span><br><span class="line">kube-system   kube-apiserver-server01            1/1       Running   0          128d      172.16.170.128   server01</span><br><span class="line">kube-system   kube-controller-manager-server01   1/1       Running   0          128d      172.16.170.128   server01</span><br><span class="line">kube-system   kube-proxy-94k52                   1/1       Running   0          128d      172.16.170.129   server02</span><br><span class="line">kube-system   kube-proxy-czg29                   1/1       Running   0          128d      172.16.170.128   server01</span><br><span class="line">kube-system   kube-proxy-mnhrb                   1/1       Running   0          128d      172.16.170.130   server03</span><br><span class="line">kube-system   kube-scheduler-server01            1/1       Running   0          128d      172.16.170.128   server01</span><br></pre></td></tr></table></figure>
<h2 id="2-选取master节点的calico-node，进入安装calicoctl命令行工具："><a href="#2-选取master节点的calico-node，进入安装calicoctl命令行工具：" class="headerlink" title="2. 选取master节点的calico-node，进入安装calicoctl命令行工具："></a>2. 选取master节点的calico-node，进入安装calicoctl命令行工具：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.1.3/calicoctl</span><br><span class="line"># kubectl cp calicoctl calico-node-644wq:/usr/local/bin/ -n kube-system -c calico-node</span><br><span class="line"># kubectl exec calico-node-644wq -n kube-system -c calico-node -- chmod 0755 /usr/local/bin/calicoctl</span><br></pre></td></tr></table></figure>
<h2 id="3-进入master节点的calico-node，使用calicoctl查看需要关注的工作负载的相关信息："><a href="#3-进入master节点的calico-node，使用calicoctl查看需要关注的工作负载的相关信息：" class="headerlink" title="3. 进入master节点的calico-node，使用calicoctl查看需要关注的工作负载的相关信息："></a>3. 进入master节点的calico-node，使用calicoctl查看需要关注的工作负载的相关信息：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"># kubectl exec -it calico-node-644wq /bin/sh -n kube-system -c calico-node</span><br><span class="line">/ # calicoctl get workloadendpoint -n default</span><br><span class="line">NAMESPACE   WORKLOAD        NODE       NETWORKS        INTERFACE</span><br><span class="line">default     network-5z4qp   server01   10.211.0.6/32   cali50914021272</span><br><span class="line">default     network-6k87z   server03   10.211.2.4/32   calif17d1193010</span><br><span class="line">default     network-mngxw   server02   10.211.1.4/32   calid406b8b6c93</span><br><span class="line"></span><br><span class="line">/ # calicoctl get workloadendpoint -n default -o yaml</span><br><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">items:</span><br><span class="line">- apiVersion: projectcalico.org/v3</span><br><span class="line">  kind: WorkloadEndpoint</span><br><span class="line">  metadata:</span><br><span class="line">    creationTimestamp: 2019-01-16T06:46:19Z</span><br><span class="line">    labels:</span><br><span class="line">      app: network</span><br><span class="line">      controller-revision-hash: &quot;66086030&quot;</span><br><span class="line">      pod-template-generation: &quot;1&quot;</span><br><span class="line">      projectcalico.org/namespace: default</span><br><span class="line">      projectcalico.org/orchestrator: k8s</span><br><span class="line">    name: server01-k8s-network--5z4qp-eth0</span><br><span class="line">    namespace: default</span><br><span class="line">    resourceVersion: &quot;25227&quot;</span><br><span class="line">    uid: 6df53e81-195a-11e9-b5d1-000c2921eba0</span><br><span class="line">  spec:</span><br><span class="line">    endpoint: eth0</span><br><span class="line">    interfaceName: cali50914021272</span><br><span class="line">    ipNetworks:</span><br><span class="line">    - 10.211.0.6/32</span><br><span class="line">    node: server01</span><br><span class="line">    orchestrator: k8s</span><br><span class="line">    pod: network-5z4qp</span><br><span class="line">    profiles:</span><br><span class="line">    - kns.default</span><br><span class="line">- apiVersion: projectcalico.org/v3</span><br><span class="line">  kind: WorkloadEndpoint</span><br><span class="line">  metadata:</span><br><span class="line">    creationTimestamp: 2019-01-16T06:46:19Z</span><br><span class="line">    labels:</span><br><span class="line">      app: network</span><br><span class="line">      controller-revision-hash: &quot;66086030&quot;</span><br><span class="line">      pod-template-generation: &quot;1&quot;</span><br><span class="line">      projectcalico.org/namespace: default</span><br><span class="line">      projectcalico.org/orchestrator: k8s</span><br><span class="line">    name: server03-k8s-network--6k87z-eth0</span><br><span class="line">    namespace: default</span><br><span class="line">    resourceVersion: &quot;25084&quot;</span><br><span class="line">    uid: 6dfe39ec-195a-11e9-b5d1-000c2921eba0</span><br><span class="line">  spec:</span><br><span class="line">    endpoint: eth0</span><br><span class="line">    interfaceName: calif17d1193010</span><br><span class="line">    ipNetworks:</span><br><span class="line">    - 10.211.2.4/32</span><br><span class="line">    node: server03</span><br><span class="line">    orchestrator: k8s</span><br><span class="line">    pod: network-6k87z</span><br><span class="line">    profiles:</span><br><span class="line">    - kns.default</span><br><span class="line">- apiVersion: projectcalico.org/v3</span><br><span class="line">  kind: WorkloadEndpoint</span><br><span class="line">  metadata:</span><br><span class="line">    creationTimestamp: 2019-01-16T06:46:19Z</span><br><span class="line">    labels:</span><br><span class="line">      app: network</span><br><span class="line">      controller-revision-hash: &quot;66086030&quot;</span><br><span class="line">      pod-template-generation: &quot;1&quot;</span><br><span class="line">      projectcalico.org/namespace: default</span><br><span class="line">      projectcalico.org/orchestrator: k8s</span><br><span class="line">    name: server02-k8s-network--mngxw-eth0</span><br><span class="line">    namespace: default</span><br><span class="line">    resourceVersion: &quot;25082&quot;</span><br><span class="line">    uid: 6e0213f6-195a-11e9-b5d1-000c2921eba0</span><br><span class="line">  spec:</span><br><span class="line">    endpoint: eth0</span><br><span class="line">    interfaceName: calid406b8b6c93</span><br><span class="line">    ipNetworks:</span><br><span class="line">    - 10.211.1.4/32</span><br><span class="line">    node: server02</span><br><span class="line">    orchestrator: k8s</span><br><span class="line">    pod: network-mngxw</span><br><span class="line">    profiles:</span><br><span class="line">    - kns.default</span><br><span class="line">kind: WorkloadEndpointList</span><br><span class="line">metadata: &#123;&#125;</span><br><span class="line"></span><br><span class="line">/ # calicoctl get workloadendpoint server01-k8s-network--5z4qp-eth0 -n default -o yaml</span><br><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">kind: WorkloadEndpoint</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: 2019-01-16T06:46:19Z</span><br><span class="line">  labels:</span><br><span class="line">    app: network</span><br><span class="line">    controller-revision-hash: &quot;66086030&quot;</span><br><span class="line">    pod-template-generation: &quot;1&quot;</span><br><span class="line">    projectcalico.org/namespace: default</span><br><span class="line">    projectcalico.org/orchestrator: k8s</span><br><span class="line">  name: server01-k8s-network--5z4qp-eth0</span><br><span class="line">  namespace: default</span><br><span class="line">  resourceVersion: &quot;25227&quot;</span><br><span class="line">  uid: 6df53e81-195a-11e9-b5d1-000c2921eba0</span><br><span class="line">spec:</span><br><span class="line">  endpoint: eth0</span><br><span class="line">  interfaceName: cali50914021272</span><br><span class="line">  ipNetworks:</span><br><span class="line">  - 10.211.0.6/32</span><br><span class="line">  node: server01</span><br><span class="line">  orchestrator: k8s</span><br><span class="line">  pod: network-5z4qp</span><br><span class="line">  profiles:</span><br><span class="line">  - kns.default</span><br><span class="line"></span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure>
<h2 id="4-选取master节点上的用于测试网络通信的Pod，进入查看它的网络配置信息："><a href="#4-选取master节点上的用于测试网络通信的Pod，进入查看它的网络配置信息：" class="headerlink" title="4. 选取master节点上的用于测试网络通信的Pod，进入查看它的网络配置信息："></a>4. 选取master节点上的用于测试网络通信的Pod，进入查看它的网络配置信息：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"># kubectl exec -it network-5z4qp /bin/sh -n default</span><br><span class="line">/ # ip link</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">2: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">4: eth0@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether 4e:30:25:dc:21:88 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">/ # ip address</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">4: eth0@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether 4e:30:25:dc:21:88 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 10.211.0.6/32 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::4c30:25ff:fedc:2188/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">/ # ip neigh</span><br><span class="line">/ # ping -c 3 10.211.0.1</span><br><span class="line">PING 10.211.0.1 (10.211.0.1) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.0.1: icmp_seq=1 ttl=64 time=0.085 ms</span><br><span class="line">64 bytes from 10.211.0.1: icmp_seq=2 ttl=64 time=0.072 ms</span><br><span class="line">64 bytes from 10.211.0.1: icmp_seq=3 ttl=64 time=0.069 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.0.1 ping statistics ---</span><br><span class="line">3 packets transmitted, 3 received, 0% packet loss, time 2001ms</span><br><span class="line">rtt min/avg/max/mdev = 0.069/0.075/0.085/0.009 ms</span><br><span class="line">/ # ip neigh</span><br><span class="line">169.254.1.1 dev eth0 lladdr ee:ee:ee:ee:ee:ee REACHABLE</span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure>
<h2 id="5-回到master节点上，查看calico的workload的yaml中cali50914021272对应的设备信息和所有设备的地址信息："><a href="#5-回到master节点上，查看calico的workload的yaml中cali50914021272对应的设备信息和所有设备的地址信息：" class="headerlink" title="5. 回到master节点上，查看calico的workload的yaml中cali50914021272对应的设备信息和所有设备的地址信息："></a>5. 回到master节点上，查看calico的workload的yaml中cali50914021272对应的设备信息和所有设备的地址信息：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"># ip link show cali50914021272</span><br><span class="line">11: cali50914021272@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 2</span><br><span class="line"></span><br><span class="line"># ip address</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:21:eb:a0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.16.170.128/24 brd 172.16.170.255 scope global ens33</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::20c:29ff:fe21:eba0/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default</span><br><span class="line">    link/ether 02:42:ca:a4:09:a5 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.17.0.1/16 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::42:caff:fea4:9a5/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">4: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1440 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">    inet 10.211.0.1/32 brd 10.211.0.1 scope global tunl0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">5: cali009d9b46eef@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet6 fe80::ecee:eeff:feee:eeee/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">6: calib99e709bd2c@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br><span class="line">    inet6 fe80::ecee:eeff:feee:eeee/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">11: cali50914021272@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 2</span><br><span class="line">    inet6 fe80::ecee:eeff:feee:eeee/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>
<h2 id="6-验证master上的Pod的网络数据包是否可以发送到master上：（其中10-211-0-1为tunl0设备的地址）"><a href="#6-验证master上的Pod的网络数据包是否可以发送到master上：（其中10-211-0-1为tunl0设备的地址）" class="headerlink" title="6. 验证master上的Pod的网络数据包是否可以发送到master上：（其中10.211.0.1为tunl0设备的地址）"></a>6. 验证master上的Pod的网络数据包是否可以发送到master上：（其中10.211.0.1为tunl0设备的地址）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 开启一个新的终端执行</span><br><span class="line"># tcpdump -i cali50914021272 icmp -v</span><br><span class="line">tcpdump: listening on cali50914021272, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">02:00:24.930193 IP (tos 0x0, ttl 64, id 51910, offset 0, flags [DF], proto ICMP (1), length 84)</span><br><span class="line">    10.211.0.6 &gt; server01: ICMP echo request, id 18, seq 1, length 64</span><br><span class="line">02:00:24.930235 IP (tos 0x0, ttl 64, id 22594, offset 0, flags [none], proto ICMP (1), length 84)</span><br><span class="line">    server01 &gt; 10.211.0.6: ICMP echo reply, id 18, seq 1, length 64</span><br><span class="line"></span><br><span class="line"># 在原有终端上执行</span><br><span class="line"># kubectl exec -it network-5z4qp /bin/sh -n default</span><br><span class="line">/ # ping -c 1 10.211.0.1</span><br><span class="line">PING 10.211.0.1 (10.211.0.1) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.0.1: icmp_seq=1 ttl=64 time=0.081 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.0.1 ping statistics ---</span><br><span class="line">1 packets transmitted, 1 received, 0% packet loss, time 0ms</span><br><span class="line">rtt min/avg/max/mdev = 0.081/0.081/0.081/0.000 ms</span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure>
<h2 id="7-查看master上的路由表，看我们即将测试的目标Pod（地址为10-211-1-4）在master上对应的下一跳在哪里："><a href="#7-查看master上的路由表，看我们即将测试的目标Pod（地址为10-211-1-4）在master上对应的下一跳在哪里：" class="headerlink" title="7. 查看master上的路由表，看我们即将测试的目标Pod（地址为10.211.1.4）在master上对应的下一跳在哪里："></a>7. 查看master上的路由表，看我们即将测试的目标Pod（地址为10.211.1.4）在master上对应的下一跳在哪里：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># ip route</span><br><span class="line">...</span><br><span class="line">blackhole 10.211.0.0/24 proto bird</span><br><span class="line">10.211.0.2 dev cali009d9b46eef scope link</span><br><span class="line">10.211.0.3 dev calib99e709bd2c scope link</span><br><span class="line">10.211.0.6 dev cali50914021272 scope link</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">10.211.1.0/24 via 172.16.170.129 dev ens33 proto bird</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h2 id="8-验证发往目标Pod（地址为10-211-1-4）的网络数据包是否发往master的主机网卡ens33上："><a href="#8-验证发往目标Pod（地址为10-211-1-4）的网络数据包是否发往master的主机网卡ens33上：" class="headerlink" title="8. 验证发往目标Pod（地址为10.211.1.4）的网络数据包是否发往master的主机网卡ens33上："></a>8. 验证发往目标Pod（地址为10.211.1.4）的网络数据包是否发往master的主机网卡ens33上：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 开启一个新的终端执行</span><br><span class="line"># tcpdump -i ens33 icmp -v</span><br><span class="line">tcpdump: listening on ens33, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">02:30:22.537037 IP (tos 0x0, ttl 63, id 23441, offset 0, flags [DF], proto ICMP (1), length 84)</span><br><span class="line">    10.211.0.6 &gt; 10.211.1.4: ICMP echo request, id 23, seq 1, length 64</span><br><span class="line">02:30:22.537476 IP (tos 0x0, ttl 63, id 62650, offset 0, flags [none], proto ICMP (1), length 84)</span><br><span class="line">    10.211.1.4 &gt; 10.211.0.6: ICMP echo reply, id 23, seq 1, length 64</span><br><span class="line"></span><br><span class="line"># 在原有终端上执行</span><br><span class="line"># kubectl exec -it network-5z4qp /bin/sh -n default</span><br><span class="line">/ # ping -c 1 10.211.1.4</span><br><span class="line">PING 10.211.1.4 (10.211.1.4) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.1.4: icmp_seq=1 ttl=62 time=0.526 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.1.4 ping statistics ---</span><br><span class="line">1 packets transmitted, 1 received, 0% packet loss, time 0ms</span><br><span class="line">rtt min/avg/max/mdev = 0.526/0.526/0.526/0.000 ms</span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure>
<h2 id="9-验证发往目标Pod（地址为10-211-1-4）的网络数据包是否发送到Node（地址为172-16-170-129）的网卡ens33上："><a href="#9-验证发往目标Pod（地址为10-211-1-4）的网络数据包是否发送到Node（地址为172-16-170-129）的网卡ens33上：" class="headerlink" title="9. 验证发往目标Pod（地址为10.211.1.4）的网络数据包是否发送到Node（地址为172.16.170.129）的网卡ens33上："></a>9. 验证发往目标Pod（地址为10.211.1.4）的网络数据包是否发送到Node（地址为172.16.170.129）的网卡ens33上：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 在地址为10.211.1.4的Pod所在的Node上</span><br><span class="line"># tcpdump -i ens33 icmp -v</span><br><span class="line">tcpdump: listening on ens33, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">02:33:17.808254 IP (tos 0x0, ttl 63, id 61238, offset 0, flags [DF], proto ICMP (1), length 84)</span><br><span class="line">    10.211.0.6 &gt; 10.211.1.4: ICMP echo request, id 33, seq 1, length 64</span><br><span class="line">02:33:17.808415 IP (tos 0x0, ttl 63, id 1878, offset 0, flags [none], proto ICMP (1), length 84)</span><br><span class="line">    10.211.1.4 &gt; 10.211.0.6: ICMP echo reply, id 33, seq 1, length 64</span><br><span class="line"></span><br><span class="line"># 回到地址为10.211.0.6的Pod所在的Node上</span><br><span class="line"># kubectl exec -it network-5z4qp /bin/sh -n default</span><br><span class="line">/ # ping -c 1 10.211.1.4</span><br><span class="line">PING 10.211.1.4 (10.211.1.4) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.1.4: icmp_seq=1 ttl=62 time=0.556 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.1.4 ping statistics ---</span><br><span class="line">1 packets transmitted, 1 received, 0% packet loss, time 0ms</span><br><span class="line">rtt min/avg/max/mdev = 0.556/0.556/0.556/0.000 ms</span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure>
<h2 id="10-在Node（地址为172-16-170-129）上查看发往目标Pod（地址为10-211-1-4）的下一跳在哪里："><a href="#10-在Node（地址为172-16-170-129）上查看发往目标Pod（地址为10-211-1-4）的下一跳在哪里：" class="headerlink" title="10. 在Node（地址为172.16.170.129）上查看发往目标Pod（地址为10.211.1.4）的下一跳在哪里："></a>10. 在Node（地址为172.16.170.129）上查看发往目标Pod（地址为10.211.1.4）的下一跳在哪里：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># ip route</span><br><span class="line">...</span><br><span class="line">10.211.1.4 dev calid406b8b6c93 scope link</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h2 id="11-验证发往目标Pod（地址为10-211-1-4）的网络数据包是否发送到Node（地址为172-16-170-129）的网络设备calid406b8b6c93上："><a href="#11-验证发往目标Pod（地址为10-211-1-4）的网络数据包是否发送到Node（地址为172-16-170-129）的网络设备calid406b8b6c93上：" class="headerlink" title="11. 验证发往目标Pod（地址为10.211.1.4）的网络数据包是否发送到Node（地址为172.16.170.129）的网络设备calid406b8b6c93上："></a>11. 验证发往目标Pod（地址为10.211.1.4）的网络数据包是否发送到Node（地址为172.16.170.129）的网络设备calid406b8b6c93上：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># 在地址为10.211.1.4的Pod所在的Node上</span><br><span class="line"># ip link show calid406b8b6c93</span><br><span class="line">7: calid406b8b6c93@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line"></span><br><span class="line"># tcpdump -i calid406b8b6c93 icmp -v</span><br><span class="line">tcpdump: listening on calid406b8b6c93, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">02:39:15.303283 IP (tos 0x0, ttl 62, id 44511, offset 0, flags [DF], proto ICMP (1), length 84)</span><br><span class="line">    10.211.0.6 &gt; 10.211.1.4: ICMP echo request, id 38, seq 1, length 64</span><br><span class="line">02:39:15.303392 IP (tos 0x0, ttl 64, id 47955, offset 0, flags [none], proto ICMP (1), length 84)</span><br><span class="line">    10.211.1.4 &gt; 10.211.0.6: ICMP echo reply, id 38, seq 1, length 64</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 回到地址为10.211.0.6的Pod所在的Node上（即master上），开启一个新的终端</span><br><span class="line"># kubectl exec -it network-5z4qp /bin/sh -n default</span><br><span class="line">/ # ping -c 1 10.211.1.4</span><br><span class="line">PING 10.211.1.4 (10.211.1.4) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.1.4: icmp_seq=1 ttl=62 time=0.602 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.1.4 ping statistics ---</span><br><span class="line">1 packets transmitted, 1 received, 0% packet loss, time 0ms</span><br><span class="line">rtt min/avg/max/mdev = 0.602/0.602/0.602/0.000 ms</span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure>
<h2 id="12-验证发往目标Pod（地址为10-211-1-4）的网络数据包是否发送到它的网络设备eth0上："><a href="#12-验证发往目标Pod（地址为10-211-1-4）的网络数据包是否发送到它的网络设备eth0上：" class="headerlink" title="12. 验证发往目标Pod（地址为10.211.1.4）的网络数据包是否发送到它的网络设备eth0上："></a>12. 验证发往目标Pod（地址为10.211.1.4）的网络数据包是否发送到它的网络设备eth0上：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"># kubectl exec -it network-mngxw /bin/sh -n default</span><br><span class="line">/ # ip link</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">2: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN mode DEFAULT group default qlen 1</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">4: eth0@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether e2:89:27:9e:6f:a6 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">/ # ip address</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">4: eth0@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether e2:89:27:9e:6f:a6 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 10.211.1.4/32 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::e089:27ff:fe9e:6fa6/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">/ # tcpdump -i eth0 icmp -v</span><br><span class="line">tcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">07:42:45.870953 IP (tos 0x0, ttl 62, id 13540, offset 0, flags [DF], proto ICMP (1), length 84)</span><br><span class="line">    10.211.0.6 &gt; network-mngxw: ICMP echo request, id 43, seq 1, length 64</span><br><span class="line">07:42:45.871065 IP (tos 0x0, ttl 64, id 19616, offset 0, flags [none], proto ICMP (1), length 84)</span><br><span class="line">    network-mngxw &gt; 10.211.0.6: ICMP echo reply, id 43, seq 1, length 64</span><br><span class="line"></span><br><span class="line"># 回到地址为10.211.0.6的Pod所在的Node上（即master上）</span><br><span class="line"># kubectl exec -it network-5z4qp /bin/sh -n default</span><br><span class="line">/ # ping -c 1 10.211.1.4</span><br><span class="line">PING 10.211.1.4 (10.211.1.4) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.1.4: icmp_seq=1 ttl=62 time=0.602 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.1.4 ping statistics ---</span><br><span class="line">1 packets transmitted, 1 received, 0% packet loss, time 0ms</span><br><span class="line">rtt min/avg/max/mdev = 0.602/0.602/0.602/0.000 ms</span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure>
<h1 id="四、参考资料"><a href="#四、参考资料" class="headerlink" title="四、参考资料"></a>四、参考资料</h1><p><a href="https://docs.projectcalico.org/v3.1/usage/calicoctl/install" target="_blank" rel="noopener">https://docs.projectcalico.org/v3.1/usage/calicoctl/install</a><br><a href="https://mp.weixin.qq.com/s/MZIj_cvvtTiAfNf_0lpfTg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/MZIj_cvvtTiAfNf_0lpfTg</a><br><a href="https://mp.weixin.qq.com/s/oKxsWDTvoLeOSHAuPIxnGw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/oKxsWDTvoLeOSHAuPIxnGw</a><br><a href="https://blog.csdn.net/ccy19910925/article/details/82424275" target="_blank" rel="noopener">https://blog.csdn.net/ccy19910925/article/details/82424275</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Calico/">Calico</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Network/">Network</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kubernetes_trouble_shooting_000" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/kubernetes_trouble_shooting_000/" class="article-date">
  	<time datetime="2019-05-27T04:34:05.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/kubernetes_trouble_shooting_000/">
        如何清理kubelet产生的挂载点？
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>有时Kubernetes节点故障，需要从集群中移除，修复后再加入集群。在这个修复过程中，清理Kubelet相关挂载点是个非常重要的操作。在实际使用过程中，我发现kubeadm工具的reset子命令中有这个操作，因此特意查阅了Kubeadm的源码，找到了源码里的实现方法，在这里做个简单的记录。</p>
<p>清理kubelet相关挂载点的命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 停止Kubernetes的某一个Node节点</span><br><span class="line">systemctl stop kubelet.service &amp;&amp; systemctl stop docker.service</span><br><span class="line"></span><br><span class="line"># 清理Kubelet的相关挂载点</span><br><span class="line">awk &apos;$2 ~ path &#123;print $2&#125;&apos; path=/var/lib/kubelet /proc/mounts | xargs -r umount</span><br><span class="line"></span><br><span class="line"># 恢复Kubernetes的某一个Node节点</span><br><span class="line">systemctl start docker.service &amp;&amp; systemctl start kubelet.service</span><br></pre></td></tr></table></figure>
<p>参考资料：<br><a href="https://github.com/kubernetes/kubernetes/blob/v1.11.0/cmd/kubeadm/app/cmd/reset.go" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/blob/v1.11.0/cmd/kubeadm/app/cmd/reset.go</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Trouble-Shooting/">Trouble Shooting</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kubernetes_glusterfs_003" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/kubernetes_glusterfs_003/" class="article-date">
  	<time datetime="2019-05-27T04:33:57.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/kubernetes_glusterfs_003/">
        使用Heketi工具管理GlusterFS集群
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="使用Heketi管理GlusterFS-Cluster"><a href="#使用Heketi管理GlusterFS-Cluster" class="headerlink" title="使用Heketi管理GlusterFS Cluster"></a>使用Heketi管理GlusterFS Cluster</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br></pre></td><td class="code"><pre><span class="line"># 创建集群</span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 --json=true cluster create &#123;&quot;id&quot;:&quot;0f0e5d914e3d9a17bef670dd6e295512&quot;,&quot;nodes&quot;:[],&quot;volumes&quot;:[]&#125;</span><br><span class="line"></span><br><span class="line"># 查看集群</span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 --json=true cluster list &#123;&quot;clusters&quot;:[&quot;0f0e5d914e3d9a17bef670dd6e295512&quot;]&#125;</span><br><span class="line"></span><br><span class="line"># 添加节点</span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 --json=true node add --cluster=0f0e5d914e3d9a17bef670dd6e295512 --management-host-name=192.168.86.135 --storage-host-name=192.168.86.135 --zone=1</span><br><span class="line">&#123;&quot;zone&quot;:1,&quot;hostnames&quot;:&#123;&quot;manage&quot;:[&quot;192.168.86.135&quot;],&quot;storage&quot;:[&quot;192.168.86.135&quot;]&#125;,&quot;cluster&quot;:&quot;0f0e5d914e3d9a17bef670dd6e295512&quot;,&quot;i</span><br><span class="line"></span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 --json=true node add --cluster=0f0e5d914e3d9a17bef670dd6e295512 --management-host-name=192.168.86.136 --storage-host-name=192.168.86.136 --zone=1</span><br><span class="line">&#123;&quot;zone&quot;:1,&quot;hostnames&quot;:&#123;&quot;manage&quot;:[&quot;192.168.86.136&quot;],&quot;storage&quot;:[&quot;192.168.86.136&quot;]&#125;,&quot;cluster&quot;:&quot;0f0e5d914e3d9a17bef670dd6e295512&quot;,&quot;i</span><br><span class="line"></span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 --json=true node add --cluster=0f0e5d914e3d9a17bef670dd6e295512 --management-host-name=192.168.86.137 --storage-host-name=192.168.86.137 --zone=1</span><br><span class="line">&#123;&quot;zone&quot;:1,&quot;hostnames&quot;:&#123;&quot;manage&quot;:[&quot;192.168.86.137&quot;],&quot;storage&quot;:[&quot;192.168.86.137&quot;]&#125;,&quot;cluster&quot;:&quot;0f0e5d914e3d9a17bef670dd6e295512&quot;,&quot;i</span><br><span class="line"></span><br><span class="line"># 查看节点</span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 --json=true node list Id:a2c00d537210d42d68679bfa240db8b5 Cluster:0f0e5d914e3d9a17bef670dd6e295512</span><br><span class="line">Id:a73f8bdd464f02a4c7e0dd30c4ed5afa Cluster:0f0e5d914e3d9a17bef670dd6e295512</span><br><span class="line">Id:aba2f2cfe93c0f3e4164aa05b8d6ddb2 Cluster:0f0e5d914e3d9a17bef670dd6e295512</span><br><span class="line"></span><br><span class="line"># 给节点添加设备(裸硬盘)</span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 --json=true device add --name=/dev/sdb --node=a2c00d537210d42d68679bfa240db8b5</span><br><span class="line">Device added successfully</span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 --json=true device add --name=/dev/sdb --node=a73f8bdd464f02a4c7e0dd30c4ed5afa</span><br><span class="line">Device added successfully</span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 --json=true device add --name=/dev/sdb --node=aba2f2cfe93c0f3e4164aa05b8d6ddb2</span><br><span class="line">Device added successfully</span><br><span class="line"></span><br><span class="line"># 创建volume</span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 volume create --size=10 --replica=2 </span><br><span class="line">Name: vol_d1fd574462c162f42fb61ef55c3e4e6e</span><br><span class="line">Size: 10</span><br><span class="line">Volume Id: d1fd574462c162f42fb61ef55c3e4e6e</span><br><span class="line">Cluster Id: 0f0e5d914e3d9a17bef670dd6e295512</span><br><span class="line">Mount: 192.168.86.135:vol_d1fd574462c162f42fb61ef55c3e4e6e</span><br><span class="line">Mount Options: backup-volfile-servers=192.168.86.136,192.168.86.137</span><br><span class="line">Durability Type: replicate</span><br><span class="line">Distributed+Replica: 2</span><br><span class="line"></span><br><span class="line"># 查看volume</span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 volume list Id:d1fd574462c162f42fb61ef55c3e4e6e Cluster:0f0e5d914e3d9a17bef670dd6e295512 Name:vol_d1fd574462c162f42fb61ef55c3e4e6e</span><br><span class="line"># 删除volume</span><br><span class="line">[root@server07 ~]# heketi-cli --server http://192.168.86.135:8080 --user admin --secret 12345678 volume delete d1fd574462c162f42fb61ef55c3e4e6e</span><br><span class="line">Volume d1fd574462c162f42fb61ef55c3e4e6e deleted</span><br><span class="line"></span><br><span class="line"># 使用topology.json初始化GlusterFS Cluster</span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/heketi/topology.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;clusters&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;nodes&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;node&quot;: &#123;</span><br><span class="line">            &quot;hostnames&quot;: &#123;</span><br><span class="line">              &quot;manage&quot;: [</span><br><span class="line">                &quot;server07&quot;</span><br><span class="line">              ],</span><br><span class="line">              &quot;storage&quot;: [</span><br><span class="line">                &quot;172.16.170.134&quot;</span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;zone&quot;: 1</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;devices&quot;: [</span><br><span class="line">            &quot;/dev/sdb&quot;</span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;node&quot;: &#123;</span><br><span class="line">            &quot;hostnames&quot;: &#123;</span><br><span class="line">              &quot;manage&quot;: [</span><br><span class="line">                &quot;server08&quot;</span><br><span class="line">              ],</span><br><span class="line">              &quot;storage&quot;: [</span><br><span class="line">                &quot;172.16.170.135&quot;</span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;zone&quot;: 1</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;devices&quot;: [</span><br><span class="line">            &quot;/dev/sdb&quot;</span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;node&quot;: &#123;</span><br><span class="line">            &quot;hostnames&quot;: &#123;</span><br><span class="line">              &quot;manage&quot;: [</span><br><span class="line">                &quot;server09&quot;</span><br><span class="line">              ],</span><br><span class="line">              &quot;storage&quot;: [</span><br><span class="line">                &quot;172.16.170.136&quot;</span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;zone&quot;: 1</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;devices&quot;: [</span><br><span class="line">            &quot;/dev/sdb&quot;</span><br><span class="line">          ]</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">heketi-cli --user admin --secret 12345678 topology load --json=/etc/heketi/topology.json</span><br><span class="line">Creating cluster ... ID: 4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line">	Allowing file volumes on cluster.</span><br><span class="line">	Allowing block volumes on cluster.</span><br><span class="line">	Creating node server07 ... ID: 2e8b83add06cde8713d56ecb7d424033</span><br><span class="line">		Adding device /dev/sdb ... OK</span><br><span class="line">	Creating node server08 ... ID: 56a23b0579e7f79511843e046e69008f</span><br><span class="line">		Adding device /dev/sdb ... OK</span><br><span class="line">	Creating node server09 ... ID: 9fb13189ba78ef302046fe4414f633bf</span><br><span class="line">		Adding device /dev/sdb ... OK</span><br><span class="line"></span><br><span class="line">heketi-cli --user admin --secret 12345678 topology info</span><br><span class="line"></span><br><span class="line">Cluster Id: 4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line"></span><br><span class="line">    File:  true</span><br><span class="line">    Block: true</span><br><span class="line"></span><br><span class="line">    Volumes:</span><br><span class="line"></span><br><span class="line">    Nodes:</span><br><span class="line"></span><br><span class="line">	Node Id: 2e8b83add06cde8713d56ecb7d424033</span><br><span class="line">	State: online</span><br><span class="line">	Cluster Id: 4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line">	Zone: 1</span><br><span class="line">	Management Hostnames: server07</span><br><span class="line">	Storage Hostnames: 172.16.170.134</span><br><span class="line">	Devices:</span><br><span class="line">		Id:4995671aaef70cb1f640d0f411e94d2f   Name:/dev/sdb            State:online    Size (GiB):19      Used (GiB):0       Free (GiB):19</span><br><span class="line">			Bricks:</span><br><span class="line"></span><br><span class="line">	Node Id: 56a23b0579e7f79511843e046e69008f</span><br><span class="line">	State: online</span><br><span class="line">	Cluster Id: 4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line">	Zone: 1</span><br><span class="line">	Management Hostnames: server08</span><br><span class="line">	Storage Hostnames: 172.16.170.135</span><br><span class="line">	Devices:</span><br><span class="line">		Id:c4a56208419516d4fbc437d9dc3b265e   Name:/dev/sdb            State:online    Size (GiB):19      Used (GiB):0       Free (GiB):19</span><br><span class="line">			Bricks:</span><br><span class="line"></span><br><span class="line">	Node Id: 9fb13189ba78ef302046fe4414f633bf</span><br><span class="line">	State: online</span><br><span class="line">	Cluster Id: 4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line">	Zone: 1</span><br><span class="line">	Management Hostnames: server09</span><br><span class="line">	Storage Hostnames: 172.16.170.136</span><br><span class="line">	Devices:</span><br><span class="line">		Id:be2a9558f0634233be72f0c55d051898   Name:/dev/sdb            State:online    Size (GiB):19      Used (GiB):0       Free (GiB):19</span><br><span class="line">			Bricks:</span><br></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GlusterFS/">GlusterFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Storage/">Storage</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kubernetes_glusterfs_002" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/kubernetes_glusterfs_002/" class="article-date">
  	<time datetime="2019-05-27T04:33:51.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/kubernetes_glusterfs_002/">
        使用Gluster工具管理GlusterFS集群
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、配置和验证GlusterFS-Cluster"><a href="#一、配置和验证GlusterFS-Cluster" class="headerlink" title="一、配置和验证GlusterFS Cluster"></a>一、配置和验证GlusterFS Cluster</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 添加节点到glusterfs cluster</span><br><span class="line">[root@server07 ~]# gluster peer probe server08</span><br><span class="line">peer probe: success.</span><br><span class="line"></span><br><span class="line">[root@server07 ~]# gluster peer probe server09 </span><br><span class="line">peer probe: success.</span><br><span class="line"></span><br><span class="line"># 查看glusterfs cluster的状态 </span><br><span class="line">[root@server07 ~]# gluster peer status </span><br><span class="line">Number of Peers: 2</span><br><span class="line"></span><br><span class="line">Hostname: server08</span><br><span class="line">Uuid: 8530c074-760f-4d03-a5a7-f1b3ccaa5cfd </span><br><span class="line">State: Peer in Cluster (Connected)</span><br><span class="line">Hostname: server09</span><br><span class="line"></span><br><span class="line">Uuid: 41a4b6df-bcb3-4650-8a21-54afc1e27cbe </span><br><span class="line">State: Peer in Cluster (Connected)</span><br></pre></td></tr></table></figure>
<h1 id="二、在GlusterFS-Cluster上操作和使用Volume"><a href="#二、在GlusterFS-Cluster上操作和使用Volume" class="headerlink" title="二、在GlusterFS Cluster上操作和使用Volume"></a>二、在GlusterFS Cluster上操作和使用Volume</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line"># 查看volume的列表</span><br><span class="line">[root@server07 ~]# gluster volume info </span><br><span class="line">No volumes present</span><br><span class="line"></span><br><span class="line"># 创建volume对应的存储目录(集群的所有主机上都要创建) </span><br><span class="line">mkdir -p /opt/gluster/data</span><br><span class="line"></span><br><span class="line"># 创建volume </span><br><span class="line">[root@server07 ~]# gluster volume create k8s-volume transport tcp server07:/opt/gluster/data server08:/opt/gluster/data server09:/opt/gluster/data force</span><br><span class="line">volume create: k8s-volume: success: please start the volume to access data</span><br><span class="line"></span><br><span class="line">[root@server07 ~]# gluster volume info k8s-volume</span><br><span class="line"></span><br><span class="line">Volume Name: k8s-volume</span><br><span class="line">Type: Distribute</span><br><span class="line">Volume ID: e4974285-1304-4ea7-b60f-ebe8375dba86</span><br><span class="line">Status: Created</span><br><span class="line">Snapshot Count: 0</span><br><span class="line">Number of Bricks: 3</span><br><span class="line">Transport-type: tcp</span><br><span class="line">Bricks:</span><br><span class="line">Brick1: server07:/opt/gluster/data</span><br><span class="line">Brick2: server08:/opt/gluster/data</span><br><span class="line">Brick3: server09:/opt/gluster/data</span><br><span class="line">Options Reconfigured:</span><br><span class="line">transport.address-family: inet </span><br><span class="line">nfs.disable: on</span><br><span class="line"></span><br><span class="line"># 启动volume</span><br><span class="line">[root@server07 ~]# gluster volume start k8s-volume </span><br><span class="line">volume start: k8s-volume: success</span><br><span class="line"></span><br><span class="line"># 查看volume的信息</span><br><span class="line">[root@server07 ~]# gluster volume info k8s-volume</span><br><span class="line"></span><br><span class="line">Volume Name: k8s-volume</span><br><span class="line">Type: Distribute</span><br><span class="line">Volume ID: e4974285-1304-4ea7-b60f-ebe8375dba86</span><br><span class="line">Status: Started</span><br><span class="line">Snapshot Count: 0</span><br><span class="line">Number of Bricks: 3</span><br><span class="line">Transport-type: tcp</span><br><span class="line">Bricks: </span><br><span class="line">Brick1: server07:/opt/gluster/data</span><br><span class="line">Brick2: server08:/opt/gluster/data</span><br><span class="line">Brick3: server09:/opt/gluster/data</span><br><span class="line">Options Reconfigured:</span><br><span class="line">transport.address-family: inet</span><br><span class="line">nfs.disable: on</span><br><span class="line"></span><br><span class="line"># 验证数据卷的挂载和数据写入</span><br><span class="line">[root@server07 ~]# mount -t glusterfs server07:k8s-volume /mnt</span><br><span class="line">[root@server07 ~]# ls -la /mnt</span><br><span class="line">drwxr-xr-x. 3 root root 4096 4月 9 23:36 .</span><br><span class="line">dr-xr-xr-x. 17 root root 224 4月 9 21:59 ..</span><br><span class="line">[root@server07 ~]# echo &quot;hello glusterfs kubernetes.&quot; &gt; /mnt/readme.md</span><br><span class="line">[root@server07 ~]# ls -la /mnt</span><br><span class="line">drwxr-xr-x. 3 root root 4096 4月 10 05:36 .</span><br><span class="line">dr-xr-xr-x. 17 root root 224 4月 9 21:59 ..</span><br><span class="line">-rw-r--r--. 1 root root 28 4月 10 05:36 readme.md</span><br><span class="line"></span><br><span class="line">[root@server07 ~]# cat /mnt/readme.md</span><br><span class="line">hello glusterfs kubernetes.</span><br><span class="line"></span><br><span class="line">[root@server07 ~]# df -h</span><br><span class="line">文件系统 容量 已用 可用 已用% 挂载点</span><br><span class="line">/dev/mapper/cl-root 8.0G 1.1G 7.0G 14% /</span><br><span class="line">devtmpfs 478M 0 478M 0% /dev</span><br><span class="line">tmpfs 489M 0 489M 0% /dev/shm</span><br><span class="line">tmpfs 489M 6.8M 482M 2% /run</span><br><span class="line">tmpfs 489M 0 489M 0% /sys/fs/cgroup</span><br><span class="line">/dev/sda1 1014M 139M 876M 14% /boot</span><br><span class="line">tmpfs 98M 0 98M 0% /run/user/0</span><br><span class="line">server07:k8s-volume 24G 3.2G 21G 14% /mnt</span><br><span class="line">[root@server07 ~]# umount server07:k8s-volume</span><br><span class="line">[root@server07 ~]# ls -la /mnt/</span><br><span class="line">drwxr-xr-x. 2 root root 6 11月 5 2016 .</span><br><span class="line">dr-xr-xr-x. 17 root root 224 4月 9 21:59 ..</span><br><span class="line"></span><br><span class="line"># 查看volume的信息</span><br><span class="line">[root@server07 ~]# gluster volume info k8s-volume</span><br><span class="line"></span><br><span class="line">Volume Name: k8s-volume</span><br><span class="line">Type: Distribute</span><br><span class="line">Volume ID: e4974285-1304-4ea7-b60f-ebe8375dba86</span><br><span class="line">Status: Started</span><br><span class="line">Snapshot Count: 0</span><br><span class="line">Number of Bricks: 3</span><br><span class="line">Transport-type: tcp</span><br><span class="line">Bricks: </span><br><span class="line">Brick1: server07:/opt/gluster/data</span><br><span class="line">Brick2: server08:/opt/gluster/data</span><br><span class="line">Brick3: server09:/opt/gluster/data</span><br><span class="line">Options Reconfigured:</span><br><span class="line">transport.address-family: inet</span><br><span class="line">nfs.disable: on</span><br><span class="line"></span><br><span class="line"># 停止volume</span><br><span class="line">[root@server07 ~]# gluster volume stop k8s-volume</span><br><span class="line">Stopping volume will make its data inaccessible. Do you want to continue? (y/n) y</span><br><span class="line">volume stop: k8s-volume: success</span><br><span class="line"></span><br><span class="line"># 查看volume的信息</span><br><span class="line">[root@server07 ~]# gluster volume info k8s-volume</span><br><span class="line"></span><br><span class="line">Volume Name: k8s-volume</span><br><span class="line">Type: Distribute</span><br><span class="line">Volume ID: e4974285-1304-4ea7-b60f-ebe8375dba86</span><br><span class="line">Status: Stopped</span><br><span class="line">Snapshot Count: 0</span><br><span class="line">Number of Bricks: 3</span><br><span class="line">Transport-type: tcp</span><br><span class="line">Bricks: </span><br><span class="line">Brick1: server07:/opt/gluster/data</span><br><span class="line">Brick2: server08:/opt/gluster/data</span><br><span class="line">Brick3: server09:/opt/gluster/data</span><br><span class="line">Options Reconfigured:</span><br><span class="line">transport.address-family: inet</span><br><span class="line">nfs.disable: on</span><br><span class="line"></span><br><span class="line"># 删除volume</span><br><span class="line">[root@server07 ~]# gluster volume delete k8s-volume</span><br><span class="line">Deleting volume will erase all information about the volume. Do you want to continue? (y/n) y</span><br><span class="line">volume delete: k8s-volume: success</span><br><span class="line"></span><br><span class="line"># 查看volume的信息</span><br><span class="line">[root@server07 ~]# gluster volume info k8s-volume</span><br><span class="line">Volume k8s-volume does not exist</span><br></pre></td></tr></table></figure>
<h1 id="三、如何重置GlusterFS-Cluster中的所有Node"><a href="#三、如何重置GlusterFS-Cluster中的所有Node" class="headerlink" title="三、如何重置GlusterFS Cluster中的所有Node"></a>三、如何重置GlusterFS Cluster中的所有Node</h1><p>假设集群中只有一个volume，它叫k8s-volume，下面对集群进行重置:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 重置glusterfs</span><br><span class="line">gluster volume list</span><br><span class="line">gluster volume stop k8s-volume </span><br><span class="line">gluster volume delete k8s-volume </span><br><span class="line">gluster volume list</span><br><span class="line">gluster peer status</span><br><span class="line">gluster peer help</span><br><span class="line">gluster peer detach server08</span><br><span class="line">gluster peer detach server09</span><br><span class="line">gluster peer status</span><br></pre></td></tr></table></figure></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GlusterFS/">GlusterFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Storage/">Storage</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kubernetes_glusterfs_001" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/kubernetes_glusterfs_001/" class="article-date">
  	<time datetime="2019-05-27T04:33:46.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/kubernetes_glusterfs_001/">
        Kubernetes对接GlusterFS（把GlusterFS部署在Kubernetes集群外）
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、环境版本信息"><a href="#一、环境版本信息" class="headerlink" title="一、环境版本信息"></a>一、环境版本信息</h1><h2 id="1-版本信息"><a href="#1-版本信息" class="headerlink" title="1. 版本信息"></a>1. 版本信息</h2><p>CentOS 7.6.1810<br>Kubernetes v1.11.0<br>GlusterFS 5.3-1.el7 @centos-gluster5<br>Heketi 8.0.0-1.el7 @centos-gluster5</p>
<h2 id="2-服务器信息"><a href="#2-服务器信息" class="headerlink" title="2. 服务器信息"></a>2. 服务器信息</h2><p>server01 <-> 172.16.170.128 <-> kubernetes master<br>server02 <-> 172.16.170.129 <-> kubernetes node<br>server03 <-> 172.16.170.130 <-> kubernetes node</-></-></-></-></-></-></p>
<p>server07 <-> 172.16.170.134 <-> glusterfs node<br>server08 <-> 172.16.170.135 <-> glusterfs node<br>server09 <-> 172.16.170.136 <-> glusterfs node</-></-></-></-></-></-></p>
<h1 id="二、相关注意事项"><a href="#二、相关注意事项" class="headerlink" title="二、相关注意事项"></a>二、相关注意事项</h1><h2 id="1-Heketi所在Node到GlusterFS-Cluster的所有Node需要配置root用户的SSH免密登录；"><a href="#1-Heketi所在Node到GlusterFS-Cluster的所有Node需要配置root用户的SSH免密登录；" class="headerlink" title="1. Heketi所在Node到GlusterFS Cluster的所有Node需要配置root用户的SSH免密登录；"></a>1. Heketi所在Node到GlusterFS Cluster的所有Node需要配置root用户的SSH免密登录；</h2><h2 id="2-etc-heketi-heketi-json-中需要修改配置，不能不做任何修改就拿来直接使用；"><a href="#2-etc-heketi-heketi-json-中需要修改配置，不能不做任何修改就拿来直接使用；" class="headerlink" title="2. /etc/heketi/heketi.json 中需要修改配置，不能不做任何修改就拿来直接使用；"></a>2. /etc/heketi/heketi.json 中需要修改配置，不能不做任何修改就拿来直接使用；</h2><p>（1）安全的做法是需要开启jwt认证，两个key字段分别用来设置admin用户和user用户的访问密码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">  &quot;use_auth&quot;: true,</span><br><span class="line"></span><br><span class="line">  &quot;_jwt&quot;: &quot;Private keys for access&quot;,</span><br><span class="line">  &quot;jwt&quot;: &#123;</span><br><span class="line">    &quot;_admin&quot;: &quot;Admin has access to all APIs&quot;,</span><br><span class="line">    &quot;admin&quot;: &#123;</span><br><span class="line">      &quot;key&quot;: &quot;12345678&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;_user&quot;: &quot;User only has access to /volumes endpoint&quot;,</span><br><span class="line">    &quot;user&quot;: &#123;</span><br><span class="line">      &quot;key&quot;: &quot;12345678&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>（2）需要把executor修改为ssh，不能使用默认的mock：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">    &quot;executor&quot;: &quot;ssh&quot;,</span><br><span class="line"></span><br><span class="line">    &quot;sshexec&quot;: &#123;</span><br><span class="line">      &quot;keyfile&quot;: &quot;/root/.ssh/id_rsa&quot;,</span><br><span class="line">      &quot;user&quot;: &quot;root&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>（3）json文件中有些配置想使用默认值，需要注释掉或者删除掉，不然无法启动Heketi，因为这些字段的value部分写得是字段的使用说明而非默认值。</p>
<h2 id="3-lib-systemd-system-heketi-service-中的User值应该为root，不修改的话默认为heketi，无法成功启动。"><a href="#3-lib-systemd-system-heketi-service-中的User值应该为root，不修改的话默认为heketi，无法成功启动。" class="headerlink" title="3. /lib/systemd/system/heketi.service 中的User值应该为root，不修改的话默认为heketi，无法成功启动。"></a>3. /lib/systemd/system/heketi.service 中的User值应该为root，不修改的话默认为heketi，无法成功启动。</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">User=root</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h1 id="三、完整的实现过程记录"><a href="#三、完整的实现过程记录" class="headerlink" title="三、完整的实现过程记录"></a>三、完整的实现过程记录</h1><h2 id="1-Kubernetes集群的所有节点上（既包括Master也包括Node）上都执行："><a href="#1-Kubernetes集群的所有节点上（既包括Master也包括Node）上都执行：" class="headerlink" title="1. Kubernetes集群的所有节点上（既包括Master也包括Node）上都执行："></a>1. Kubernetes集群的所有节点上（既包括Master也包括Node）上都执行：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 安装GlusterFS的yum源</span><br><span class="line">yum install -y centos-release-gluster</span><br><span class="line"># 安装挂载GlusterFS Volume所需要的驱动</span><br><span class="line">yum install -y glusterfs glusterfs-fuse</span><br></pre></td></tr></table></figure>
<h2 id="2-在Kubernetes集群外部安装GlusterFS集群："><a href="#2-在Kubernetes集群外部安装GlusterFS集群：" class="headerlink" title="2. 在Kubernetes集群外部安装GlusterFS集群："></a>2. 在Kubernetes集群外部安装GlusterFS集群：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"># 在server07、server08和server09三台服务器上执行</span><br><span class="line">yum update -y</span><br><span class="line">setenforce 0</span><br><span class="line">systemctl disable firewalld.service</span><br><span class="line">systemctl stop firewalld.service</span><br><span class="line"></span><br><span class="line">systemctl disable NetworkManager.service</span><br><span class="line">systemctl stop NetworkManager.service</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; /etc/hosts</span><br><span class="line"></span><br><span class="line"># For GlusterFS Cluster</span><br><span class="line">172.16.170.134 server07</span><br><span class="line">172.16.170.135 server08</span><br><span class="line">172.16.170.136 server09</span><br><span class="line"></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">yum install -y centos-release-gluster</span><br><span class="line">yum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma glusterfs-geo-replication glusterfs-devel</span><br><span class="line">mkdir /opt/glusterd</span><br><span class="line">sed -i &apos;s/var\/lib/opt/g&apos; /etc/glusterfs/glusterd.vol</span><br><span class="line">systemctl enable glusterd.service</span><br><span class="line">systemctl start glusterd.service</span><br><span class="line">systemctl status glusterd.service</span><br><span class="line"></span><br><span class="line"># 在server07服务器上执行</span><br><span class="line">yum install -y heketi heketi-client</span><br><span class="line">ssh-keygen</span><br><span class="line">ssh-copy-id root@server07</span><br><span class="line">ssh-copy-id root@server08</span><br><span class="line">ssh-copy-id root@server09</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/heketi/heketi.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;_port_comment&quot;: &quot;Heketi Server Port Number&quot;,</span><br><span class="line">  &quot;port&quot;: &quot;8080&quot;,</span><br><span class="line"></span><br><span class="line">  &quot;_use_auth&quot;: &quot;Enable JWT authorization. Please enable for deployment&quot;,</span><br><span class="line">  &quot;use_auth&quot;: true,</span><br><span class="line"></span><br><span class="line">  &quot;_jwt&quot;: &quot;Private keys for access&quot;,</span><br><span class="line">  &quot;jwt&quot;: &#123;</span><br><span class="line">    &quot;_admin&quot;: &quot;Admin has access to all APIs&quot;,</span><br><span class="line">    &quot;admin&quot;: &#123;</span><br><span class="line">      &quot;key&quot;: &quot;12345678&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;_user&quot;: &quot;User only has access to /volumes endpoint&quot;,</span><br><span class="line">    &quot;user&quot;: &#123;</span><br><span class="line">      &quot;key&quot;: &quot;12345678&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  &quot;_glusterfs_comment&quot;: &quot;GlusterFS Configuration&quot;,</span><br><span class="line">  &quot;glusterfs&quot;: &#123;</span><br><span class="line">    &quot;_executor_comment&quot;: [</span><br><span class="line">      &quot;Execute plugin. Possible choices: mock, ssh&quot;,</span><br><span class="line">      &quot;mock: This setting is used for testing and development.&quot;,</span><br><span class="line">      &quot;      It will not send commands to any node.&quot;,</span><br><span class="line">      &quot;ssh:  This setting will notify Heketi to ssh to the nodes.&quot;,</span><br><span class="line">      &quot;      It will need the values in sshexec to be configured.&quot;,</span><br><span class="line">      &quot;kubernetes: Communicate with GlusterFS containers over&quot;,</span><br><span class="line">      &quot;            Kubernetes exec api.&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;executor&quot;: &quot;ssh&quot;,</span><br><span class="line"></span><br><span class="line">    &quot;sshexec&quot;: &#123;</span><br><span class="line">      &quot;keyfile&quot;: &quot;/root/.ssh/id_rsa&quot;,</span><br><span class="line">      &quot;user&quot;: &quot;root&quot;</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    &quot;_kubeexec_comment&quot;: &quot;Kubernetes configuration&quot;,</span><br><span class="line">    &quot;kubeexec&quot;: &#123;</span><br><span class="line">      &quot;host&quot; :&quot;https://kubernetes.host:8443&quot;,</span><br><span class="line">      &quot;cert&quot; : &quot;/path/to/crt.file&quot;,</span><br><span class="line">      &quot;insecure&quot;: false,</span><br><span class="line">      &quot;user&quot;: &quot;kubernetes username&quot;,</span><br><span class="line">      &quot;password&quot;: &quot;password for kubernetes user&quot;,</span><br><span class="line">      &quot;namespace&quot;: &quot;OpenShift project or Kubernetes namespace&quot;,</span><br><span class="line">      &quot;fstab&quot;: &quot;Optional: Specify fstab file on node.  Default is /etc/fstab&quot;</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    &quot;_db_comment&quot;: &quot;Database file name&quot;,</span><br><span class="line">    &quot;db&quot;: &quot;/var/lib/heketi/heketi.db&quot;,</span><br><span class="line"></span><br><span class="line">    &quot;_loglevel_comment&quot;: [</span><br><span class="line">      &quot;Set log level. Choices are:&quot;,</span><br><span class="line">      &quot;  none, critical, error, warning, info, debug&quot;,</span><br><span class="line">      &quot;Default is warning&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;loglevel&quot; : &quot;debug&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /lib/systemd/system/heketi.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Heketi Server</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=simple</span><br><span class="line">WorkingDirectory=/var/lib/heketi</span><br><span class="line">User=root</span><br><span class="line">ExecStart=/usr/bin/heketi --config=/etc/heketi/heketi.json</span><br><span class="line">Restart=on-failure</span><br><span class="line">StandardOutput=syslog</span><br><span class="line">StandardError=syslog</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable heketi.service</span><br><span class="line">systemctl start heketi.service</span><br><span class="line">systemctl status heketi.service</span><br><span class="line"></span><br><span class="line">curl -XGET http://localhost:8080/hello</span><br><span class="line">Hello from Heketi</span><br></pre></td></tr></table></figure>
<h2 id="3-使用Heketi初始化GlusterFS集群："><a href="#3-使用Heketi初始化GlusterFS集群：" class="headerlink" title="3. 使用Heketi初始化GlusterFS集群："></a>3. 使用Heketi初始化GlusterFS集群：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; /etc/heketi/topology.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;clusters&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;nodes&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;node&quot;: &#123;</span><br><span class="line">            &quot;hostnames&quot;: &#123;</span><br><span class="line">              &quot;manage&quot;: [</span><br><span class="line">                &quot;server07&quot;</span><br><span class="line">              ],</span><br><span class="line">              &quot;storage&quot;: [</span><br><span class="line">                &quot;172.16.170.134&quot;</span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;zone&quot;: 1</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;devices&quot;: [</span><br><span class="line">            &quot;/dev/sdb&quot;</span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;node&quot;: &#123;</span><br><span class="line">            &quot;hostnames&quot;: &#123;</span><br><span class="line">              &quot;manage&quot;: [</span><br><span class="line">                &quot;server08&quot;</span><br><span class="line">              ],</span><br><span class="line">              &quot;storage&quot;: [</span><br><span class="line">                &quot;172.16.170.135&quot;</span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;zone&quot;: 1</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;devices&quot;: [</span><br><span class="line">            &quot;/dev/sdb&quot;</span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;node&quot;: &#123;</span><br><span class="line">            &quot;hostnames&quot;: &#123;</span><br><span class="line">              &quot;manage&quot;: [</span><br><span class="line">                &quot;server09&quot;</span><br><span class="line">              ],</span><br><span class="line">              &quot;storage&quot;: [</span><br><span class="line">                &quot;172.16.170.136&quot;</span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;zone&quot;: 1</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;devices&quot;: [</span><br><span class="line">            &quot;/dev/sdb&quot;</span><br><span class="line">          ]</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">heketi-cli --user admin --secret 12345678 topology load --json=/etc/heketi/topology.json</span><br><span class="line">Creating cluster ... ID: 4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line">	Allowing file volumes on cluster.</span><br><span class="line">	Allowing block volumes on cluster.</span><br><span class="line">	Creating node server07 ... ID: 2e8b83add06cde8713d56ecb7d424033</span><br><span class="line">		Adding device /dev/sdb ... OK</span><br><span class="line">	Creating node server08 ... ID: 56a23b0579e7f79511843e046e69008f</span><br><span class="line">		Adding device /dev/sdb ... OK</span><br><span class="line">	Creating node server09 ... ID: 9fb13189ba78ef302046fe4414f633bf</span><br><span class="line">		Adding device /dev/sdb ... OK</span><br><span class="line"></span><br><span class="line">heketi-cli --user admin --secret 12345678 cluster list</span><br><span class="line">Clusters:</span><br><span class="line">Id:4e01ca44d2e6f0077ee3abe9c6783183 [file][block]</span><br><span class="line"></span><br><span class="line">heketi-cli --user admin --secret 12345678 --json=true node list</span><br><span class="line">Id:2e8b83add06cde8713d56ecb7d424033	Cluster:4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line">Id:56a23b0579e7f79511843e046e69008f	Cluster:4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line">Id:9fb13189ba78ef302046fe4414f633bf	Cluster:4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">heketi-cli --user admin --secret 12345678 topology info</span><br><span class="line"></span><br><span class="line">Cluster Id: 4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line"></span><br><span class="line">    File:  true</span><br><span class="line">    Block: true</span><br><span class="line"></span><br><span class="line">    Volumes:</span><br><span class="line"></span><br><span class="line">    Nodes:</span><br><span class="line"></span><br><span class="line">	Node Id: 2e8b83add06cde8713d56ecb7d424033</span><br><span class="line">	State: online</span><br><span class="line">	Cluster Id: 4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line">	Zone: 1</span><br><span class="line">	Management Hostnames: server07</span><br><span class="line">	Storage Hostnames: 172.16.170.134</span><br><span class="line">	Devices:</span><br><span class="line">		Id:4995671aaef70cb1f640d0f411e94d2f   Name:/dev/sdb            State:online    Size (GiB):19      Used (GiB):0       Free (GiB):19</span><br><span class="line">			Bricks:</span><br><span class="line"></span><br><span class="line">	Node Id: 56a23b0579e7f79511843e046e69008f</span><br><span class="line">	State: online</span><br><span class="line">	Cluster Id: 4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line">	Zone: 1</span><br><span class="line">	Management Hostnames: server08</span><br><span class="line">	Storage Hostnames: 172.16.170.135</span><br><span class="line">	Devices:</span><br><span class="line">		Id:c4a56208419516d4fbc437d9dc3b265e   Name:/dev/sdb            State:online    Size (GiB):19      Used (GiB):0       Free (GiB):19</span><br><span class="line">			Bricks:</span><br><span class="line"></span><br><span class="line">	Node Id: 9fb13189ba78ef302046fe4414f633bf</span><br><span class="line">	State: online</span><br><span class="line">	Cluster Id: 4e01ca44d2e6f0077ee3abe9c6783183</span><br><span class="line">	Zone: 1</span><br><span class="line">	Management Hostnames: server09</span><br><span class="line">	Storage Hostnames: 172.16.170.136</span><br><span class="line">	Devices:</span><br><span class="line">		Id:be2a9558f0634233be72f0c55d051898   Name:/dev/sdb            State:online    Size (GiB):19      Used (GiB):0       Free (GiB):19</span><br><span class="line">			Bricks:</span><br></pre></td></tr></table></figure>
<h2 id="4-编写验证Heketi和GlusterFS集群对接Kubernetes集群的相关YAML："><a href="#4-编写验证Heketi和GlusterFS集群对接Kubernetes集群的相关YAML：" class="headerlink" title="4. 编写验证Heketi和GlusterFS集群对接Kubernetes集群的相关YAML："></a>4. 编写验证Heketi和GlusterFS集群对接Kubernetes集群的相关YAML：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p glusterfs/</span><br><span class="line">cd glusterfs/</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 01-heketi-secret.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: heketi-secret</span><br><span class="line">  namespace: default</span><br><span class="line">data:</span><br><span class="line">  # base64 encoded password. E.g.: echo -n &quot;mypassword&quot; | base64</span><br><span class="line">  key: MTIzNDU2Nzg=</span><br><span class="line">type: kubernetes.io/glusterfs</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 02-glusterfs-storageclass.yaml</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: glusterfs</span><br><span class="line">provisioner: kubernetes.io/glusterfs</span><br><span class="line">parameters:</span><br><span class="line">  resturl: &quot;http://172.16.170.134:8080&quot;</span><br><span class="line">  clusterid: &quot;4e01ca44d2e6f0077ee3abe9c6783183&quot;</span><br><span class="line">  restauthenabled: &quot;true&quot;</span><br><span class="line">  restuser: &quot;admin&quot;</span><br><span class="line">  secretNamespace: &quot;default&quot;</span><br><span class="line">  secretName: &quot;heketi-secret&quot;</span><br><span class="line">  volumetype: &quot;replicate:3&quot;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 03-persistentvolumeclaim.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: myclaim</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br><span class="line">  storageClassName: glusterfs</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 04-pod.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: alpine</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - image: alpine:3.8</span><br><span class="line">    command:</span><br><span class="line">    - sleep</span><br><span class="line">    - &quot;3600&quot;</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    name: alpine</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: myvolume</span><br><span class="line">      mountPath: &quot;/data&quot;</span><br><span class="line">  restartPolicy: Always</span><br><span class="line">  volumes:</span><br><span class="line">  - name: myvolume</span><br><span class="line">    persistentVolumeClaim:</span><br><span class="line">      claimName: myclaim</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="5-验证Heketi和GlusterFS集群对接Kubernetes集群："><a href="#5-验证Heketi和GlusterFS集群对接Kubernetes集群：" class="headerlink" title="5. 验证Heketi和GlusterFS集群对接Kubernetes集群："></a>5. 验证Heketi和GlusterFS集群对接Kubernetes集群：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"># 创建StorageClass对接到GlusterFS</span><br><span class="line">kubectl create -f 01-heketi-secret.yaml</span><br><span class="line">secret/heketi-secret created</span><br><span class="line"></span><br><span class="line">kubectl create -f 02-glusterfs-storageclass.yaml</span><br><span class="line">storageclass.storage.k8s.io/glusterfs created</span><br><span class="line"></span><br><span class="line"># 创建PVC使用上面的StorageClass</span><br><span class="line">kubectl create -f 03-persistentvolumeclaim.yaml</span><br><span class="line">persistentvolumeclaim/myclaim created</span><br><span class="line"></span><br><span class="line"># 验证PVC动态供应PV</span><br><span class="line">kubectl get pvc -o wide</span><br><span class="line">NAME      STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">myclaim   Bound     pvc-9d5a1b14-2456-11e9-a588-000c2921eba0   1Gi        RWX            glusterfs      40s</span><br><span class="line"></span><br><span class="line">kubectl get pv -o wide</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM             STORAGECLASS   REASON    AGE</span><br><span class="line">pvc-9d5a1b14-2456-11e9-a588-000c2921eba0   1Gi        RWX            Delete           Bound     default/myclaim   glusterfs                40s</span><br><span class="line"></span><br><span class="line"># 创建测试Pod使用上面的PVC</span><br><span class="line">kubectl create -f 04-pod.yaml</span><br><span class="line">pod/alpine created</span><br><span class="line"></span><br><span class="line">kubectl get pod -o wide</span><br><span class="line">NAME      READY     STATUS    RESTARTS   AGE       IP           NODE</span><br><span class="line">alpine    1/1       Running   0          17s       10.211.1.2   server02</span><br><span class="line"></span><br><span class="line"># 验证PVC是否挂载到Pod中</span><br><span class="line">kubectl exec -it alpine /bin/sh</span><br><span class="line">/ # df -h</span><br><span class="line">Filesystem                Size      Used Available Use% Mounted on</span><br><span class="line">overlay                  17.0G      2.2G     14.8G  13% /</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /dev</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /sys/fs/cgroup</span><br><span class="line">172.16.170.134:vol_b46e22b816b18672ecb5a8c3289b0193</span><br><span class="line">                       1014.0M     42.7M    971.3M   4% /data</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /dev/termination-log</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /etc/resolv.conf</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /etc/hostname</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /etc/hosts</span><br><span class="line">shm                      64.0M         0     64.0M   0% /dev/shm</span><br><span class="line">tmpfs                   911.8M     12.0K    911.8M   0% /run/secrets/kubernetes.io/serviceaccount</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/kcore</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/timer_list</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/timer_stats</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/sched_debug</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /sys/firmware</span><br><span class="line"></span><br><span class="line"># 创建测试文件，验证存储容量的限制</span><br><span class="line">/data # fallocate -l 971M onebox.disk</span><br><span class="line">/data # df -h</span><br><span class="line">Filesystem                Size      Used Available Use% Mounted on</span><br><span class="line">overlay                  17.0G      2.2G     14.8G  13% /</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /dev</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /sys/fs/cgroup</span><br><span class="line">172.16.170.134:vol_b46e22b816b18672ecb5a8c3289b0193</span><br><span class="line">                       1014.0M   1013.7M    348.0K 100% /data</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /dev/termination-log</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /etc/resolv.conf</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /etc/hostname</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /etc/hosts</span><br><span class="line">shm                      64.0M         0     64.0M   0% /dev/shm</span><br><span class="line">tmpfs                   911.8M     12.0K    911.8M   0% /run/secrets/kubernetes.io/serviceaccount</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/kcore</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/timer_list</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/timer_stats</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/sched_debug</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /sys/firmware</span><br><span class="line">/data # fallocate -l 971M onebox.disk</span><br><span class="line">fallocate: fallocate &apos;onebox.disk&apos;: No space left on device</span><br><span class="line"></span><br><span class="line"># 测试文件删除后，验证容量是否恢复</span><br><span class="line">/data # rm -rf onebox.disk</span><br><span class="line">/data # df -h</span><br><span class="line">Filesystem                Size      Used Available Use% Mounted on</span><br><span class="line">overlay                  17.0G      2.2G     14.8G  13% /</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /dev</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /sys/fs/cgroup</span><br><span class="line">172.16.170.134:vol_b46e22b816b18672ecb5a8c3289b0193</span><br><span class="line">                       1014.0M     42.7M    971.3M   4% /data</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /dev/termination-log</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /etc/resolv.conf</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /etc/hostname</span><br><span class="line">/dev/mapper/cl-root      17.0G      2.2G     14.8G  13% /etc/hosts</span><br><span class="line">shm                      64.0M         0     64.0M   0% /dev/shm</span><br><span class="line">tmpfs                   911.8M     12.0K    911.8M   0% /run/secrets/kubernetes.io/serviceaccount</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/kcore</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/timer_list</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/timer_stats</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /proc/sched_debug</span><br><span class="line">tmpfs                   911.8M         0    911.8M   0% /sys/firmware</span><br></pre></td></tr></table></figure>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GlusterFS/">GlusterFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Storage/">Storage</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kubernetes_glusterfs_000" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/kubernetes_glusterfs_000/" class="article-date">
  	<time datetime="2019-05-27T04:33:41.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/kubernetes_glusterfs_000/">
        Kubernetes对接GlusterFS（把GlusterFS部署在Kubernetes集群内）
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、环境版本信息"><a href="#一、环境版本信息" class="headerlink" title="一、环境版本信息"></a>一、环境版本信息</h1><h2 id="1-版本信息"><a href="#1-版本信息" class="headerlink" title="1. 版本信息"></a>1. 版本信息</h2><p>CentOS 7.6.1810<br>Kubernetes v1.11.0<br>GlusterFS gluster3u13_centos7<br>Heketi 4</p>
<h2 id="2-服务器信息"><a href="#2-服务器信息" class="headerlink" title="2. 服务器信息"></a>2. 服务器信息</h2><p>server03 <-> 172.16.170.130 <-> kubernetes master<br>server07 <-> 172.16.170.134 <-> kubernetes node<br>server08 <-> 172.16.170.135 <-> kubernetes node<br>server09 <-> 172.16.170.136 <-> kubernetes node</-></-></-></-></-></-></-></-></p>
<h1 id="二、相关注意事项"><a href="#二、相关注意事项" class="headerlink" title="二、相关注意事项"></a>二、相关注意事项</h1><h2 id="1-不要使用链接-https-github-com-gluster-gluster-kubernetes-tree-v1-1-中的部署脚本；"><a href="#1-不要使用链接-https-github-com-gluster-gluster-kubernetes-tree-v1-1-中的部署脚本；" class="headerlink" title="1. 不要使用链接 https://github.com/gluster/gluster-kubernetes/tree/v1.1 中的部署脚本；"></a>1. 不要使用链接 <a href="https://github.com/gluster/gluster-kubernetes/tree/v1.1" target="_blank" rel="noopener">https://github.com/gluster/gluster-kubernetes/tree/v1.1</a> 中的部署脚本；</h2><p>笔者最开始用了，这个过程中还参考了网上很多的帖子，可谓是吃尽了苦头。。。。。。</p>
<h2 id="2-Heketi只有ServiceAccount是不够用的，需要给这个ServiceAccount创建ClusterRoleBinding对象，为其分配比较高的API访问权限；"><a href="#2-Heketi只有ServiceAccount是不够用的，需要给这个ServiceAccount创建ClusterRoleBinding对象，为其分配比较高的API访问权限；" class="headerlink" title="2. Heketi只有ServiceAccount是不够用的，需要给这个ServiceAccount创建ClusterRoleBinding对象，为其分配比较高的API访问权限；"></a>2. Heketi只有ServiceAccount是不够用的，需要给这个ServiceAccount创建ClusterRoleBinding对象，为其分配比较高的API访问权限；</h2><p>比如说笔者这里就简单处理，为其分配cluster-admin权限。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: heketi</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: heketi</span><br><span class="line">  namespace: storage</span><br></pre></td></tr></table></figure></p>
<h2 id="3-所有宿主机都要执行modprobe-dm-thin-pool，否则创建volume会报错；"><a href="#3-所有宿主机都要执行modprobe-dm-thin-pool，否则创建volume会报错；" class="headerlink" title="3. 所有宿主机都要执行modprobe dm_thin_pool，否则创建volume会报错；"></a>3. 所有宿主机都要执行modprobe dm_thin_pool，否则创建volume会报错；</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">modprobe dm_thin_pool</span><br></pre></td></tr></table></figure>
<h2 id="4-GlusterFS和Heketi的Docker镜像不要像社区代码中那样使用latest和dev这样镜像内容会随时更新的标签，建议选择内容固定的标签对应的镜像。"><a href="#4-GlusterFS和Heketi的Docker镜像不要像社区代码中那样使用latest和dev这样镜像内容会随时更新的标签，建议选择内容固定的标签对应的镜像。" class="headerlink" title="4. GlusterFS和Heketi的Docker镜像不要像社区代码中那样使用latest和dev这样镜像内容会随时更新的标签，建议选择内容固定的标签对应的镜像。"></a>4. GlusterFS和Heketi的Docker镜像不要像社区代码中那样使用latest和dev这样镜像内容会随时更新的标签，建议选择内容固定的标签对应的镜像。</h2><p>笔者选择的Docker镜像如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gluster/gluster-centos:gluster3u13_centos7</span><br><span class="line">heketi/heketi:4</span><br></pre></td></tr></table></figure></p>
<h1 id="三、完整的实现过程记录"><a href="#三、完整的实现过程记录" class="headerlink" title="三、完整的实现过程记录"></a>三、完整的实现过程记录</h1><h2 id="1-Kubernetes集群的所有节点上（既包括Master也包括Node）上都执行："><a href="#1-Kubernetes集群的所有节点上（既包括Master也包括Node）上都执行：" class="headerlink" title="1. Kubernetes集群的所有节点上（既包括Master也包括Node）上都执行："></a>1. Kubernetes集群的所有节点上（既包括Master也包括Node）上都执行：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 安装GlusterFS的yum源</span><br><span class="line">yum install -y centos-release-gluster</span><br><span class="line"># 安装挂载GlusterFS Volume所需要的驱动</span><br><span class="line">yum install -y glusterfs glusterfs-fuse</span><br><span class="line"># 开启Linux内核的 device-mapper target(s) 支持</span><br><span class="line">modprobe dm_thin_pool</span><br></pre></td></tr></table></figure>
<h2 id="2-参考社区的YAML，编写部署GlusterFS的YAML："><a href="#2-参考社区的YAML，编写部署GlusterFS的YAML：" class="headerlink" title="2. 参考社区的YAML，编写部署GlusterFS的YAML："></a>2. 参考社区的YAML，编写部署GlusterFS的YAML：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p gluster-kubernetes/deploy/gluster-kubernetes/</span><br><span class="line">cd gluster-kubernetes/deploy/gluster-kubernetes/</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 01-common-namespace.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: storage</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 02-glusterfs-daemonset.yaml</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: glusterfs</span><br><span class="line">  namespace: storage</span><br><span class="line">  labels:</span><br><span class="line">    glusterfs: daemonset</span><br><span class="line">  annotations:</span><br><span class="line">    description: GlusterFS DaemonSet</span><br><span class="line">    tags: glusterfs</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: glusterfs</span><br><span class="line">      labels:</span><br><span class="line">        glusterfs-node: pod</span><br><span class="line">    spec:</span><br><span class="line">      nodeSelector:</span><br><span class="line">        storagenode: glusterfs</span><br><span class="line">      hostNetwork: true</span><br><span class="line">      containers:</span><br><span class="line">      - image: gluster/gluster-centos:gluster3u13_centos7</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        name: glusterfs</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: glusterfs-heketi</span><br><span class="line">          mountPath: &quot;/var/lib/heketi&quot;</span><br><span class="line">        - name: glusterfs-run</span><br><span class="line">          mountPath: &quot;/run&quot;</span><br><span class="line">        - name: glusterfs-lvm</span><br><span class="line">          mountPath: &quot;/run/lvm&quot;</span><br><span class="line">        - name: glusterfs-etc</span><br><span class="line">          mountPath: &quot;/etc/glusterfs&quot;</span><br><span class="line">        - name: glusterfs-logs</span><br><span class="line">          mountPath: &quot;/var/log/glusterfs&quot;</span><br><span class="line">        - name: glusterfs-config</span><br><span class="line">          mountPath: &quot;/var/lib/glusterd&quot;</span><br><span class="line">        - name: glusterfs-dev</span><br><span class="line">          mountPath: &quot;/dev&quot;</span><br><span class="line">        - name: glusterfs-misc</span><br><span class="line">          mountPath: &quot;/var/lib/misc/glusterfsd&quot;</span><br><span class="line">        - name: glusterfs-cgroup</span><br><span class="line">          mountPath: &quot;/sys/fs/cgroup&quot;</span><br><span class="line">          readOnly: true</span><br><span class="line">        - name: glusterfs-ssl</span><br><span class="line">          mountPath: &quot;/etc/ssl&quot;</span><br><span class="line">          readOnly: true</span><br><span class="line">        securityContext:</span><br><span class="line">          capabilities: &#123;&#125;</span><br><span class="line">          privileged: true</span><br><span class="line">        readinessProbe:</span><br><span class="line">          timeoutSeconds: 3</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          exec:</span><br><span class="line">            command:</span><br><span class="line">            - &quot;/bin/bash&quot;</span><br><span class="line">            - &quot;-c&quot;</span><br><span class="line">            - systemctl status glusterd.service</span><br><span class="line">        livenessProbe:</span><br><span class="line">          timeoutSeconds: 3</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          exec:</span><br><span class="line">            command:</span><br><span class="line">            - &quot;/bin/bash&quot;</span><br><span class="line">            - &quot;-c&quot;</span><br><span class="line">            - systemctl status glusterd.service</span><br><span class="line">      volumes:</span><br><span class="line">      - name: glusterfs-heketi</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &quot;/var/lib/heketi&quot;</span><br><span class="line">      - name: glusterfs-run</span><br><span class="line">      - name: glusterfs-lvm</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &quot;/run/lvm&quot;</span><br><span class="line">      - name: glusterfs-etc</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &quot;/etc/glusterfs&quot;</span><br><span class="line">      - name: glusterfs-logs</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &quot;/var/log/glusterfs&quot;</span><br><span class="line">      - name: glusterfs-config</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &quot;/var/lib/glusterd&quot;</span><br><span class="line">      - name: glusterfs-dev</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &quot;/dev&quot;</span><br><span class="line">      - name: glusterfs-misc</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &quot;/var/lib/misc/glusterfsd&quot;</span><br><span class="line">      - name: glusterfs-cgroup</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &quot;/sys/fs/cgroup&quot;</span><br><span class="line">      - name: glusterfs-ssl</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &quot;/etc/ssl&quot;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 03-heketi-serviceaccount.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: heketi</span><br><span class="line">  namespace: storage</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 04-heketi-clusterrolebinding.yaml</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: heketi</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: heketi</span><br><span class="line">  namespace: storage</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 05-heketi-deployment.yaml</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: heketi</span><br><span class="line">  namespace: storage</span><br><span class="line">  labels:</span><br><span class="line">    glusterfs: heketi-deployment</span><br><span class="line">    deploy-heketi: heketi-deployment</span><br><span class="line">  annotations:</span><br><span class="line">    description: Defines how to deploy Heketi</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: heketi</span><br><span class="line">      labels:</span><br><span class="line">        name: deploy-heketi</span><br><span class="line">        glusterfs: heketi-pod</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: heketi</span><br><span class="line">      containers:</span><br><span class="line">      - image: heketi/heketi:4</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        name: heketi</span><br><span class="line">        env:</span><br><span class="line">        - name: HEKETI_EXECUTOR</span><br><span class="line">          value: kubernetes</span><br><span class="line">        - name: HEKETI_FSTAB</span><br><span class="line">          value: &quot;/var/lib/heketi/fstab&quot;</span><br><span class="line">        - name: HEKETI_SNAPSHOT_LIMIT</span><br><span class="line">          value: &apos;14&apos;</span><br><span class="line">        - name: HEKETI_KUBE_GLUSTER_DAEMONSET</span><br><span class="line">          value: &quot;y&quot;</span><br><span class="line">        - name: HEKETI_CLI_SERVER</span><br><span class="line">          value: &quot;http://127.0.0.1:8080&quot;</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: db</span><br><span class="line">          mountPath: &quot;/var/lib/heketi&quot;</span><br><span class="line">        readinessProbe:</span><br><span class="line">          timeoutSeconds: 3</span><br><span class="line">          initialDelaySeconds: 3</span><br><span class="line">          httpGet:</span><br><span class="line">            path: &quot;/hello&quot;</span><br><span class="line">            port: 8080</span><br><span class="line">        livenessProbe:</span><br><span class="line">          timeoutSeconds: 3</span><br><span class="line">          initialDelaySeconds: 30</span><br><span class="line">          httpGet:</span><br><span class="line">            path: &quot;/hello&quot;</span><br><span class="line">            port: 8080</span><br><span class="line">      volumes:</span><br><span class="line">      - name: db</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 06-heketi-service.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: heketi</span><br><span class="line">  namespace: storage</span><br><span class="line">  labels:</span><br><span class="line">    glusterfs: heketi-service</span><br><span class="line">    deploy-heketi: support</span><br><span class="line">  annotations:</span><br><span class="line">    description: Exposes Heketi Service</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    name: deploy-heketi</span><br><span class="line">  ports:</span><br><span class="line">  - name: deploy-heketi</span><br><span class="line">    port: 8080</span><br><span class="line">    targetPort: 8080</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="3-在Kubernetes集群上创建Heketi和GlusterFS集群："><a href="#3-在Kubernetes集群上创建Heketi和GlusterFS集群：" class="headerlink" title="3. 在Kubernetes集群上创建Heketi和GlusterFS集群："></a>3. 在Kubernetes集群上创建Heketi和GlusterFS集群：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cd ../../</span><br><span class="line">kubectl create -f deploy/gluster-kubernetes/</span><br><span class="line"></span><br><span class="line">kubectl get pod -n storage -o wide</span><br><span class="line">NAME                      READY     STATUS    RESTARTS   AGE       IP               NODE</span><br><span class="line">glusterfs-6shxs           1/1       Running   0          1m        172.16.170.135   server08</span><br><span class="line">glusterfs-9vpjg           1/1       Running   0          1m        172.16.170.136   server09</span><br><span class="line">glusterfs-pmq2n           1/1       Running   0          1m        172.16.170.134   server07</span><br><span class="line">heketi-5bdc8f9db5-nmjrw   1/1       Running   0          1m        10.211.1.12      server07</span><br></pre></td></tr></table></figure>
<h2 id="4-使用Heketi初始化GlusterFS集群："><a href="#4-使用Heketi初始化GlusterFS集群：" class="headerlink" title="4. 使用Heketi初始化GlusterFS集群："></a>4. 使用Heketi初始化GlusterFS集群：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; topology.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;clusters&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;nodes&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;node&quot;: &#123;</span><br><span class="line">            &quot;hostnames&quot;: &#123;</span><br><span class="line">              &quot;manage&quot;: [</span><br><span class="line">                &quot;server07&quot;</span><br><span class="line">              ],</span><br><span class="line">              &quot;storage&quot;: [</span><br><span class="line">                &quot;172.16.170.134&quot;</span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;zone&quot;: 1</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;devices&quot;: [</span><br><span class="line">            &quot;/dev/sdb&quot;</span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;node&quot;: &#123;</span><br><span class="line">            &quot;hostnames&quot;: &#123;</span><br><span class="line">              &quot;manage&quot;: [</span><br><span class="line">                &quot;server08&quot;</span><br><span class="line">              ],</span><br><span class="line">              &quot;storage&quot;: [</span><br><span class="line">                &quot;172.16.170.135&quot;</span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;zone&quot;: 1</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;devices&quot;: [</span><br><span class="line">            &quot;/dev/sdb&quot;</span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;node&quot;: &#123;</span><br><span class="line">            &quot;hostnames&quot;: &#123;</span><br><span class="line">              &quot;manage&quot;: [</span><br><span class="line">                &quot;server09&quot;</span><br><span class="line">              ],</span><br><span class="line">              &quot;storage&quot;: [</span><br><span class="line">                &quot;172.16.170.136&quot;</span><br><span class="line">              ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;zone&quot;: 1</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;devices&quot;: [</span><br><span class="line">            &quot;/dev/sdb&quot;</span><br><span class="line">          ]</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl cp topology.json heketi-5bdc8f9db5-nmjrw:/ -n storage</span><br><span class="line"></span><br><span class="line">kubectl exec -it heketi-5bdc8f9db5-nmjrw /bin/bash -n storage</span><br><span class="line">[root@heketi-5bdc8f9db5-nmjrw /]# heketi-cli topology load --json=topology.json</span><br><span class="line">Creating cluster ... ID: dde6f33a6cb852e04cb0685e210d3959</span><br><span class="line">	Creating node server07 ... ID: 2818eb53b31234c2cc852b339538d856</span><br><span class="line">		Adding device /dev/sdb ... OK</span><br><span class="line">	Creating node server08 ... ID: d5773e74abd6f3041d07b095a2f4ae99</span><br><span class="line">		Adding device /dev/sdb ... OK</span><br><span class="line">	Creating node server09 ... ID: 4caafd29101b77da4a6cd265dad65140</span><br><span class="line">		Adding device /dev/sdb ... OK</span><br><span class="line"></span><br><span class="line">heketi-cli topology info</span><br><span class="line"></span><br><span class="line">Cluster Id: dde6f33a6cb852e04cb0685e210d3959</span><br><span class="line"></span><br><span class="line">    Volumes:</span><br><span class="line"></span><br><span class="line">    Nodes:</span><br><span class="line"></span><br><span class="line">	Node Id: 2818eb53b31234c2cc852b339538d856</span><br><span class="line">	State: online</span><br><span class="line">	Cluster Id: dde6f33a6cb852e04cb0685e210d3959</span><br><span class="line">	Zone: 1</span><br><span class="line">	Management Hostname: server07</span><br><span class="line">	Storage Hostname: 172.16.170.134</span><br><span class="line">	Devices:</span><br><span class="line">		Id:25bbec25514b711d48d0abef98809310   Name:/dev/sdb            State:online    Size (GiB):19      Used (GiB):0       Free (GiB):19</span><br><span class="line">			Bricks:</span><br><span class="line"></span><br><span class="line">	Node Id: 4caafd29101b77da4a6cd265dad65140</span><br><span class="line">	State: online</span><br><span class="line">	Cluster Id: dde6f33a6cb852e04cb0685e210d3959</span><br><span class="line">	Zone: 1</span><br><span class="line">	Management Hostname: server09</span><br><span class="line">	Storage Hostname: 172.16.170.136</span><br><span class="line">	Devices:</span><br><span class="line">		Id:78385691f170d5ae6aae78448ab7d710   Name:/dev/sdb            State:online    Size (GiB):19      Used (GiB):0       Free (GiB):19</span><br><span class="line">			Bricks:</span><br><span class="line"></span><br><span class="line">	Node Id: d5773e74abd6f3041d07b095a2f4ae99</span><br><span class="line">	State: online</span><br><span class="line">	Cluster Id: dde6f33a6cb852e04cb0685e210d3959</span><br><span class="line">	Zone: 1</span><br><span class="line">	Management Hostname: server08</span><br><span class="line">	Storage Hostname: 172.16.170.135</span><br><span class="line">	Devices:</span><br><span class="line">		Id:2678d65a18f0c21b38288d218d209a3f   Name:/dev/sdb            State:online    Size (GiB):19      Used (GiB):0       Free (GiB):19</span><br><span class="line">			Bricks:</span><br></pre></td></tr></table></figure>
<h2 id="5-编写验证Heketi和GlusterFS集群对接Kubernetes集群的相关YAML："><a href="#5-编写验证Heketi和GlusterFS集群对接Kubernetes集群的相关YAML：" class="headerlink" title="5. 编写验证Heketi和GlusterFS集群对接Kubernetes集群的相关YAML："></a>5. 编写验证Heketi和GlusterFS集群对接Kubernetes集群的相关YAML：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p example/</span><br><span class="line">cd example/</span><br><span class="line"></span><br><span class="line">kubectl get service -n storage</span><br><span class="line">NAME      TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">heketi    ClusterIP   10.96.162.3   &lt;none&gt;        8080/TCP   1h</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 01-glusterfs-storageclass.yaml</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: glusterfs</span><br><span class="line">provisioner: kubernetes.io/glusterfs</span><br><span class="line">parameters:</span><br><span class="line">  resturl: &quot;http://10.96.162.3:8080&quot;</span><br><span class="line">  restauthenabled: &quot;false&quot;</span><br><span class="line">  volumetype: &quot;replicate:3&quot;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 02-persistentvolumeclaim.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: myclaim</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br><span class="line">  storageClassName: glusterfs</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; 03-pod.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: alpine</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - image: alpine:3.8</span><br><span class="line">    command:</span><br><span class="line">    - sleep</span><br><span class="line">    - &quot;3600&quot;</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    name: alpine</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: myvolume</span><br><span class="line">      mountPath: &quot;/data&quot;</span><br><span class="line">  restartPolicy: Always</span><br><span class="line">  volumes:</span><br><span class="line">  - name: myvolume</span><br><span class="line">    persistentVolumeClaim:</span><br><span class="line">      claimName: myclaim</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="6-验证Heketi和GlusterFS集群对接Kubernetes集群："><a href="#6-验证Heketi和GlusterFS集群对接Kubernetes集群：" class="headerlink" title="6. 验证Heketi和GlusterFS集群对接Kubernetes集群："></a>6. 验证Heketi和GlusterFS集群对接Kubernetes集群：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"># 创建StorageClass对接到GlusterFS</span><br><span class="line">kubectl create -f 01-glusterfs-storageclass.yaml</span><br><span class="line">storageclass.storage.k8s.io/glusterfs created</span><br><span class="line"></span><br><span class="line"># 创建PVC使用上面的StorageClass</span><br><span class="line">kubectl create -f 02-persistentvolumeclaim.yaml</span><br><span class="line">persistentvolumeclaim/myclaim created</span><br><span class="line"></span><br><span class="line"># 验证PVC动态供应PV</span><br><span class="line">kubectl get pvc -o wide</span><br><span class="line">NAME      STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">myclaim   Bound     pvc-f910f4b2-239d-11e9-b3b6-000c295a4c4c   1Gi        RWX            glusterfs      1m</span><br><span class="line"></span><br><span class="line">kubectl get pv -o wide</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM             STORAGECLASS   REASON    AGE</span><br><span class="line">pvc-f910f4b2-239d-11e9-b3b6-000c295a4c4c   1Gi        RWX            Delete           Bound     default/myclaim   glusterfs                1m</span><br><span class="line"></span><br><span class="line"># 创建测试Pod使用上面的PVC</span><br><span class="line">kubectl create -f 03-pod.yaml</span><br><span class="line">pod/alpine created</span><br><span class="line"></span><br><span class="line">kubectl get pod -o wide</span><br><span class="line">NAME      READY     STATUS    RESTARTS   AGE       IP            NODE</span><br><span class="line">alpine    1/1       Running   0          5s        10.211.2.10   server08</span><br><span class="line"></span><br><span class="line"># 验证PVC是否挂载到Pod中</span><br><span class="line">kubectl exec -it alpine /bin/sh</span><br><span class="line">/ # df -h</span><br><span class="line">Filesystem                Size      Used Available Use% Mounted on</span><br><span class="line">overlay                  17.0G      4.3G     12.7G  25% /</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /dev</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /sys/fs/cgroup</span><br><span class="line">172.16.170.134:vol_7f5ea2a6035e2f7885687db4ef46d2e3</span><br><span class="line">                       1015.3M     42.8M    982.6M   4% /data</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /dev/termination-log</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /etc/resolv.conf</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /etc/hostname</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /etc/hosts</span><br><span class="line">shm                      64.0M         0     64.0M   0% /dev/shm</span><br><span class="line">tmpfs                   909.8M     12.0K    909.8M   0% /run/secrets/kubernetes.io/serviceaccount</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/kcore</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/timer_list</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/timer_stats</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/sched_debug</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /sys/firmware</span><br><span class="line"></span><br><span class="line"># 创建测试文件，验证存储容量的限制</span><br><span class="line">/data # fallocate -l 982M onebox.disk</span><br><span class="line">/data # df -h</span><br><span class="line">Filesystem                Size      Used Available Use% Mounted on</span><br><span class="line">overlay                  17.0G      4.3G     12.7G  25% /</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /dev</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /sys/fs/cgroup</span><br><span class="line">172.16.170.134:vol_7f5ea2a6035e2f7885687db4ef46d2e3</span><br><span class="line">                       1015.3M      1.0G    572.0K 100% /data</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /dev/termination-log</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /etc/resolv.conf</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /etc/hostname</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /etc/hosts</span><br><span class="line">shm                      64.0M         0     64.0M   0% /dev/shm</span><br><span class="line">tmpfs                   909.8M     12.0K    909.8M   0% /run/secrets/kubernetes.io/serviceaccount</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/kcore</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/timer_list</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/timer_stats</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/sched_debug</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /sys/firmware</span><br><span class="line">/data # fallocate -l 982M onebox.disk</span><br><span class="line">fallocate: fallocate &apos;onebox.disk&apos;: No space left on device</span><br><span class="line"></span><br><span class="line"># 测试文件删除后，验证容量是否恢复</span><br><span class="line">/data # rm -rf onebox.disk</span><br><span class="line">/data # df -h</span><br><span class="line">Filesystem                Size      Used Available Use% Mounted on</span><br><span class="line">overlay                  17.0G      4.3G     12.7G  25% /</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /dev</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /sys/fs/cgroup</span><br><span class="line">172.16.170.134:vol_7f5ea2a6035e2f7885687db4ef46d2e3</span><br><span class="line">                       1015.3M     42.8M    982.6M   4% /data</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /dev/termination-log</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /etc/resolv.conf</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /etc/hostname</span><br><span class="line">/dev/mapper/cl-root      17.0G      4.3G     12.7G  25% /etc/hosts</span><br><span class="line">shm                      64.0M         0     64.0M   0% /dev/shm</span><br><span class="line">tmpfs                   909.8M     12.0K    909.8M   0% /run/secrets/kubernetes.io/serviceaccount</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/kcore</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/timer_list</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/timer_stats</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /proc/sched_debug</span><br><span class="line">tmpfs                   909.8M         0    909.8M   0% /sys/firmware</span><br></pre></td></tr></table></figure>
<h1 id="四、参考资料"><a href="#四、参考资料" class="headerlink" title="四、参考资料"></a>四、参考资料</h1><p><a href="https://github.com/gluster/gluster-kubernetes/tree/v1.1" target="_blank" rel="noopener">https://github.com/gluster/gluster-kubernetes/tree/v1.1</a><br><a href="https://hub.docker.com/r/gluster/gluster-centos/tags" target="_blank" rel="noopener">https://hub.docker.com/r/gluster/gluster-centos/tags</a><br><a href="https://blog.csdn.net/q1403539144/article/details/86566796" target="_blank" rel="noopener">https://blog.csdn.net/q1403539144/article/details/86566796</a><br><a href="http://www.cnitblog.com/xijia0524/archive/2014/04/19/89466.html" target="_blank" rel="noopener">http://www.cnitblog.com/xijia0524/archive/2014/04/19/89466.html</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GlusterFS/">GlusterFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Storage/">Storage</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kubernetes_calico_001" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/kubernetes_calico_001/" class="article-date">
  	<time datetime="2019-05-27T04:33:29.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/kubernetes_calico_001/">
        Calico IPIP（Always和CrossSubnet）模式和BGP模式的区别
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、分析网路配置的区别"><a href="#一、分析网路配置的区别" class="headerlink" title="一、分析网路配置的区别"></a>一、分析网路配置的区别</h1><h2 id="1-IPIP-Always-模式"><a href="#1-IPIP-Always-模式" class="headerlink" title="1. IPIP Always 模式"></a>1. IPIP Always 模式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"># ip link</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:21:eb:a0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default</span><br><span class="line">    link/ether 02:42:2f:64:85:3d brd ff:ff:ff:ff:ff:ff</span><br><span class="line">8: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1440 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">9: cali436bd393c1c@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">10: cali09539dd2c0c@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br><span class="line"># ip address</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:21:eb:a0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.16.170.128/24 brd 172.16.170.255 scope global ens33</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::20c:29ff:fe21:eba0/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default</span><br><span class="line">    link/ether 02:42:2f:64:85:3d brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.17.0.1/16 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">8: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1440 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">    inet 10.211.0.1/32 brd 10.211.0.1 scope global tunl0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">9: cali436bd393c1c@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet6 fe80::ecee:eeff:feee:eeee/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">10: cali09539dd2c0c@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br><span class="line">    inet6 fe80::ecee:eeff:feee:eeee/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"># ip route</span><br><span class="line">default via 172.16.170.2 dev ens33</span><br><span class="line">blackhole 10.211.0.0/24 proto bird</span><br><span class="line">10.211.0.16 dev cali436bd393c1c scope link</span><br><span class="line">10.211.0.17 dev cali09539dd2c0c scope link</span><br><span class="line">10.211.1.0/24 via 172.16.170.129 dev tunl0 proto bird onlink</span><br><span class="line">10.211.2.0/24 via 172.16.170.130 dev tunl0 proto bird onlink</span><br><span class="line">169.254.0.0/16 dev ens33 scope link metric 1002</span><br><span class="line">172.16.170.0/24 dev ens33 proto kernel scope link src 172.16.170.128</span><br><span class="line">172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1</span><br></pre></td></tr></table></figure>
<h2 id="2-IPIP-CrossSubnet-模式"><a href="#2-IPIP-CrossSubnet-模式" class="headerlink" title="2. IPIP CrossSubnet 模式"></a>2. IPIP CrossSubnet 模式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"># ip link</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:21:eb:a0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default</span><br><span class="line">    link/ether 02:42:74:e2:55:16 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">4: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1440 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">5: califb703006b7d@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">6: cali3c997f406a6@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br><span class="line"># ip address</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:21:eb:a0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.16.170.128/24 brd 172.16.170.255 scope global ens33</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::20c:29ff:fe21:eba0/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default</span><br><span class="line">    link/ether 02:42:74:e2:55:16 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.17.0.1/16 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">4: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1440 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">    inet 10.211.0.1/32 brd 10.211.0.1 scope global tunl0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">5: califb703006b7d@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet6 fe80::ecee:eeff:feee:eeee/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">6: cali3c997f406a6@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br><span class="line">    inet6 fe80::ecee:eeff:feee:eeee/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"># ip route</span><br><span class="line">default via 172.16.170.2 dev ens33</span><br><span class="line">blackhole 10.211.0.0/24 proto bird</span><br><span class="line">10.211.0.18 dev califb703006b7d scope link</span><br><span class="line">10.211.0.19 dev cali3c997f406a6 scope link</span><br><span class="line">10.211.1.0/24 via 172.16.170.129 dev ens33 proto bird</span><br><span class="line">10.211.2.0/24 via 172.16.170.130 dev ens33 proto bird</span><br><span class="line">169.254.0.0/16 dev ens33 scope link metric 1002</span><br><span class="line">172.16.170.0/24 dev ens33 proto kernel scope link src 172.16.170.128</span><br><span class="line">172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1</span><br></pre></td></tr></table></figure>
<h2 id="3-BGP-模式"><a href="#3-BGP-模式" class="headerlink" title="3. BGP 模式"></a>3. BGP 模式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"># ip link</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:21:eb:a0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default</span><br><span class="line">    link/ether 02:42:b7:6f:cc:b2 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">6: cali72590f2ff6e@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">7: cali439e67ff763@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br><span class="line"># ip address</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:21:eb:a0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.16.170.128/24 brd 172.16.170.255 scope global ens33</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::20c:29ff:fe21:eba0/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default</span><br><span class="line">    link/ether 02:42:b7:6f:cc:b2 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.17.0.1/16 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">6: cali72590f2ff6e@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet6 fe80::ecee:eeff:feee:eeee/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">7: cali439e67ff763@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br><span class="line">    inet6 fe80::ecee:eeff:feee:eeee/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"># ip route</span><br><span class="line">default via 172.16.170.2 dev ens33</span><br><span class="line">blackhole 10.211.0.0/24 proto bird</span><br><span class="line">10.211.0.22 dev cali72590f2ff6e scope link</span><br><span class="line">10.211.0.23 dev cali439e67ff763 scope link</span><br><span class="line">10.211.1.0/24 via 172.16.170.129 dev ens33 proto bird</span><br><span class="line">10.211.2.0/24 via 172.16.170.130 dev ens33 proto bird</span><br><span class="line">169.254.0.0/16 dev ens33 scope link metric 1002</span><br><span class="line">172.16.170.0/24 dev ens33 proto kernel scope link src 172.16.170.128</span><br><span class="line">172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1</span><br></pre></td></tr></table></figure>
<h1 id="二、区别总结"><a href="#二、区别总结" class="headerlink" title="二、区别总结"></a>二、区别总结</h1><p>Calico的IP Pool包括IPIP模式和BGP模式，其中IPIP模式又包括Always和CrossSubnet。IPIP Always简单说是指，Calico网路的路由的分发始终通过Node上的tunl0隧道实现；IPIP CrossSubnet简单说是指，当两个Pod所在的Node的地址在同一网段时，Calico网路的路由的分发则通过各个Node上的主机网卡实现。当两个Pod所在的Node的地址不在同一网段时，Calico网路的路由的分发才通过Node上的tunl0隧道实现。这种模式是IPIP Always和BGP模式的合体实现。</p>
<h1 id="三、参考资料"><a href="#三、参考资料" class="headerlink" title="三、参考资料"></a>三、参考资料</h1><p>Calico开启BGP模式：<br><a href="http://www.cnblogs.com/jinxj/p/9414830.html" target="_blank" rel="noopener">http://www.cnblogs.com/jinxj/p/9414830.html</a></p>
<p>Calico原理解读：<br><a href="https://blog.csdn.net/ccy19910925/article/details/82423452" target="_blank" rel="noopener">https://blog.csdn.net/ccy19910925/article/details/82423452</a></p>
<p>Calico基本原理和模拟：<br><a href="http://ju.outofmemory.cn/entry/367749" target="_blank" rel="noopener">http://ju.outofmemory.cn/entry/367749</a></p>
<p>calico/node配置文档：<br><a href="https://docs.projectcalico.org/v3.1/reference/node/configuration" target="_blank" rel="noopener">https://docs.projectcalico.org/v3.1/reference/node/configuration</a></p>
<p>Calico IP Pool介绍：<br><a href="https://www.jianshu.com/p/dcad6d74e526" target="_blank" rel="noopener">https://www.jianshu.com/p/dcad6d74e526</a><br><a href="http://www.361way.com/linux-tunnel/5199.html" target="_blank" rel="noopener">http://www.361way.com/linux-tunnel/5199.html</a><br><a href="https://blog.csdn.net/kkdelta/article/details/39611061" target="_blank" rel="noopener">https://blog.csdn.net/kkdelta/article/details/39611061</a></p>
<p>Calico 跨网段问题：<br><a href="https://blog.csdn.net/mailjoin/article/details/79695463" target="_blank" rel="noopener">https://blog.csdn.net/mailjoin/article/details/79695463</a><br><a href="https://www.lijiaocn.com/项目/2017/09/25/calico-ipip.html" target="_blank" rel="noopener">https://www.lijiaocn.com/项目/2017/09/25/calico-ipip.html</a></p>
<p>静态路由配置示例：<br><a href="https://jingyan.baidu.com/article/6dad5075f7c67aa123e36eb9.html" target="_blank" rel="noopener">https://jingyan.baidu.com/article/6dad5075f7c67aa123e36eb9.html</a><br><a href="http://blog.51cto.com/11101034/1906726" target="_blank" rel="noopener">http://blog.51cto.com/11101034/1906726</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Calico/">Calico</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Network/">Network</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kubernetes_build_000" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/kubernetes_build_000/" class="article-date">
  	<time datetime="2019-05-27T04:33:14.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/kubernetes_build_000/">
        如何使用Kubernetes源代码做编译构建
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、操作系统版本"><a href="#一、操作系统版本" class="headerlink" title="一、操作系统版本"></a>一、操作系统版本</h1><p>CentOS 7.6.1810<br>Kubernetes v1.11.0</p>
<h1 id="二、准备工作"><a href="#二、准备工作" class="headerlink" title="二、准备工作"></a>二、准备工作</h1><h2 id="1-直接使用yum安装rsync和gcc"><a href="#1-直接使用yum安装rsync和gcc" class="headerlink" title="1. 直接使用yum安装rsync和gcc"></a>1. 直接使用yum安装rsync和gcc</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y git rsync gcc</span><br></pre></td></tr></table></figure>
<h2 id="2-源码安装golang-1-10-2"><a href="#2-源码安装golang-1-10-2" class="headerlink" title="2. 源码安装golang 1.10.2"></a>2. 源码安装golang 1.10.2</h2><p>详见官方文档</p>
<h2 id="3-克隆源代码到GOPATH的src目录下"><a href="#3-克隆源代码到GOPATH的src目录下" class="headerlink" title="3. 克隆源代码到GOPATH的src目录下"></a>3. 克隆源代码到GOPATH的src目录下</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd $GOPATH/src/</span><br><span class="line">mkdir -p k8s.io/</span><br><span class="line">cd k8s.io/</span><br><span class="line">git clone https://github.com/kubernetes/kubernetes.git</span><br><span class="line">cd kubernetes/</span><br><span class="line">git checkout -b v1.11.0 v1.11.0</span><br></pre></td></tr></table></figure>
<h1 id="三、构建过程记录"><a href="#三、构建过程记录" class="headerlink" title="三、构建过程记录"></a>三、构建过程记录</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">[root@server01 kubernetes]# make clean</span><br><span class="line">+++ [0107 11:01:28] Verifying Prerequisites....</span><br><span class="line">+++ [0107 11:01:28] Removing _output directory</span><br><span class="line">Removing pkg/generated/openapi/zz_generated.openapi.go ..</span><br><span class="line">Removing pkg/generated/bindata.go ..</span><br><span class="line">Removing test/e2e/generated/bindata.go ..</span><br><span class="line">[root@server01 kubernetes]# KUBE_GIT_MAJOR=&quot;1&quot; KUBE_GIT_MINOR=&quot;11&quot; KUBE_GIT_VERSION=&quot;v1.11.0&quot; KUBE_GIT_COMMIT=&quot;91e7b4fd31fcd3d5f436da26c980becec37ceefe&quot; KUBE_GIT_TREE_STATE=&quot;clean&quot; make all</span><br><span class="line">+++ [0107 11:01:57] Building go targets for linux/amd64:</span><br><span class="line">    ./vendor/k8s.io/code-generator/cmd/deepcopy-gen</span><br><span class="line">+++ [0107 11:02:05] Building go targets for linux/amd64:</span><br><span class="line">    ./vendor/k8s.io/code-generator/cmd/defaulter-gen</span><br><span class="line">+++ [0107 11:02:09] Building go targets for linux/amd64:</span><br><span class="line">    ./vendor/k8s.io/code-generator/cmd/conversion-gen</span><br><span class="line">+++ [0107 11:02:13] Building go targets for linux/amd64:</span><br><span class="line">    ./vendor/k8s.io/code-generator/cmd/openapi-gen</span><br><span class="line">+++ [0107 11:02:19] Building go targets for linux/amd64:</span><br><span class="line">    ./vendor/github.com/jteeuwen/go-bindata/go-bindata</span><br><span class="line">+++ [0107 11:02:20] Building go targets for linux/amd64:</span><br><span class="line">    cmd/kube-proxy</span><br><span class="line">    cmd/kube-apiserver</span><br><span class="line">    cmd/kube-controller-manager</span><br><span class="line">    cmd/cloud-controller-manager</span><br><span class="line">    cmd/kubelet</span><br><span class="line">    cmd/kubeadm</span><br><span class="line">    cmd/hyperkube</span><br><span class="line">    cmd/kube-scheduler</span><br><span class="line">    vendor/k8s.io/kube-aggregator</span><br><span class="line">    vendor/k8s.io/apiextensions-apiserver</span><br><span class="line">    cluster/gce/gci/mounter</span><br><span class="line">    cmd/kubectl</span><br><span class="line">    cmd/gendocs</span><br><span class="line">    cmd/genkubedocs</span><br><span class="line">    cmd/genman</span><br><span class="line">    cmd/genyaml</span><br><span class="line">    cmd/genswaggertypedocs</span><br><span class="line">    cmd/linkcheck</span><br><span class="line">    vendor/github.com/onsi/ginkgo/ginkgo</span><br><span class="line">    test/e2e/e2e.test</span><br><span class="line">    cmd/kubemark</span><br><span class="line">    vendor/github.com/onsi/ginkgo/ginkgo</span><br><span class="line">    test/e2e_node/e2e_node.test</span><br><span class="line">[root@server01 kubernetes]# ls -la _output/bin/</span><br><span class="line">总用量 2323024</span><br><span class="line">drwxr-xr-x. 2 root root      4096 1月   7 11:08 .</span><br><span class="line">drwxr-xr-x. 3 root root        19 1月   7 11:01 ..</span><br><span class="line">-rwxr-xr-x. 1 root root  59300539 1月   7 11:08 apiextensions-apiserver</span><br><span class="line">-rwxr-xr-x. 1 root root 138056772 1月   7 11:08 cloud-controller-manager</span><br><span class="line">-rwxr-xr-x. 1 root root   7691655 1月   7 11:02 conversion-gen</span><br><span class="line">-rwxr-xr-x. 1 root root   7687498 1月   7 11:01 deepcopy-gen</span><br><span class="line">-rwxr-xr-x. 1 root root   7665142 1月   7 11:02 defaulter-gen</span><br><span class="line">-rwxr-xr-x. 1 root root 209978368 1月   7 11:08 e2e_node.test</span><br><span class="line">-rwxr-xr-x. 1 root root 173425408 1月   7 11:08 e2e.test</span><br><span class="line">-rwxr-xr-x. 1 root root  54135796 1月   7 11:08 gendocs</span><br><span class="line">-rwxr-xr-x. 1 root root 226087352 1月   7 11:08 genkubedocs</span><br><span class="line">-rwxr-xr-x. 1 root root 232056512 1月   7 11:08 genman</span><br><span class="line">-rwxr-xr-x. 1 root root   5477486 1月   7 11:08 genswaggertypedocs</span><br><span class="line">-rwxr-xr-x. 1 root root  54084772 1月   7 11:08 genyaml</span><br><span class="line">-rwxr-xr-x. 1 root root  10641201 1月   7 11:08 ginkgo</span><br><span class="line">-rwxr-xr-x. 1 root root   2831370 1月   7 11:02 go-bindata</span><br><span class="line">-rwxr-xr-x. 1 root root 227335584 1月   7 11:08 hyperkube</span><br><span class="line">-rwxr-xr-x. 1 root root  57251126 1月   7 11:08 kubeadm</span><br><span class="line">-rwxr-xr-x. 1 root root  57912503 1月   7 11:08 kube-aggregator</span><br><span class="line">-rwxr-xr-x. 1 root root 185160079 1月   7 11:08 kube-apiserver</span><br><span class="line">-rwxr-xr-x. 1 root root 153806893 1月   7 11:08 kube-controller-manager</span><br><span class="line">-rwxr-xr-x. 1 root root  55277901 1月   7 11:08 kubectl</span><br><span class="line">-rwxr-xr-x. 1 root root 162729192 1月   7 11:08 kubelet</span><br><span class="line">-rwxr-xr-x. 1 root root 160066432 1月   7 11:08 kubemark</span><br><span class="line">-rwxr-xr-x. 1 root root  51920351 1月   7 11:08 kube-proxy</span><br><span class="line">-rwxr-xr-x. 1 root root  55479582 1月   7 11:08 kube-scheduler</span><br><span class="line">-rwxr-xr-x. 1 root root   6698678 1月   7 11:08 linkcheck</span><br><span class="line">-rwxr-xr-x. 1 root root   2330265 1月   7 11:08 mounter</span><br><span class="line">-rwxr-xr-x. 1 root root  13634378 1月   7 11:02 openapi-gen</span><br></pre></td></tr></table></figure>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Source-Build/">Source Build</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kubeadm_ceph_rbd" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/kubeadm_ceph_rbd/" class="article-date">
  	<time datetime="2019-05-27T04:32:39.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/kubeadm_ceph_rbd/">
        Kubernetes对接Ceph RBD关键点记录
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Ceph-RBD-External-Provisioner版本"><a href="#Ceph-RBD-External-Provisioner版本" class="headerlink" title="Ceph RBD External Provisioner版本"></a>Ceph RBD External Provisioner版本</h1><p><a href="https://github.com/kubernetes-incubator/external-storage/blob/v5.1.0/ceph/rbd/" target="_blank" rel="noopener">https://github.com/kubernetes-incubator/external-storage/blob/v5.1.0/ceph/rbd/</a></p>
<h1 id="对Linux操作系统的内核版本有要求，经初步测试需要版本为4-x"><a href="#对Linux操作系统的内核版本有要求，经初步测试需要版本为4-x" class="headerlink" title="对Linux操作系统的内核版本有要求，经初步测试需要版本为4.x"></a>对Linux操作系统的内核版本有要求，经初步测试需要版本为4.x</h1><p>CentOS 7.5.1804如何升级操作系统内核？<br><a href="https://www.howtoforge.com/tutorial/how-to-upgrade-kernel-in-centos-7-server/" target="_blank" rel="noopener">https://www.howtoforge.com/tutorial/how-to-upgrade-kernel-in-centos-7-server/</a><br><a href="https://zhuanlan.zhihu.com/p/29617407" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29617407</a></p>
<h1 id="Kubernetes各个Node上需要安装匹配版本的ceph-common"><a href="#Kubernetes各个Node上需要安装匹配版本的ceph-common" class="headerlink" title="Kubernetes各个Node上需要安装匹配版本的ceph-common"></a>Kubernetes各个Node上需要安装匹配版本的ceph-common</h1><p>这里以宿主机操作系统为CentOS 7.5.1804，ceph版本为mimic为例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rpm -Uvh https://download.ceph.com/rpm-mimic/el7/noarch/ceph-release-1-1.el7.noarch.rpm</span><br><span class="line">yum install -y --nogpgcheck ceph-common</span><br></pre></td></tr></table></figure></p>
<p>参考资料：<br><a href="https://github.com/kubernetes-incubator/external-storage/blob/v5.1.0/ceph/rbd/Dockerfile" target="_blank" rel="noopener">https://github.com/kubernetes-incubator/external-storage/blob/v5.1.0/ceph/rbd/Dockerfile</a></p>
<h1 id="StorageClass和Ceph-RBD-External-Provisioner的正常工作依赖于Kubernetes内部的DNS解析"><a href="#StorageClass和Ceph-RBD-External-Provisioner的正常工作依赖于Kubernetes内部的DNS解析" class="headerlink" title="StorageClass和Ceph RBD External Provisioner的正常工作依赖于Kubernetes内部的DNS解析"></a>StorageClass和Ceph RBD External Provisioner的正常工作依赖于Kubernetes内部的DNS解析</h1><ol>
<li><p>配置DNS记录，让Kubernetes集群内部可以把ceph monitor解析到对应的外部地址上；</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: ceph-mon</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  type: ExternalName</span><br><span class="line">  externalName: 172.16.170.134.xip.io # ceph monitor的地址</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置StorageClass，让monitors参数使用1中配置的DNS记录。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">kind: StorageClass</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: managed-cephrbd-storage</span><br><span class="line">provisioner: yonghui.cn/cephrbd</span><br><span class="line">parameters:</span><br><span class="line">  monitors: ceph-mon.kube-system.svc.cluster.local:6789 # 这里需要使用Kubernetes内部的DNS配置ceph monitor的地址</span><br><span class="line">  pool: kube</span><br><span class="line">  adminId: admin</span><br><span class="line">  adminSecretNamespace: kube-system</span><br><span class="line">  adminSecretName: cephrbd-admin-secret</span><br><span class="line">  userId: kube</span><br><span class="line">  userSecretNamespace: kube-system</span><br><span class="line">  userSecretName: cephrbd-secret</span><br><span class="line">  imageFormat: &quot;2&quot;</span><br><span class="line">  imageFeatures: layering</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>推荐参考资料：<br><a href="http://blog.51cto.com/ygqygq2/2163656" target="_blank" rel="noopener">http://blog.51cto.com/ygqygq2/2163656</a><br><a href="https://segmentfault.com/q/1010000011440882" target="_blank" rel="noopener">https://segmentfault.com/q/1010000011440882</a></p>
<h1 id="Ceph-RBD的简单操作记录"><a href="#Ceph-RBD的简单操作记录" class="headerlink" title="Ceph RBD的简单操作记录"></a>Ceph RBD的简单操作记录</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># rbd ls -p kube</span><br><span class="line">kubernetes-dynamic-pvc-4392eab0-fc4c-11e8-a9f7-ee2ca7a031f1</span><br><span class="line"># rbd rm kube/kubernetes-dynamic-pvc-4392eab0-fc4c-11e8-a9f7-ee2ca7a031f1</span><br><span class="line">Removing image: 100% complete...done.</span><br><span class="line"># rbd ls -p kube</span><br></pre></td></tr></table></figure>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Ceph/">Ceph</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Storage/">Storage</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-fluentd_trouble_shooting_001" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/fluentd_trouble_shooting_001/" class="article-date">
  	<time datetime="2019-05-27T04:32:32.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/fluentd_trouble_shooting_001/">
        Fluentd不使用运行时的时区向Elasticsearch写入日志记录，怎么解决？
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、环境版本信息"><a href="#一、环境版本信息" class="headerlink" title="一、环境版本信息"></a>一、环境版本信息</h1><p>Fluentd 1.1.0<br>Elasticsearch 6.3.0<br>fluent-plugin-elasticsearch 2.4.1</p>
<h1 id="二、报错现象描述"><a href="#二、报错现象描述" class="headerlink" title="二、报错现象描述"></a>二、报错现象描述</h1><p>以时区设置为Asia/Shanghai为例：<br>虽然Fluentd所在的运行时已经设置正确，但是上报到Elasticsearch对应索引中的日志记录，与实际相比还是相差八个小时。更直接的现象是，你发现明明应该归档在今天对应的索引中的日志，却归档在了昨天对应的索引中。再有就是你会发现，无论你怎么修改运行时的时区，Fluentd向Elasticsearch写入日志记录都始终使用UTC时区。</p>
<h1 id="三、解决方法"><a href="#三、解决方法" class="headerlink" title="三、解决方法"></a>三、解决方法</h1><p>把utc_index的值设置为false，允许Fluentd使用本地运行时设置的时区。</p>
<p>官方文档的原文描述如下：（详见参考资料部分的链接）<br>By default, the records inserted into index logstash-YYMMDD with UTC (Coordinated Universal Time). This option allows to use local time if you describe utc_index to false.</p>
<p>配置修改示例如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&lt;match **&gt;</span><br><span class="line">  @id elasticsearch</span><br><span class="line">  @type elasticsearch</span><br><span class="line">  @log_level info</span><br><span class="line">  include_tag_key true</span><br><span class="line">  host elasticsearch-ingest.logging</span><br><span class="line">  port 9200</span><br><span class="line">  logstash_format true</span><br><span class="line">  utc_index false</span><br><span class="line">  slow_flush_log_threshold 60.0</span><br><span class="line">  &lt;buffer&gt;</span><br><span class="line">    @type file</span><br><span class="line">    path /var/log/fluentd-buffers/kubernetes.system.buffer</span><br><span class="line">    flush_mode interval</span><br><span class="line">    retry_type exponential_backoff</span><br><span class="line">    flush_thread_count 2</span><br><span class="line">    flush_interval 5s</span><br><span class="line">    retry_forever</span><br><span class="line">    retry_max_interval 30</span><br><span class="line">    chunk_limit_size 6M</span><br><span class="line">    queue_limit_length 256</span><br><span class="line">    overflow_action block</span><br><span class="line">  &lt;/buffer&gt;</span><br><span class="line">&lt;/match&gt;</span><br></pre></td></tr></table></figure></p>
<h1 id="四、参考资料"><a href="#四、参考资料" class="headerlink" title="四、参考资料"></a>四、参考资料</h1><p><a href="https://github.com/uken/fluent-plugin-elasticsearch/tree/v2.4.1#utc_index" target="_blank" rel="noopener">https://github.com/uken/fluent-plugin-elasticsearch/tree/v2.4.1#utc_index</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Fluentd/">Fluentd</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Trouble-Shooting/">Trouble Shooting</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/2/">&laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/4/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2020 Singh Wang
      </div>
        <div class="footer-right">
          <a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/smackgg/hexo-theme-smackdown" target="_blank">Smackdown</a>
        </div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="/js/main.js"></script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js"></script>


  </div>
</body>
</html>