<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  <title>Singh Wang</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Singh Wang">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Singh Wang">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Singh Wang">
  
    <link rel="alternative" href="/atom.xml" title="Singh Wang" type="application/atom+xml">
  
  
    <link rel="icon" href="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg">
  
  <link rel="stylesheet" href="/css/style.css">
  
  

  <script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>
  <script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>

  
</head></html>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="https://avatars0.githubusercontent.com/u/17465385?s=400&amp;u=cea56ed1710ec6ebab160c1ce1e068317e05b025&amp;v=4" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Singh Wang</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/archives">所有文章</a></li>
				        
							<li><a href="/categories/容器云技术/">容器云技术</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/singhwang" title="github">github</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/Calico/" style="font-size: 11.11px;">Calico</a> <a href="/tags/Ceph/" style="font-size: 12.22px;">Ceph</a> <a href="/tags/Common/" style="font-size: 10px;">Common</a> <a href="/tags/DNS/" style="font-size: 10px;">DNS</a> <a href="/tags/Docker/" style="font-size: 20px;">Docker</a> <a href="/tags/Dragonfly/" style="font-size: 10px;">Dragonfly</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Fluentd/" style="font-size: 11.11px;">Fluentd</a> <a href="/tags/GlusterFS/" style="font-size: 13.33px;">GlusterFS</a> <a href="/tags/HAProxy/" style="font-size: 10px;">HAProxy</a> <a href="/tags/Highly-Available/" style="font-size: 10px;">Highly Available</a> <a href="/tags/Ingress/" style="font-size: 10px;">Ingress</a> <a href="/tags/Kubeadm/" style="font-size: 11.11px;">Kubeadm</a> <a href="/tags/Kubernetes/" style="font-size: 18.89px;">Kubernetes</a> <a href="/tags/Label/" style="font-size: 10px;">Label</a> <a href="/tags/Logging/" style="font-size: 12.22px;">Logging</a> <a href="/tags/Monitoring/" style="font-size: 13.33px;">Monitoring</a> <a href="/tags/Network/" style="font-size: 14.44px;">Network</a> <a href="/tags/Open-Falcon/" style="font-size: 10px;">Open Falcon</a> <a href="/tags/P2P/" style="font-size: 10px;">P2P</a> <a href="/tags/Prometheus/" style="font-size: 11.11px;">Prometheus</a> <a href="/tags/Setup/" style="font-size: 15.56px;">Setup</a> <a href="/tags/Smartping/" style="font-size: 10px;">Smartping</a> <a href="/tags/Source-Build/" style="font-size: 10px;">Source Build</a> <a href="/tags/Storage/" style="font-size: 16.67px;">Storage</a> <a href="/tags/TimeZone/" style="font-size: 10px;">TimeZone</a> <a href="/tags/Trouble-Shooting/" style="font-size: 17.78px;">Trouble Shooting</a>
					</div>
				</section>
				
				
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">Singh Wang</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="https://avatars0.githubusercontent.com/u/17465385?s=400&amp;u=cea56ed1710ec6ebab160c1ce1e068317e05b025&amp;v=4" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author">Singh Wang</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/archives">所有文章</a></li>
		        
					<li><a href="/categories/容器云技术/">容器云技术</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/singhwang" title="github">github</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap">
  
    <article id="post-binary_kubernetes_cluster_000" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/31/binary_kubernetes_cluster_000/" class="article-date">
  	<time datetime="2019-05-31T05:34:31.000Z" itemprop="datePublished">2019-05-31</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/31/binary_kubernetes_cluster_000/">
        使用二进制文件的方式安装 Kubernetes v1.11.0 集群（一）
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、实现方式介绍"><a href="#一、实现方式介绍" class="headerlink" title="一、实现方式介绍"></a>一、实现方式介绍</h1><h2 id="1-控制平面的所有组件都采用二进制方式部署，交由systemd统一管理；"><a href="#1-控制平面的所有组件都采用二进制方式部署，交由systemd统一管理；" class="headerlink" title="1. 控制平面的所有组件都采用二进制方式部署，交由systemd统一管理；"></a>1. 控制平面的所有组件都采用二进制方式部署，交由systemd统一管理；</h2><p>控制平面包括：etcd、kube-apiserver、kube-controller-manager和kube-scheduler。</p>
<h2 id="2-网络插件依然采用Addon方式部署，交由kubernetes统一管理。"><a href="#2-网络插件依然采用Addon方式部署，交由kubernetes统一管理。" class="headerlink" title="2. 网络插件依然采用Addon方式部署，交由kubernetes统一管理。"></a>2. 网络插件依然采用Addon方式部署，交由kubernetes统一管理。</h2><p>网络插件包括：calico-node及其相关的组件。</p>
<h1 id="二、实验环境版本信息"><a href="#二、实验环境版本信息" class="headerlink" title="二、实验环境版本信息"></a>二、实验环境版本信息</h1><h2 id="1-操作系统的版本信息"><a href="#1-操作系统的版本信息" class="headerlink" title="1. 操作系统的版本信息"></a>1. 操作系统的版本信息</h2><p>CentOS Linux release 7.6.1810 (Core)</p>
<h2 id="2-各组件的版本信息"><a href="#2-各组件的版本信息" class="headerlink" title="2. 各组件的版本信息"></a>2. 各组件的版本信息</h2><p>etcd v3.2.18<br>kube-apiserver v1.11.0<br>kube-controller-manager v1.11.0<br>kube-scheduler v1.11.0<br>kubectl v1.11.0</p>
<p>docker 17.03.1-ce<br>kubelet v1.11.0<br>calico v3.1.3</p>
<h1 id="三、部署架构"><a href="#三、部署架构" class="headerlink" title="三、部署架构"></a>三、部署架构</h1><h2 id="1-Kubernetes-Master（Control-Plane）"><a href="#1-Kubernetes-Master（Control-Plane）" class="headerlink" title="1. Kubernetes Master（Control Plane）"></a>1. Kubernetes Master（Control Plane）</h2><p>192.168.112.128 master -&gt; etcd kube-apiserver kube-controller-manager kube-scheduler</p>
<h2 id="2-Kubernetes-Node"><a href="#2-Kubernetes-Node" class="headerlink" title="2. Kubernetes Node"></a>2. Kubernetes Node</h2><p>192.168.112.134 node01 -&gt; docker kubelet kube-proxy calico-node<br>192.168.112.135 node02 -&gt; docker kubelet kube-proxy calico-node</p>
<h1 id="四、准备二进制文件与Docker镜像"><a href="#四、准备二进制文件与Docker镜像" class="headerlink" title="四、准备二进制文件与Docker镜像"></a>四、准备二进制文件与Docker镜像</h1><h2 id="1-下载相关的二进制文件压缩包"><a href="#1-下载相关的二进制文件压缩包" class="headerlink" title="1. 下载相关的二进制文件压缩包"></a>1. 下载相关的二进制文件压缩包</h2><p><a href="https://github.com/etcd-io/etcd/releases/download/v3.2.18/etcd-v3.2.18-linux-amd64.tar.gz" target="_blank" rel="noopener">https://github.com/etcd-io/etcd/releases/download/v3.2.18/etcd-v3.2.18-linux-amd64.tar.gz</a><br><a href="https://dl.k8s.io/v1.11.0/kubernetes-server-linux-amd64.tar.gz" target="_blank" rel="noopener">https://dl.k8s.io/v1.11.0/kubernetes-server-linux-amd64.tar.gz</a></p>
<h2 id="2-拉取相关的Docker镜像"><a href="#2-拉取相关的Docker镜像" class="headerlink" title="2. 拉取相关的Docker镜像"></a>2. 拉取相关的Docker镜像</h2><p>calico网络组件的镜像：<br>calico/cni:v3.1.3<br>calico/node:v3.1.3<br>calico/typha:v0.7.4</p>
<p>kube-dns的镜像：<br>registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.1.3</p>
<p>infra容器的镜像：<br>registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1</p>
<h1 id="五、部署过程记录"><a href="#五、部署过程记录" class="headerlink" title="五、部署过程记录"></a>五、部署过程记录</h1><h2 id="1-准备基础环境（Master和Node上都执行）"><a href="#1-准备基础环境（Master和Node上都执行）" class="headerlink" title="1. 准备基础环境（Master和Node上都执行）"></a>1. 准备基础环境（Master和Node上都执行）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"># 更新系统</span><br><span class="line">yum update -y</span><br><span class="line"></span><br><span class="line"># 设置正确的时区和时间</span><br><span class="line">yum install -y ntpdate</span><br><span class="line">timedatectl set-timezone Asia/Shanghai</span><br><span class="line">ntpdate cn.ntp.org.cn</span><br><span class="line"></span><br><span class="line"># 关闭防火墙</span><br><span class="line">systemctl disable firewalld.service</span><br><span class="line">systemctl stop firewalld.service</span><br><span class="line"></span><br><span class="line"># 关闭swap分区</span><br><span class="line">swapoff -a</span><br><span class="line">sed -i &apos;s#/dev/mapper/cl-swap#\# /dev/mapper/cl-swap#&apos; /etc/fstab</span><br><span class="line"></span><br><span class="line"># 关闭selinux</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i &apos;s/SELINUX=enforcing/SELINUX=disabled/&apos; /etc/selinux/config</span><br><span class="line"></span><br><span class="line"># 设置各个节点的主机名</span><br><span class="line">## 192.168.112.128</span><br><span class="line">hostnamectl set-hostname master</span><br><span class="line"></span><br><span class="line">## 192.168.112.129</span><br><span class="line">hostnamectl set-hostname node01</span><br><span class="line"></span><br><span class="line">## 192.168.112.130</span><br><span class="line">hostnamectl set-hostname node02</span><br><span class="line"></span><br><span class="line"># 配置主机名和IP的映射</span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; /etc/hosts</span><br><span class="line"></span><br><span class="line"># For Kubernetes Cluster</span><br><span class="line">192.168.112.128 master</span><br><span class="line">192.168.112.129 node01</span><br><span class="line">192.168.112.130 node02</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 修改内核参数</span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/sysctl.d/kubernetes.conf</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line"></span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_timestamps = 1</span><br><span class="line">net.ipv4.tcp_tw_recycle = 1 </span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_fin_timeout = 30</span><br><span class="line">net.ipv4.tcp_tw_resue = 1</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sysctl --system</span><br><span class="line"></span><br><span class="line"># 修改ulimit限制</span><br><span class="line">cat &lt;&lt;EOF &gt;  /etc/security/limits.d/kubernetes.conf</span><br><span class="line">* hard nofile 65535</span><br><span class="line">* soft nofile 65535</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="2-安装Docker环境（在所有Node上都执行）"><a href="#2-安装Docker环境（在所有Node上都执行）" class="headerlink" title="2. 安装Docker环境（在所有Node上都执行）"></a>2. 安装Docker环境（在所有Node上都执行）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">yum install -y yum-utils device-mapper-persistent-data lvm2</span><br><span class="line">yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><br><span class="line">yum makecache fast</span><br><span class="line"></span><br><span class="line">yum install -y https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch.rpm</span><br><span class="line"></span><br><span class="line">yum install -y docker-ce-17.03.1.ce-1.el7.centos</span><br><span class="line"></span><br><span class="line">systemctl enable docker.service</span><br><span class="line">systemctl start docker.service</span><br><span class="line">systemctl status docker.service</span><br><span class="line"></span><br><span class="line">docker version</span><br></pre></td></tr></table></figure>
<h2 id="3-复制所有二进制文件到操作系统-usr-bin-目录下"><a href="#3-复制所有二进制文件到操作系统-usr-bin-目录下" class="headerlink" title="3. 复制所有二进制文件到操作系统/usr/bin/目录下"></a>3. 复制所有二进制文件到操作系统/usr/bin/目录下</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 192.168.112.128 （Master上执行）</span><br><span class="line">tar -zxvf etcd-v3.2.18-linux-amd64.tar.gz</span><br><span class="line">tar -zxvf kubernetes-server-linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line">cp etcd-v3.2.18-linux-amd64/etcd /usr/bin/</span><br><span class="line">cp etcd-v3.2.18-linux-amd64/etcdctl /usr/bin/</span><br><span class="line">cp kubernetes/server/bin/kube-apiserver /usr/bin/</span><br><span class="line">cp kubernetes/server/bin/kube-controller-manager /usr/bin/</span><br><span class="line">cp kubernetes/server/bin/kube-scheduler /usr/bin/</span><br><span class="line">cp kubernetes/server/bin/kubectl /usr/bin/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 192.168.112.129 和 192.168.112.130 （Node上执行）</span><br><span class="line">tar -zxvf kubernetes-server-linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line">cp kubernetes/server/bin/kubelet /usr/bin/</span><br><span class="line">cp kubernetes/server/bin/kube-proxy /usr/bin/</span><br></pre></td></tr></table></figure>
<h1 id="4-在Master上生成所有组件的相关证书和配置文件"><a href="#4-在Master上生成所有组件的相关证书和配置文件" class="headerlink" title="4. 在Master上生成所有组件的相关证书和配置文件"></a>4. 在Master上生成所有组件的相关证书和配置文件</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"># 创建证书和配置文件的存放目录</span><br><span class="line">mkdir -p /etc/kubernetes/pki/</span><br><span class="line"></span><br><span class="line"># 生成rsa的公钥和私钥</span><br><span class="line">cd /etc/kubernetes/</span><br><span class="line">openssl genrsa -out sa.key 2048</span><br><span class="line">openssl rsa -in sa.key -pubout -out sa.pub</span><br><span class="line"></span><br><span class="line"># 进入证书目录</span><br><span class="line">cd /etc/kubernetes/pki/</span><br><span class="line"></span><br><span class="line"># 生成根证书</span><br><span class="line">openssl genrsa -out ca.key 2048</span><br><span class="line">openssl req -x509 -new -nodes -key ca.key -subj &quot;/CN=master&quot; -days 5000 -out ca.crt</span><br><span class="line"></span><br><span class="line"># 为kube-apiserver生成相关的证书和配置文件</span><br><span class="line">cat &lt;&lt;EOF &gt; master_ssl.cnf</span><br><span class="line">[req]</span><br><span class="line">req_extensions = v3_req</span><br><span class="line">distinguished_name = req_distinguished_name</span><br><span class="line"></span><br><span class="line">[req_distinguished_name]</span><br><span class="line">[v3_req]</span><br><span class="line">basicConstraints = CA:FALSE</span><br><span class="line">keyUsage = nonRepudiation,digitalSignature,keyEncipherment</span><br><span class="line">subjectAltName = @alt_names</span><br><span class="line">[alt_names]</span><br><span class="line">DNS.1 = kubernetes</span><br><span class="line">DNS.2 = kubernetes.default</span><br><span class="line">DNS.3 = kubernetes.default.svc</span><br><span class="line">DNS.4 = kubernetes.default.svc.cluster.local</span><br><span class="line">DNS.5 = master</span><br><span class="line">IP.1 = 10.96.0.1</span><br><span class="line">IP.2 = 192.168.112.128</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">openssl genrsa -out apiserver.key 2048</span><br><span class="line">openssl req -new -key apiserver.key -subj &quot;/CN=master&quot; -config master_ssl.cnf -out apiserver.csr</span><br><span class="line">openssl x509 -req -in apiserver.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days 5000 -extensions v3_req -extfile master_ssl.cnf -out apiserver.crt</span><br><span class="line"></span><br><span class="line"># 为kube-controller-manager生成相关的证书和配置文件</span><br><span class="line">openssl genrsa -out controller-manager.key 2048</span><br><span class="line">openssl req -new -key controller-manager.key -subj &quot;/CN=master&quot; -out controller-manager.csr</span><br><span class="line">openssl x509 -req -in controller-manager.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out controller-manager.crt -days 5000</span><br><span class="line"></span><br><span class="line">export KUBECONFIG=/etc/kubernetes/controller-manager.conf</span><br><span class="line">kubectl config set-cluster kubernetes --server=https://192.168.112.128:6443 --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true</span><br><span class="line">kubectl config set-credentials system:kube-controller-manager --client-certificate=/etc/kubernetes/pki/controller-manager.crt --client-key=/etc/kubernetes/pki/controller-manager.key --embed-certs=true</span><br><span class="line">kubectl config set-context system:kube-controller-manager@kubernetes --cluster=kubernetes --user=system:kube-controller-manager</span><br><span class="line">kubectl config use-context system:kube-controller-manager@kubernetes</span><br><span class="line">unset KUBECONFIG</span><br><span class="line"></span><br><span class="line"># 为kube-scheduler生成相关的证书和配置文件</span><br><span class="line">openssl genrsa -out scheduler.key 2048</span><br><span class="line">openssl req -new -key scheduler.key -subj &quot;/CN=master&quot; -out scheduler.csr</span><br><span class="line">openssl x509 -req -in scheduler.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out scheduler.crt -days 5000</span><br><span class="line"></span><br><span class="line">export KUBECONFIG=/etc/kubernetes/scheduler.conf</span><br><span class="line">kubectl config set-cluster kubernetes --server=https://192.168.112.128:6443 --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true</span><br><span class="line">kubectl config set-credentials system:kube-scheduler --client-certificate=/etc/kubernetes/pki/scheduler.crt --client-key=/etc/kubernetes/pki/scheduler.key --embed-certs=true</span><br><span class="line">kubectl config set-context system:kube-scheduler@kubernetes --cluster=kubernetes --user=system:kube-scheduler</span><br><span class="line">kubectl config use-context system:kube-scheduler@kubernetes</span><br><span class="line">unset KUBECONFIG</span><br><span class="line"></span><br><span class="line"># 为kubectl生成相关的证书和配置文件</span><br><span class="line">openssl genrsa -out kubectl.key 2048</span><br><span class="line">openssl req -new -key kubectl.key -subj &quot;/CN=master&quot; -out kubectl.csr</span><br><span class="line">openssl x509 -req -in kubectl.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out kubectl.crt -days 5000</span><br><span class="line"></span><br><span class="line">export KUBECONFIG=/etc/kubernetes/admin.conf</span><br><span class="line">kubectl config set-cluster kubernetes --server=https://192.168.112.128:6443 --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true</span><br><span class="line">kubectl config set-credentials system:admin --client-certificate=/etc/kubernetes/pki/kubectl.crt --client-key=/etc/kubernetes/pki/kubectl.key --embed-certs=true</span><br><span class="line">kubectl config set-context system:admin@kubernetes --cluster=kubernetes --user=system:admin</span><br><span class="line">kubectl config use-context system:admin@kubernetes</span><br><span class="line">unset KUBECONFIG</span><br><span class="line"></span><br><span class="line"># 为kubelet生成相关的证书和配置文件</span><br><span class="line">openssl genrsa -out kubelet.key 2048</span><br><span class="line">openssl req -new -key kubelet.key -subj &quot;/CN=node&quot; -out kubelet.csr</span><br><span class="line">openssl x509 -req -in kubelet.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out kubelet.crt -days 5000</span><br><span class="line"></span><br><span class="line">export KUBECONFIG=/etc/kubernetes/kubelet.conf</span><br><span class="line">kubectl config set-cluster kubernetes --server=https://192.168.112.128:6443 --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true</span><br><span class="line">kubectl config set-credentials system:kubelet --client-certificate=/etc/kubernetes/pki/kubelet.crt --client-key=/etc/kubernetes/pki/kubelet.key --embed-certs=true</span><br><span class="line">kubectl config set-context system:kubelet@kubernetes --cluster=kubernetes --user=system:kubelet</span><br><span class="line">kubectl config use-context system:kubelet@kubernetes</span><br><span class="line">unset KUBECONFIG</span><br><span class="line"></span><br><span class="line"># 为kube-proxy生成相关的证书和配置文件</span><br><span class="line">openssl genrsa -out proxy.key 2048</span><br><span class="line">openssl req -new -key proxy.key -subj &quot;/CN=node&quot; -out proxy.csr</span><br><span class="line">openssl x509 -req -in proxy.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out proxy.crt -days 5000</span><br><span class="line"></span><br><span class="line">export KUBECONFIG=/etc/kubernetes/proxy.conf</span><br><span class="line">kubectl config set-cluster kubernetes --server=https://192.168.112.128:6443 --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true</span><br><span class="line">kubectl config set-credentials system:proxy --client-certificate=/etc/kubernetes/pki/proxy.crt --client-key=/etc/kubernetes/pki/proxy.key --embed-certs=true</span><br><span class="line">kubectl config set-context system:proxy@kubernetes --cluster=kubernetes --user=system:proxy</span><br><span class="line">kubectl config use-context system:proxy@kubernetes</span><br><span class="line">unset KUBECONFIG</span><br></pre></td></tr></table></figure>
<h2 id="5-配置和启动Master上的所有组件"><a href="#5-配置和启动Master上的所有组件" class="headerlink" title="5. 配置和启动Master上的所有组件"></a>5. 配置和启动Master上的所有组件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"># 配置和启动etcd服务</span><br><span class="line">mkdir -p /var/lib/etcd/</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /usr/lib/systemd/system/etcd.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Etcd Server </span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=simple</span><br><span class="line">WorkingDirectory=/var/lib/etcd/</span><br><span class="line">EnvironmentFile=-/etc/etcd/etcd.conf</span><br><span class="line">ExecStart=/usr/bin/etcd</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable etcd.service</span><br><span class="line">systemctl start etcd.service</span><br><span class="line">systemctl status etcd.service</span><br><span class="line"></span><br><span class="line"># 配置和启动kube-apiserver服务</span><br><span class="line">cat &lt;&lt;EOF &gt; /usr/lib/systemd/system/kube-apiserver.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes API Server</span><br><span class="line">Documentation=https://github.com/kubernetes/kubernetes</span><br><span class="line">After=etcd.service</span><br><span class="line">Wants=etcd.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=-/etc/kubernetes/kube-apiserver</span><br><span class="line">ExecStart=/usr/bin/kube-apiserver \$KUBE_API_ARGS</span><br><span class="line">Restart=on-failure</span><br><span class="line">Type=notify</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/kubernetes/kube-apiserver</span><br><span class="line">KUBE_API_ARGS=&quot;--storage-backend=etcd3 --etcd-servers=http://127.0.0.1:2379 --allow-privileged=true --client-ca-file=/etc/kubernetes/pki/ca.crt --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key --service-account-key-file=/etc/kubernetes/sa.pub --advertise-address=192.168.112.128 --secure-port=6443 --insecure-bind-address=0.0.0.0 --insecure-port=8080 --service-cluster-ip-range=10.96.0.0/16  --service-node-port-range=30000-32767 --authorization-mode=Node,RBAC  --logtostderr=false --log-dir=/var/log/kubernetes --v=2 --enable-admission-plugins=ResourceQuota,LimitRanger&quot;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-apiserver.service</span><br><span class="line">systemctl start kube-apiserver.service</span><br><span class="line">systemctl status kube-apiserver.service</span><br><span class="line"></span><br><span class="line"># 为第4步中涉及的用户master和node绑定cluster-admin角色</span><br><span class="line">kubectl create clusterrolebinding system:component:master --clusterrole=cluster-admin --user=master</span><br><span class="line">kubectl create clusterrolebinding system:component:node --clusterrole=cluster-admin --user=node</span><br><span class="line"></span><br><span class="line"># 配置和启动kube-controller-manager服务</span><br><span class="line">cat &lt;&lt;EOF &gt; /usr/lib/systemd/system/kube-controller-manager.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Controller Manager</span><br><span class="line">Documentation=https://github.com/kubernetes/kubernetes</span><br><span class="line">After=kube-apiserver.service</span><br><span class="line">Requires=kube-apiserver.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=-/etc/kubernetes/kube-controller-manager</span><br><span class="line">ExecStart=/usr/bin/kube-controller-manager \$KUBE_CONTROLLER_MANAGER_ARGS</span><br><span class="line">Restart=on-failure</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/kubernetes/kube-controller-manager</span><br><span class="line">KUBE_CONTROLLER_MANAGER_ARGS=&quot;--service-account-private-key-file=/etc/kubernetes/sa.key  --leader-elect=true --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --root-ca-file=/etc/kubernetes/pki/ca.crt --kubeconfig=/etc/kubernetes/controller-manager.conf --allocate-node-cidrs=true  --cluster-cidr=10.211.0.0/16 --logtostderr=false --log-dir=/var/log/kubernetes --v=2&quot;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-controller-manager.service</span><br><span class="line">systemctl start kube-controller-manager.service</span><br><span class="line">systemctl status kube-controller-manager.service</span><br><span class="line"></span><br><span class="line"># 配置和启动kube-scheduler服务</span><br><span class="line">cat &lt;&lt;EOF &gt; /usr/lib/systemd/system/kube-scheduler.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Scheduler</span><br><span class="line">Documentation=https://github.com/kubernetes/kubernetes</span><br><span class="line">After=kube-apiserver.service</span><br><span class="line">Requires=kube-apiserver.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=-/etc/kubernetes/kube-scheduler</span><br><span class="line">ExecStart=/usr/bin/kube-scheduler \$KUBE_SCHEDULER_ARGS</span><br><span class="line">Restart=on-failure</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/kubernetes/kube-scheduler</span><br><span class="line">KUBE_SCHEDULER_ARGS=&quot;--kubeconfig=/etc/kubernetes/scheduler.conf --leader-elect=true --logtostderr=false --log-dir=/var/log/kubernetes --v=2&quot;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-scheduler.service</span><br><span class="line">systemctl start kube-scheduler.service</span><br><span class="line">systemctl status kube-scheduler.service</span><br></pre></td></tr></table></figure>
<h2 id="5-配置和启动Node上的所有组件"><a href="#5-配置和启动Node上的所有组件" class="headerlink" title="5. 配置和启动Node上的所有组件"></a>5. 配置和启动Node上的所有组件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"># 创建配置目录和工作目录</span><br><span class="line">mkdir -p /etc/kubernetes</span><br><span class="line">mkdir -p /var/lib/kubelet</span><br><span class="line"></span><br><span class="line"># 传输相关配置文件到当前节点上</span><br><span class="line">scp root@192.168.112.128:/etc/kubernetes/kubelet.conf /etc/kubernetes/</span><br><span class="line">scp root@192.168.112.128:/etc/kubernetes/proxy.conf /etc/kubernetes/</span><br><span class="line"></span><br><span class="line"># 配置和启动kubelet服务</span><br><span class="line">cat &lt;&lt;EOF &gt; /usr/lib/systemd/system/kubelet.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Kubelet Server</span><br><span class="line">Documentation=https://github.com/kubernetes/kubernetes</span><br><span class="line">After=docker.service</span><br><span class="line">Requires=docker.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=/var/lib/kubelet</span><br><span class="line">EnvironmentFile=-/etc/kubernetes/kubelet</span><br><span class="line">ExecStart=/usr/bin/kubelet \$KUBELET_ARGS</span><br><span class="line">Restart=on-failure</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/kubernetes/kubelet</span><br><span class="line">KUBELET_ARGS=&quot;--kubeconfig=/etc/kubernetes/kubelet.conf --cluster-dns=10.96.0.10 --cluster-domain=cluster.local --logtostderr=false --log-dir=/var/log/kubernetes --v=2 --cgroup-driver=cgroupfs --cni-bin-dir=/opt/cni/bin --cni-conf-dir=/etc/cni/net.d --network-plugin=cni --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1&quot;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kubelet.service</span><br><span class="line">systemctl start kubelet.service</span><br><span class="line">systemctl status kubelet.service</span><br><span class="line"></span><br><span class="line"># 配置和启动kube-proxy服务</span><br><span class="line">mkdir -p /var/lib/kube-proxy/</span><br><span class="line">cat &lt;&lt;EOF &gt; /var/lib/kube-proxy/config.conf</span><br><span class="line">apiVersion: kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line">bindAddress: 0.0.0.0</span><br><span class="line">clientConnection:</span><br><span class="line">  acceptContentTypes: &quot;&quot;</span><br><span class="line">  burst: 10</span><br><span class="line">  contentType: application/vnd.kubernetes.protobuf</span><br><span class="line">  kubeconfig: /etc/kubernetes/proxy.conf</span><br><span class="line">  qps: 5</span><br><span class="line">clusterCIDR: 10.211.0.0/16</span><br><span class="line">configSyncPeriod: 15m0s</span><br><span class="line">conntrack:</span><br><span class="line">  max: null</span><br><span class="line">  maxPerCore: 32768</span><br><span class="line">  min: 131072</span><br><span class="line">  tcpCloseWaitTimeout: 1h0m0s</span><br><span class="line">  tcpEstablishedTimeout: 24h0m0s</span><br><span class="line">enableProfiling: false</span><br><span class="line">healthzBindAddress: 0.0.0.0:10256</span><br><span class="line">hostnameOverride: &quot;&quot;</span><br><span class="line">iptables:</span><br><span class="line">  masqueradeAll: false</span><br><span class="line">  masqueradeBit: 14</span><br><span class="line">  minSyncPeriod: 0s</span><br><span class="line">  syncPeriod: 30s</span><br><span class="line">ipvs:</span><br><span class="line">  excludeCIDRs: null</span><br><span class="line">  minSyncPeriod: 0s</span><br><span class="line">  scheduler: &quot;&quot;</span><br><span class="line">  syncPeriod: 30s</span><br><span class="line">kind: KubeProxyConfiguration</span><br><span class="line">metricsBindAddress: 127.0.0.1:10249</span><br><span class="line">mode: iptables</span><br><span class="line">nodePortAddresses: null</span><br><span class="line">oomScoreAdj: -999</span><br><span class="line">portRange: &quot;&quot;</span><br><span class="line">resourceContainer: /kube-proxy</span><br><span class="line">udpIdleTimeout: 250ms</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /usr/lib/systemd/system/kube-proxy.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Proxy Server</span><br><span class="line">Documentation=https://github.com/kubernetes/kubernetes</span><br><span class="line">After=network.target</span><br><span class="line">Requires=network.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=-/etc/kubernetes/kube-proxy</span><br><span class="line">ExecStart=/usr/bin/kube-proxy \$KUBE_PROXY_ARGS</span><br><span class="line">Restart=on-failure</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/kubernetes/kube-proxy</span><br><span class="line">KUBE_PROXY_ARGS=&quot;--config=/var/lib/kube-proxy/config.conf --logtostderr=false --log-dir=/var/log/kubernetes --v=2&quot;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-proxy.service</span><br><span class="line">systemctl start kube-proxy.service</span><br><span class="line">systemctl status kube-proxy.service</span><br></pre></td></tr></table></figure>
<h2 id="6-配置和安装网络插件"><a href="#6-配置和安装网络插件" class="headerlink" title="6. 配置和安装网络插件"></a>6. 配置和安装网络插件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p calico</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; calico/rbac-kdd.yaml</span><br><span class="line"># Calico Version v3.1.3</span><br><span class="line"># https://docs.projectcalico.org/v3.1/releases#v3.1.3</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-node</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - namespaces</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - pods/status</span><br><span class="line">    verbs:</span><br><span class="line">      - update</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - pods</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">      - patch</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - services</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - endpoints</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - nodes</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - update</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups: [&quot;extensions&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - networkpolicies</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups: [&quot;networking.k8s.io&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - networkpolicies</span><br><span class="line">    verbs:</span><br><span class="line">      - watch</span><br><span class="line">      - list</span><br><span class="line">  - apiGroups: [&quot;crd.projectcalico.org&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - globalfelixconfigs</span><br><span class="line">      - felixconfigurations</span><br><span class="line">      - bgppeers</span><br><span class="line">      - globalbgpconfigs</span><br><span class="line">      - bgpconfigurations</span><br><span class="line">      - ippools</span><br><span class="line">      - globalnetworkpolicies</span><br><span class="line">      - globalnetworksets</span><br><span class="line">      - networkpolicies</span><br><span class="line">      - clusterinformations</span><br><span class="line">      - hostendpoints</span><br><span class="line">    verbs:</span><br><span class="line">      - create</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - update</span><br><span class="line">      - watch</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-node</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: calico-node</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: calico-node</span><br><span class="line">  namespace: kube-system</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; calico/calico.yaml</span><br><span class="line"># Calico Version v3.1.3</span><br><span class="line"># https://docs.projectcalico.org/v3.1/releases#v3.1.3</span><br><span class="line"># This manifest includes the following component versions:</span><br><span class="line">#   calico/node:v3.1.3</span><br><span class="line">#   calico/cni:v3.1.3</span><br><span class="line"></span><br><span class="line"># This ConfigMap is used to configure a self-hosted Calico installation.</span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-config</span><br><span class="line">  namespace: kube-system</span><br><span class="line">data:</span><br><span class="line">  # To enable Typha, set this to &quot;calico-typha&quot; *and* set a non-zero value for Typha replicas</span><br><span class="line">  # below.  We recommend using Typha if you have more than 50 nodes. Above 100 nodes it is</span><br><span class="line">  # essential.</span><br><span class="line">  typha_service_name: &quot;none&quot;</span><br><span class="line"></span><br><span class="line">  # The CNI network configuration to install on each node.</span><br><span class="line">  cni_network_config: |-</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;k8s-pod-network&quot;,</span><br><span class="line">      &quot;cniVersion&quot;: &quot;0.3.0&quot;,</span><br><span class="line">      &quot;plugins&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;type&quot;: &quot;calico&quot;,</span><br><span class="line">          &quot;log_level&quot;: &quot;info&quot;,</span><br><span class="line">          &quot;datastore_type&quot;: &quot;kubernetes&quot;,</span><br><span class="line">          &quot;nodename&quot;: &quot;__KUBERNETES_NODE_NAME__&quot;,</span><br><span class="line">          &quot;mtu&quot;: 1500,</span><br><span class="line">          &quot;ipam&quot;: &#123;</span><br><span class="line">            &quot;type&quot;: &quot;host-local&quot;,</span><br><span class="line">            &quot;subnet&quot;: &quot;usePodCidr&quot;</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;policy&quot;: &#123;</span><br><span class="line">            &quot;type&quot;: &quot;k8s&quot;</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;kubernetes&quot;: &#123;</span><br><span class="line">            &quot;kubeconfig&quot;: &quot;__KUBECONFIG_FILEPATH__&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;type&quot;: &quot;portmap&quot;,</span><br><span class="line">          &quot;snat&quot;: true,</span><br><span class="line">          &quot;capabilities&quot;: &#123;&quot;portMappings&quot;: true&#125;</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"># This manifest creates a Service, which will be backed by Calico&apos;s Typha daemon.</span><br><span class="line"># Typha sits in between Felix and the API server, reducing Calico&apos;s load on the API server.</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-typha</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: calico-typha</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">    - port: 5473</span><br><span class="line">      protocol: TCP</span><br><span class="line">      targetPort: calico-typha</span><br><span class="line">      name: calico-typha</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: calico-typha</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"># This manifest creates a Deployment of Typha to back the above service.</span><br><span class="line"></span><br><span class="line">apiVersion: apps/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-typha</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: calico-typha</span><br><span class="line">spec:</span><br><span class="line">  # Number of Typha replicas.  To enable Typha, set this to a non-zero value *and* set the</span><br><span class="line">  # typha_service_name variable in the calico-config ConfigMap above.</span><br><span class="line">  #</span><br><span class="line">  # We recommend using Typha if you have more than 50 nodes.  Above 100 nodes it is essential</span><br><span class="line">  # (when using the Kubernetes datastore).  Use one replica for every 100-200 nodes.  In</span><br><span class="line">  # production, we recommend running at least 3 replicas to reduce the impact of rolling upgrade.</span><br><span class="line">  replicas: 0</span><br><span class="line">  revisionHistoryLimit: 2</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: calico-typha</span><br><span class="line">      annotations:</span><br><span class="line">        # This, along with the CriticalAddonsOnly toleration below, marks the pod as a critical</span><br><span class="line">        # add-on, ensuring it gets priority scheduling and that its resources are reserved</span><br><span class="line">        # if it ever gets evicted.</span><br><span class="line">        scheduler.alpha.kubernetes.io/critical-pod: &apos;&apos;</span><br><span class="line">    spec:</span><br><span class="line">      hostNetwork: true</span><br><span class="line">      tolerations:</span><br><span class="line">        # Mark the pod as a critical add-on for rescheduling.</span><br><span class="line">        - key: CriticalAddonsOnly</span><br><span class="line">          operator: Exists</span><br><span class="line">      # Since Calico can&apos;t network a pod until Typha is up, we need to run Typha itself</span><br><span class="line">      # as a host-networked pod.</span><br><span class="line">      serviceAccountName: calico-node</span><br><span class="line">      containers:</span><br><span class="line">      - image: calico/typha:v0.7.4</span><br><span class="line">        name: calico-typha</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 5473</span><br><span class="line">          name: calico-typha</span><br><span class="line">          protocol: TCP</span><br><span class="line">        env:</span><br><span class="line">          # Enable &quot;info&quot; logging by default.  Can be set to &quot;debug&quot; to increase verbosity.</span><br><span class="line">          - name: TYPHA_LOGSEVERITYSCREEN</span><br><span class="line">            value: &quot;info&quot;</span><br><span class="line">          # Disable logging to file and syslog since those don&apos;t make sense in Kubernetes.</span><br><span class="line">          - name: TYPHA_LOGFILEPATH</span><br><span class="line">            value: &quot;none&quot;</span><br><span class="line">          - name: TYPHA_LOGSEVERITYSYS</span><br><span class="line">            value: &quot;none&quot;</span><br><span class="line">          # Monitor the Kubernetes API to find the number of running instances and rebalance</span><br><span class="line">          # connections.</span><br><span class="line">          - name: TYPHA_CONNECTIONREBALANCINGMODE</span><br><span class="line">            value: &quot;kubernetes&quot;</span><br><span class="line">          - name: TYPHA_DATASTORETYPE</span><br><span class="line">            value: &quot;kubernetes&quot;</span><br><span class="line">          - name: TYPHA_HEALTHENABLED</span><br><span class="line">            value: &quot;true&quot;</span><br><span class="line">          # Uncomment these lines to enable prometheus metrics.  Since Typha is host-networked,</span><br><span class="line">          # this opens a port on the host, which may need to be secured.</span><br><span class="line">          #- name: TYPHA_PROMETHEUSMETRICSENABLED</span><br><span class="line">          #  value: &quot;true&quot;</span><br><span class="line">          #- name: TYPHA_PROMETHEUSMETRICSPORT</span><br><span class="line">          #  value: &quot;9093&quot;</span><br><span class="line">        livenessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /liveness</span><br><span class="line">            port: 9098</span><br><span class="line">          periodSeconds: 30</span><br><span class="line">          initialDelaySeconds: 30</span><br><span class="line">        readinessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /readiness</span><br><span class="line">            port: 9098</span><br><span class="line">          periodSeconds: 10</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: tz-config</span><br><span class="line">          mountPath: /etc/localtime</span><br><span class="line">          readOnly: true</span><br><span class="line">      volumes:</span><br><span class="line">      - name: tz-config</span><br><span class="line">        hostPath:</span><br><span class="line">          path: /etc/localtime</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"># This manifest installs the calico/node container, as well</span><br><span class="line"># as the Calico CNI plugins and network config on</span><br><span class="line"># each master and worker node in a Kubernetes cluster.</span><br><span class="line">kind: DaemonSet</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-node</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: calico-node</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: calico-node</span><br><span class="line">  updateStrategy:</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">    rollingUpdate:</span><br><span class="line">      maxUnavailable: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: calico-node</span><br><span class="line">      annotations:</span><br><span class="line">        # This, along with the CriticalAddonsOnly toleration below,</span><br><span class="line">        # marks the pod as a critical add-on, ensuring it gets</span><br><span class="line">        # priority scheduling and that its resources are reserved</span><br><span class="line">        # if it ever gets evicted.</span><br><span class="line">        scheduler.alpha.kubernetes.io/critical-pod: &apos;&apos;</span><br><span class="line">    spec:</span><br><span class="line">      hostNetwork: true</span><br><span class="line">      tolerations:</span><br><span class="line">        # Make sure calico/node gets scheduled on all nodes.</span><br><span class="line">        - effect: NoSchedule</span><br><span class="line">          operator: Exists</span><br><span class="line">        # Mark the pod as a critical add-on for rescheduling.</span><br><span class="line">        - key: CriticalAddonsOnly</span><br><span class="line">          operator: Exists</span><br><span class="line">        - effect: NoExecute</span><br><span class="line">          operator: Exists</span><br><span class="line">      serviceAccountName: calico-node</span><br><span class="line">      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a &quot;force</span><br><span class="line">      # deletion&quot;: https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.</span><br><span class="line">      terminationGracePeriodSeconds: 0</span><br><span class="line">      containers:</span><br><span class="line">        # Runs calico/node container on each Kubernetes node.  This</span><br><span class="line">        # container programs network policy and routes on each</span><br><span class="line">        # host.</span><br><span class="line">        - name: calico-node</span><br><span class="line">          image: calico/node:v3.1.3</span><br><span class="line">          env:</span><br><span class="line">            # Use Kubernetes API as the backing datastore.</span><br><span class="line">            - name: DATASTORE_TYPE</span><br><span class="line">              value: &quot;kubernetes&quot;</span><br><span class="line">            # Enable felix info logging.</span><br><span class="line">            - name: FELIX_LOGSEVERITYSCREEN</span><br><span class="line">              value: &quot;info&quot;</span><br><span class="line">            # Cluster type to identify the deployment type</span><br><span class="line">            - name: CLUSTER_TYPE</span><br><span class="line">              value: &quot;k8s,bgp&quot;</span><br><span class="line">            # Disable file logging so \`kubectl logs\` works.</span><br><span class="line">            - name: CALICO_DISABLE_FILE_LOGGING</span><br><span class="line">              value: &quot;true&quot;</span><br><span class="line">            # Set Felix endpoint to host default action to ACCEPT.</span><br><span class="line">            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION</span><br><span class="line">              value: &quot;ACCEPT&quot;</span><br><span class="line">            # Disable IPV6 on Kubernetes.</span><br><span class="line">            - name: FELIX_IPV6SUPPORT</span><br><span class="line">              value: &quot;false&quot;</span><br><span class="line">            # Set MTU for tunnel device used if ipip is enabled</span><br><span class="line">            - name: FELIX_IPINIPMTU</span><br><span class="line">              value: &quot;1440&quot;</span><br><span class="line">            # Wait for the datastore.</span><br><span class="line">            - name: WAIT_FOR_DATASTORE</span><br><span class="line">              value: &quot;true&quot;</span><br><span class="line">            # The default IPv4 pool to create on startup if none exists. Pod IPs will be</span><br><span class="line">            # chosen from this range. Changing this value after installation will have</span><br><span class="line">            # no effect. This should fall within \`--cluster-cidr\`.</span><br><span class="line">            - name: CALICO_IPV4POOL_CIDR</span><br><span class="line">              value: &quot;10.211.0.0/16&quot;</span><br><span class="line">            # Enable IPIP</span><br><span class="line">            - name: CALICO_IPV4POOL_IPIP</span><br><span class="line">              value: &quot;CrossSubnet&quot;</span><br><span class="line">            # Enable IP-in-IP within Felix.</span><br><span class="line">            - name: FELIX_IPINIPENABLED</span><br><span class="line">              value: &quot;true&quot;</span><br><span class="line">            # Typha support: controlled by the ConfigMap.</span><br><span class="line">            - name: FELIX_TYPHAK8SSERVICENAME</span><br><span class="line">              valueFrom:</span><br><span class="line">                configMapKeyRef:</span><br><span class="line">                  name: calico-config</span><br><span class="line">                  key: typha_service_name</span><br><span class="line">            # Set based on the k8s node name.</span><br><span class="line">            - name: NODENAME</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: spec.nodeName</span><br><span class="line">            # Auto-detect the BGP IP address.</span><br><span class="line">            - name: IP</span><br><span class="line">              value: &quot;autodetect&quot;</span><br><span class="line">            - name: FELIX_HEALTHENABLED</span><br><span class="line">              value: &quot;true&quot;</span><br><span class="line">          securityContext:</span><br><span class="line">            privileged: true</span><br><span class="line">          resources:</span><br><span class="line">            requests:</span><br><span class="line">              cpu: 250m</span><br><span class="line">          livenessProbe:</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /liveness</span><br><span class="line">              port: 9099</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            initialDelaySeconds: 10</span><br><span class="line">            failureThreshold: 6</span><br><span class="line">          readinessProbe:</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /readiness</span><br><span class="line">              port: 9099</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - mountPath: /lib/modules</span><br><span class="line">              name: lib-modules</span><br><span class="line">              readOnly: true</span><br><span class="line">            - mountPath: /var/run/calico</span><br><span class="line">              name: var-run-calico</span><br><span class="line">              readOnly: false</span><br><span class="line">            - mountPath: /var/lib/calico</span><br><span class="line">              name: var-lib-calico</span><br><span class="line">              readOnly: false</span><br><span class="line">            - name: tz-config</span><br><span class="line">              mountPath: /etc/localtime</span><br><span class="line">              readOnly: true</span><br><span class="line">        # This container installs the Calico CNI binaries</span><br><span class="line">        # and CNI network config file on each node.</span><br><span class="line">        - name: install-cni</span><br><span class="line">          image: calico/cni:v3.1.3</span><br><span class="line">          command: [&quot;/install-cni.sh&quot;]</span><br><span class="line">          env:</span><br><span class="line">            # Name of the CNI config file to create.</span><br><span class="line">            - name: CNI_CONF_NAME</span><br><span class="line">              value: &quot;10-calico.conflist&quot;</span><br><span class="line">            # The CNI network config to install on each node.</span><br><span class="line">            - name: CNI_NETWORK_CONFIG</span><br><span class="line">              valueFrom:</span><br><span class="line">                configMapKeyRef:</span><br><span class="line">                  name: calico-config</span><br><span class="line">                  key: cni_network_config</span><br><span class="line">            # Set the hostname based on the k8s node name.</span><br><span class="line">            - name: KUBERNETES_NODE_NAME</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: spec.nodeName</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - mountPath: /host/opt/cni/bin</span><br><span class="line">              name: cni-bin-dir</span><br><span class="line">            - mountPath: /host/etc/cni/net.d</span><br><span class="line">              name: cni-net-dir</span><br><span class="line">            - name: tz-config</span><br><span class="line">              mountPath: /etc/localtime</span><br><span class="line">              readOnly: true</span><br><span class="line">      volumes:</span><br><span class="line">        # Used by calico/node.</span><br><span class="line">        - name: lib-modules</span><br><span class="line">          hostPath:</span><br><span class="line">            path: /lib/modules</span><br><span class="line">        - name: var-run-calico</span><br><span class="line">          hostPath:</span><br><span class="line">            path: /var/run/calico</span><br><span class="line">        - name: var-lib-calico</span><br><span class="line">          hostPath:</span><br><span class="line">            path: /var/lib/calico</span><br><span class="line">        # Used to install CNI.</span><br><span class="line">        - name: cni-bin-dir</span><br><span class="line">          hostPath:</span><br><span class="line">            path: /opt/cni/bin</span><br><span class="line">        - name: cni-net-dir</span><br><span class="line">          hostPath:</span><br><span class="line">            path: /etc/cni/net.d</span><br><span class="line">        - name: tz-config</span><br><span class="line">          hostPath:</span><br><span class="line">            path: /etc/localtime</span><br><span class="line"></span><br><span class="line"># Create all the CustomResourceDefinitions needed for</span><br><span class="line"># Calico policy and networking mode.</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">   name: felixconfigurations.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: FelixConfiguration</span><br><span class="line">    plural: felixconfigurations</span><br><span class="line">    singular: felixconfiguration</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: bgppeers.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: BGPPeer</span><br><span class="line">    plural: bgppeers</span><br><span class="line">    singular: bgppeer</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: bgpconfigurations.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: BGPConfiguration</span><br><span class="line">    plural: bgpconfigurations</span><br><span class="line">    singular: bgpconfiguration</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: ippools.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: IPPool</span><br><span class="line">    plural: ippools</span><br><span class="line">    singular: ippool</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: hostendpoints.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: HostEndpoint</span><br><span class="line">    plural: hostendpoints</span><br><span class="line">    singular: hostendpoint</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: clusterinformations.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: ClusterInformation</span><br><span class="line">    plural: clusterinformations</span><br><span class="line">    singular: clusterinformation</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: globalnetworkpolicies.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: GlobalNetworkPolicy</span><br><span class="line">    plural: globalnetworkpolicies</span><br><span class="line">    singular: globalnetworkpolicy</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: globalnetworksets.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: GlobalNetworkSet</span><br><span class="line">    plural: globalnetworksets</span><br><span class="line">    singular: globalnetworkset</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: networkpolicies.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Namespaced</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: NetworkPolicy</span><br><span class="line">    plural: networkpolicies</span><br><span class="line">    singular: networkpolicy</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-node</span><br><span class="line">  namespace: kube-system</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl create -f calico/</span><br></pre></td></tr></table></figure>
<h2 id="7-配置和安装DNS插件"><a href="#7-配置和安装DNS插件" class="headerlink" title="7. 配置和安装DNS插件"></a>7. 配置和安装DNS插件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p kube-dns/</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; kube-dns/deployment.yaml</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kube-dns</span><br><span class="line">  name: coredns</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  revisionHistoryLimit: 10</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: kube-dns</span><br><span class="line">  strategy:</span><br><span class="line">    rollingUpdate:</span><br><span class="line">      maxSurge: 25%</span><br><span class="line">      maxUnavailable: 1</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: kube-dns</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - args:</span><br><span class="line">        - -conf</span><br><span class="line">        - /etc/coredns/Corefile</span><br><span class="line">        image: registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.1.3</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        livenessProbe:</span><br><span class="line">          failureThreshold: 5</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /health</span><br><span class="line">            port: 8080</span><br><span class="line">            scheme: HTTP</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          periodSeconds: 10</span><br><span class="line">          successThreshold: 1</span><br><span class="line">          timeoutSeconds: 5</span><br><span class="line">        name: coredns</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 53</span><br><span class="line">          name: dns</span><br><span class="line">          protocol: UDP</span><br><span class="line">        - containerPort: 53</span><br><span class="line">          name: dns-tcp</span><br><span class="line">          protocol: TCP</span><br><span class="line">        - containerPort: 9153</span><br><span class="line">          name: metrics</span><br><span class="line">          protocol: TCP</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            memory: 170Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 100m</span><br><span class="line">            memory: 70Mi</span><br><span class="line">        securityContext:</span><br><span class="line">          allowPrivilegeEscalation: false</span><br><span class="line">          capabilities:</span><br><span class="line">            add:</span><br><span class="line">            - NET_BIND_SERVICE</span><br><span class="line">            drop:</span><br><span class="line">            - all</span><br><span class="line">          readOnlyRootFilesystem: true</span><br><span class="line">        terminationMessagePath: /dev/termination-log</span><br><span class="line">        terminationMessagePolicy: File</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: /etc/coredns</span><br><span class="line">          name: config-volume</span><br><span class="line">          readOnly: true</span><br><span class="line">        - mountPath: /etc/localtime</span><br><span class="line">          name: tz-config</span><br><span class="line">          readOnly: true</span><br><span class="line">      dnsPolicy: Default</span><br><span class="line">      restartPolicy: Always</span><br><span class="line">      schedulerName: default-scheduler</span><br><span class="line">      securityContext: &#123;&#125;</span><br><span class="line">      serviceAccount: coredns</span><br><span class="line">      serviceAccountName: coredns</span><br><span class="line">      terminationGracePeriodSeconds: 30</span><br><span class="line">      tolerations:</span><br><span class="line">      - key: CriticalAddonsOnly</span><br><span class="line">        operator: Exists</span><br><span class="line">      - effect: NoSchedule</span><br><span class="line">        key: node-role.kubernetes.io/master</span><br><span class="line">      volumes:</span><br><span class="line">      - configMap:</span><br><span class="line">          defaultMode: 420</span><br><span class="line">          items:</span><br><span class="line">          - key: Corefile</span><br><span class="line">            path: Corefile</span><br><span class="line">          name: coredns</span><br><span class="line">        name: config-volume</span><br><span class="line">      - hostPath:</span><br><span class="line">          path: /etc/localtime</span><br><span class="line">          type: &quot;&quot;</span><br><span class="line">        name: tz-config</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; kube-dns/service.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    prometheus.io/scrape: &quot;true&quot;</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kube-dns</span><br><span class="line">    kubernetes.io/cluster-service: &quot;true&quot;</span><br><span class="line">    kubernetes.io/name: KubeDNS</span><br><span class="line">  name: kube-dns</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  clusterIP: 10.96.0.10</span><br><span class="line">  ports:</span><br><span class="line">  - name: dns</span><br><span class="line">    port: 53</span><br><span class="line">    protocol: UDP</span><br><span class="line">    targetPort: 53</span><br><span class="line">  - name: dns-tcp</span><br><span class="line">    port: 53</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: 53</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: kube-dns</span><br><span class="line">  sessionAffinity: None</span><br><span class="line">  type: ClusterIP</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; kube-dns/configmap.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: coredns</span><br><span class="line">  namespace: kube-system</span><br><span class="line">data:</span><br><span class="line">  Corefile: |</span><br><span class="line">    .:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        health</span><br><span class="line">        kubernetes cluster.local in-addr.arpa ip6.arpa &#123;</span><br><span class="line">           pods insecure</span><br><span class="line">           upstream</span><br><span class="line">           fallthrough in-addr.arpa ip6.arpa</span><br><span class="line">        &#125;</span><br><span class="line">        prometheus :9153</span><br><span class="line">        proxy . /etc/resolv.conf</span><br><span class="line">        cache 30</span><br><span class="line">        reload</span><br><span class="line">    &#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; kube-dns/serviceaccount.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: coredns</span><br><span class="line">  namespace: kube-system</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; kube-dns/clusterrole.yaml</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: system:coredns</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - &quot;&quot;</span><br><span class="line">  resources:</span><br><span class="line">  - endpoints</span><br><span class="line">  - services</span><br><span class="line">  - pods</span><br><span class="line">  - namespaces</span><br><span class="line">  verbs:</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; kube-dns/clusterrolebinding.yaml</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: system:coredns</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:coredns</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: coredns</span><br><span class="line">  namespace: kube-system</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl create -f kube-dns/</span><br></pre></td></tr></table></figure>
<h2 id="7-验证Service的访问"><a href="#7-验证Service的访问" class="headerlink" title="7. 验证Service的访问"></a>7. 验证Service的访问</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p nginx/</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; nginx/01-deployment.yaml</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.7.9</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; nginx/02-service.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  ports:</span><br><span class="line">    - nodePort: 31073</span><br><span class="line">      protocol: TCP</span><br><span class="line">      port: 80</span><br><span class="line">      targetPort: 80</span><br><span class="line">  type: NodePort</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 有响应视为正常，否则视为不正常</span><br><span class="line">curl -XGET http://192.168.112.129:31073</span><br><span class="line">curl -XGET http://192.168.112.130:31073</span><br></pre></td></tr></table></figure>
<h2 id="8-验证Pod的网络和DNS配置"><a href="#8-验证Pod的网络和DNS配置" class="headerlink" title="8. 验证Pod的网络和DNS配置"></a>8. 验证Pod的网络和DNS配置</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line"># 在node01节点和node02节点上分别操作</span><br><span class="line">mkdir -p network/</span><br><span class="line">cd network/</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; Dockerfile</span><br><span class="line">FROM alpine:3.8</span><br><span class="line"></span><br><span class="line">MAINTAINER wangxin_0611@126.com</span><br><span class="line"></span><br><span class="line">RUN apk add --no-cache ca-certificates bind-tools iputils iproute2 net-tools tcpdump</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">docker build -t alpine:3.8-network .</span><br><span class="line"></span><br><span class="line"># 在master节点上操作</span><br><span class="line">mkdir -p network/</span><br><span class="line">cd network/</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; network.yaml</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: network</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: network</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: network</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: network</span><br><span class="line">        image: alpine:3.8-network</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        command:</span><br><span class="line">        - sleep</span><br><span class="line">        - &quot;3600&quot;</span><br><span class="line">      restartPolicy: Always</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl create -f network/</span><br><span class="line"></span><br><span class="line">[root@master ~]# kubectl get pod -o wide</span><br><span class="line">NAME            READY     STATUS    RESTARTS   AGE       IP           NODE</span><br><span class="line">network-bl7jx   1/1       Running   0          6m        10.211.1.4   node02</span><br><span class="line">network-m2vp6   1/1       Running   0          6m        10.211.0.4   node01</span><br><span class="line"></span><br><span class="line"># 在node01上的pod中验证</span><br><span class="line">[root@master ~]# kubectl exec -it network-m2vp6 /bin/sh</span><br><span class="line">/ # cat /etc/resolv.conf</span><br><span class="line">nameserver 10.96.0.10</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br><span class="line">/ # ip address</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">4: eth0@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether 72:01:49:fa:fb:f3 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 10.211.0.4/32 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::7001:49ff:fefa:fbf3/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">/ # ping -c 4 10.211.1.4</span><br><span class="line">PING 10.211.1.4 (10.211.1.4) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.1.4: icmp_seq=1 ttl=62 time=0.314 ms</span><br><span class="line">64 bytes from 10.211.1.4: icmp_seq=2 ttl=62 time=0.490 ms</span><br><span class="line">64 bytes from 10.211.1.4: icmp_seq=3 ttl=62 time=0.415 ms</span><br><span class="line">64 bytes from 10.211.1.4: icmp_seq=4 ttl=62 time=0.491 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.1.4 ping statistics ---</span><br><span class="line">4 packets transmitted, 4 received, 0% packet loss, time 3000ms</span><br><span class="line">rtt min/avg/max/mdev = 0.314/0.427/0.491/0.075 ms</span><br><span class="line">/ # nslookup kubernetes.default</span><br><span class="line">Server:   10.96.0.10</span><br><span class="line">Address:  10.96.0.10#53</span><br><span class="line"></span><br><span class="line">Name: kubernetes.default.svc.cluster.local</span><br><span class="line">Address: 10.96.0.1</span><br><span class="line"></span><br><span class="line">/ # nslookup kubernetes</span><br><span class="line">Server:   10.96.0.10</span><br><span class="line">Address:  10.96.0.10#53</span><br><span class="line"></span><br><span class="line">Name: kubernetes.default.svc.cluster.local</span><br><span class="line">Address: 10.96.0.1</span><br><span class="line"></span><br><span class="line">/ # exit</span><br><span class="line"></span><br><span class="line"># 在node02上的pod中验证</span><br><span class="line">[root@master ~]# kubectl get pod -o wide</span><br><span class="line">NAME            READY     STATUS    RESTARTS   AGE       IP           NODE</span><br><span class="line">network-bl7jx   1/1       Running   0          9m        10.211.1.4   node02</span><br><span class="line">network-m2vp6   1/1       Running   0          9m        10.211.0.4   node01</span><br><span class="line">[root@master ~]# kubectl exec -it network-bl7jx /bin/sh</span><br><span class="line">/ # cat /etc/resolv.conf</span><br><span class="line">nameserver 10.96.0.10</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br><span class="line">/ # ip address</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">4: eth0@if9: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether 5a:a6:51:22:9d:2b brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 10.211.1.4/32 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::58a6:51ff:fe22:9d2b/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">/ # ping -c 4 10.211.0.4</span><br><span class="line">PING 10.211.0.4 (10.211.0.4) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.0.4: icmp_seq=1 ttl=62 time=0.450 ms</span><br><span class="line">64 bytes from 10.211.0.4: icmp_seq=2 ttl=62 time=0.685 ms</span><br><span class="line">64 bytes from 10.211.0.4: icmp_seq=3 ttl=62 time=0.726 ms</span><br><span class="line">64 bytes from 10.211.0.4: icmp_seq=4 ttl=62 time=0.707 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.0.4 ping statistics ---</span><br><span class="line">4 packets transmitted, 4 received, 0% packet loss, time 3003ms</span><br><span class="line">rtt min/avg/max/mdev = 0.450/0.642/0.726/0.111 ms</span><br><span class="line">/ # nslookup kubernetes.default</span><br><span class="line">Server:   10.96.0.10</span><br><span class="line">Address:  10.96.0.10#53</span><br><span class="line"></span><br><span class="line">Name: kubernetes.default.svc.cluster.local</span><br><span class="line">Address: 10.96.0.1</span><br><span class="line"></span><br><span class="line">/ # nslookup kubernetes</span><br><span class="line">Server:   10.96.0.10</span><br><span class="line">Address:  10.96.0.10#53</span><br><span class="line"></span><br><span class="line">Name: kubernetes.default.svc.cluster.local</span><br><span class="line">Address: 10.96.0.1</span><br><span class="line"></span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Setup/">Setup</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-verify_network" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/verify_network/" class="article-date">
  	<time datetime="2019-05-27T04:37:10.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/verify_network/">
        Kubernetes集群中验证集群网络
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在Kubernetes集群中验证网络是否好用，一般从以下几个方面入手：</p>
<ol>
<li>相同主机上的不同Pod之间的网络连通性；</li>
<li>不同主机上的不同Pod之间的网络连通性；</li>
<li>不同主机上的Pod中能否解析Kubernetes集群中的DNS记录。</li>
</ol>
<p>为了顺利完成上述验证，需要准备一个验证环境。下面先介绍一下，如何构建这样一个网络验证环境，然后再介绍如何验证网络。</p>
<h1 id="一、构建集群网络验证环境"><a href="#一、构建集群网络验证环境" class="headerlink" title="一、构建集群网络验证环境"></a>一、构建集群网络验证环境</h1><h2 id="1-构建网络验证环境基础镜像"><a href="#1-构建网络验证环境基础镜像" class="headerlink" title="1. 构建网络验证环境基础镜像"></a>1. 构建网络验证环境基础镜像</h2><h3 id="Alpine-3-8-Dockerfile-Example"><a href="#Alpine-3-8-Dockerfile-Example" class="headerlink" title="Alpine 3.8 Dockerfile Example"></a>Alpine 3.8 Dockerfile Example</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM alpine:3.8</span><br><span class="line"></span><br><span class="line">MAINTAINER wangxin_0611@126.com</span><br><span class="line"></span><br><span class="line">RUN apk add --no-cache ca-certificates bind-tools iputils iproute2 net-tools tcpdump</span><br></pre></td></tr></table></figure>
<h3 id="Ubuntu-16-04-Dockerfile-Example"><a href="#Ubuntu-16-04-Dockerfile-Example" class="headerlink" title="Ubuntu 16.04 Dockerfile Example"></a>Ubuntu 16.04 Dockerfile Example</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">FROM ubuntu:16.04</span><br><span class="line"></span><br><span class="line">MAINTAINER wangxin_0611@126.com</span><br><span class="line"></span><br><span class="line">RUN apt-get update &amp;&amp; \</span><br><span class="line">    apt-get install -y iproute2 &amp;&amp; \</span><br><span class="line">    apt-get install -y dnsutils &amp;&amp; \</span><br><span class="line">    apt-get install -y net-tools &amp;&amp; \</span><br><span class="line">    apt-get install -y iputils-ping &amp;&amp; \</span><br><span class="line">    apt-get install -y tcpdump</span><br></pre></td></tr></table></figure>
<h3 id="CentOS-7-5-1804-Dockerfile-Example"><a href="#CentOS-7-5-1804-Dockerfile-Example" class="headerlink" title="CentOS 7.5.1804 Dockerfile Example"></a>CentOS 7.5.1804 Dockerfile Example</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">FROM centos:7.5.1804</span><br><span class="line"></span><br><span class="line">MAINTAINER wangxin_0611@126.com</span><br><span class="line"></span><br><span class="line">RUN yum makecache fast &amp;&amp; \</span><br><span class="line">    yum install -y iproute &amp;&amp; \</span><br><span class="line">    yum install -y bind-utils &amp;&amp; \</span><br><span class="line">    yum install -y net-tools &amp;&amp; \</span><br><span class="line">    yum install -y iputils &amp;&amp; \</span><br><span class="line">    yum install -y tcpdump</span><br></pre></td></tr></table></figure>
<h2 id="2-利用基础镜像在各个宿主机上部署一个Pod，对于该种情况，利用DaemonSet实现最为合适。"><a href="#2-利用基础镜像在各个宿主机上部署一个Pod，对于该种情况，利用DaemonSet实现最为合适。" class="headerlink" title="2. 利用基础镜像在各个宿主机上部署一个Pod，对于该种情况，利用DaemonSet实现最为合适。"></a>2. 利用基础镜像在各个宿主机上部署一个Pod，对于该种情况，利用DaemonSet实现最为合适。</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: network</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: network</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: network</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: network</span><br><span class="line">        image: 10.0.55.126/base/alpine:3.8-network</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        command:</span><br><span class="line">        - sleep</span><br><span class="line">        - &quot;3600&quot;</span><br><span class="line">      restartPolicy: Always</span><br><span class="line">      tolerations:</span><br><span class="line">      - effect: NoSchedule</span><br><span class="line">        operator: Exists</span><br></pre></td></tr></table></figure>
<p>创建后，查看DaemonSet和其对应的Pod信息如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@master alpine]# kubectl get daemonsets</span><br><span class="line">NAME             DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span><br><span class="line">network          9         9         9         9            9           &lt;none&gt;          1m</span><br><span class="line">[root@master alpine]# kubectl get pods -o wide</span><br><span class="line">NAME                   READY     STATUS    RESTARTS   AGE       IP              NODE</span><br><span class="line">network-76xzk          1/1       Running   0          1m        10.211.3.42     node03</span><br><span class="line">network-79dzf          1/1       Running   0          1m        10.211.2.195    node02</span><br><span class="line">network-ftn7g          1/1       Running   0          1m        10.211.4.25     node04</span><br><span class="line">network-jbr8g          1/1       Running   0          1m        10.211.13.187   node07</span><br><span class="line">network-kflgv          1/1       Running   0          1m        10.211.0.153    master</span><br><span class="line">network-mvqlx          1/1       Running   0          1m        10.211.14.97    node08</span><br><span class="line">network-nbzsc          1/1       Running   0          1m        10.211.12.94    node06</span><br><span class="line">network-rxc2f          1/1       Running   0          1m        10.211.6.5      node05</span><br><span class="line">network-w89xg          1/1       Running   0          1m        10.211.1.240    node01</span><br></pre></td></tr></table></figure></p>
<h1 id="二、验证集群网络"><a href="#二、验证集群网络" class="headerlink" title="二、验证集群网络"></a>二、验证集群网络</h1><h2 id="1-进入master上对应的Pod后，ping各个Node上对应的Pod的IP地址，以此验证连通性；"><a href="#1-进入master上对应的Pod后，ping各个Node上对应的Pod的IP地址，以此验证连通性；" class="headerlink" title="1. 进入master上对应的Pod后，ping各个Node上对应的Pod的IP地址，以此验证连通性；"></a>1. 进入master上对应的Pod后，ping各个Node上对应的Pod的IP地址，以此验证连通性；</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@master alpine]# kubectl exec -it network-kflgv /bin/sh</span><br><span class="line">/ # ping -c 4 10.211.1.240</span><br><span class="line">PING 10.211.1.240 (10.211.1.240) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.1.240: icmp_seq=1 ttl=62 time=0.575 ms</span><br><span class="line">64 bytes from 10.211.1.240: icmp_seq=2 ttl=62 time=0.374 ms</span><br><span class="line">64 bytes from 10.211.1.240: icmp_seq=3 ttl=62 time=0.445 ms</span><br><span class="line">64 bytes from 10.211.1.240: icmp_seq=4 ttl=62 time=0.380 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.1.240 ping statistics ---</span><br><span class="line">4 packets transmitted, 4 received, 0% packet loss, time 3103ms</span><br><span class="line">rtt min/avg/max/mdev = 0.374/0.443/0.575/0.083 ms</span><br><span class="line">/ # ping -c 4 10.211.2.195</span><br><span class="line">PING 10.211.2.195 (10.211.2.195) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.2.195: icmp_seq=1 ttl=62 time=0.390 ms</span><br><span class="line">64 bytes from 10.211.2.195: icmp_seq=2 ttl=62 time=0.544 ms</span><br><span class="line">64 bytes from 10.211.2.195: icmp_seq=3 ttl=62 time=0.460 ms</span><br><span class="line">64 bytes from 10.211.2.195: icmp_seq=4 ttl=62 time=0.483 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.2.195 ping statistics ---</span><br><span class="line">4 packets transmitted, 4 received, 0% packet loss, time 3064ms</span><br><span class="line">rtt min/avg/max/mdev = 0.390/0.469/0.544/0.057 ms</span><br><span class="line">/ # ping -c 4 10.211.3.42</span><br><span class="line">PING 10.211.3.42 (10.211.3.42) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.3.42: icmp_seq=1 ttl=62 time=0.598 ms</span><br><span class="line">64 bytes from 10.211.3.42: icmp_seq=2 ttl=62 time=0.463 ms</span><br><span class="line">64 bytes from 10.211.3.42: icmp_seq=3 ttl=62 time=0.530 ms</span><br><span class="line">64 bytes from 10.211.3.42: icmp_seq=4 ttl=62 time=0.426 ms</span><br><span class="line"></span><br><span class="line">以此类推 。。。。。。</span><br></pre></td></tr></table></figure>
<h2 id="2-验证各个Pod中能否解析Kubernetes集群中的DNS记录。"><a href="#2-验证各个Pod中能否解析Kubernetes集群中的DNS记录。" class="headerlink" title="2. 验证各个Pod中能否解析Kubernetes集群中的DNS记录。"></a>2. 验证各个Pod中能否解析Kubernetes集群中的DNS记录。</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">[root@master alpine]# kubectl exec -it network-kflgv /bin/sh</span><br><span class="line">/ # nslookup kubernetes.default</span><br><span class="line">Server:		10.96.0.10</span><br><span class="line">Address:	10.96.0.10#53</span><br><span class="line"></span><br><span class="line">Name:	kubernetes.default.svc.cluster.local</span><br><span class="line">Address: 10.96.0.1</span><br><span class="line"></span><br><span class="line">[root@master alpine]# kubectl exec -it network-w89xg /bin/sh</span><br><span class="line">/ # nslookup kubernetes.default</span><br><span class="line">Server:		10.96.0.10</span><br><span class="line">Address:	10.96.0.10#53</span><br><span class="line"></span><br><span class="line">Name:	kubernetes.default.svc.cluster.local</span><br><span class="line">Address: 10.96.0.1</span><br><span class="line"></span><br><span class="line">[root@master alpine]# kubectl exec -it network-79dzf /bin/sh</span><br><span class="line">/ # nslookup kubernetes.default</span><br><span class="line">Server:		10.96.0.10</span><br><span class="line">Address:	10.96.0.10#53</span><br><span class="line"></span><br><span class="line">Name:	kubernetes.default.svc.cluster.local</span><br><span class="line">Address: 10.96.0.1</span><br><span class="line"></span><br><span class="line">[root@master alpine]# kubectl exec -it network-76xzk /bin/sh</span><br><span class="line">/ # nslookup kubernetes.default</span><br><span class="line">Server:		10.96.0.10</span><br><span class="line">Address:	10.96.0.10#53</span><br><span class="line"></span><br><span class="line">Name:	kubernetes.default.svc.cluster.local</span><br><span class="line">Address: 10.96.0.1</span><br><span class="line"></span><br><span class="line">以此类推 。。。。。。</span><br></pre></td></tr></table></figure>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Network/">Network</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-prometheus_resources" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/prometheus_resources/" class="article-date">
  	<time datetime="2019-05-27T04:35:49.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/prometheus_resources/">
        Prometheus 经典资料汇总
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="官方文档"><a href="#官方文档" class="headerlink" title="官方文档"></a>官方文档</h1><p><a href="https://prometheus.io/docs/prometheus/2.2/getting_started/" target="_blank" rel="noopener">https://prometheus.io/docs/prometheus/2.2/getting_started/</a></p>
<h1 id="Prometheus-Operator-开源项目地址"><a href="#Prometheus-Operator-开源项目地址" class="headerlink" title="Prometheus Operator 开源项目地址"></a>Prometheus Operator 开源项目地址</h1><p><a href="https://github.com/coreos/prometheus-operator/tree/v0.25.0" target="_blank" rel="noopener">https://github.com/coreos/prometheus-operator/tree/v0.25.0</a></p>
<h1 id="经典中文资料"><a href="#经典中文资料" class="headerlink" title="经典中文资料"></a>经典中文资料</h1><p><a href="https://yunlzheng.gitbook.io/prometheus-book/introduction" target="_blank" rel="noopener">https://yunlzheng.gitbook.io/prometheus-book/introduction</a></p>
<h1 id="使用Golang实现Prometheus-Exporter"><a href="#使用Golang实现Prometheus-Exporter" class="headerlink" title="使用Golang实现Prometheus Exporter"></a>使用Golang实现Prometheus Exporter</h1><p><a href="https://blog.csdn.net/u014029783/article/details/80001251" target="_blank" rel="noopener">https://blog.csdn.net/u014029783/article/details/80001251</a></p>
<h1 id="解惑Prometheus-API查询的时间戳格式"><a href="#解惑Prometheus-API查询的时间戳格式" class="headerlink" title="解惑Prometheus API查询的时间戳格式"></a>解惑Prometheus API查询的时间戳格式</h1><p><a href="https://www.crifan.com/timestamp_format_support_decimal_point_or_not/" target="_blank" rel="noopener">https://www.crifan.com/timestamp_format_support_decimal_point_or_not/</a></p>
<h1 id="Golang实现四舍五入（监控指标的格式化经常用到）"><a href="#Golang实现四舍五入（监控指标的格式化经常用到）" class="headerlink" title="Golang实现四舍五入（监控指标的格式化经常用到）"></a>Golang实现四舍五入（监控指标的格式化经常用到）</h1><p><a href="https://www.jianshu.com/p/ca52f4f58353" target="_blank" rel="noopener">https://www.jianshu.com/p/ca52f4f58353</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Monitoring/">Monitoring</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Prometheus/">Prometheus</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-p2p_distribute_images_000" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/p2p_distribute_images_000/" class="article-date">
  	<time datetime="2019-05-27T04:35:44.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/p2p_distribute_images_000/">
        使用Dragonfly实现P2P分发镜像
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、Dragonfly概述"><a href="#一、Dragonfly概述" class="headerlink" title="一、Dragonfly概述"></a>一、Dragonfly概述</h1><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h2><p>Dragonfly 是一款基于 P2P 的智能镜像和文件分发工具。它旨在提高文件传输的效率和速率，最大限度地利用网络带宽，尤其是在分发大量数据时，例如应用分发、缓存分发、日志分发和镜像分发。<br>尽管容器技术大部分时候简化了运维工作，但是它也带来了一些挑战：例如镜像分发的效率问题，尤其是必须在多个主机上复制镜像分发时。Dragonfly 在这种场景下能够完美支持 Docker ，相比原生方式，它能将容器镜像的分发速度提高了 57 倍，并让 Registry 网络出口流量降低 99.5%。使用它可以让容器镜像的分发变得简单而经济。</p>
<h2 id="2-官方文档地址"><a href="#2-官方文档地址" class="headerlink" title="2. 官方文档地址"></a>2. 官方文档地址</h2><p><a href="https://d7y.io/en-us/" target="_blank" rel="noopener">https://d7y.io/en-us/</a></p>
<h2 id="3-GitHub地址"><a href="#3-GitHub地址" class="headerlink" title="3. GitHub地址"></a>3. GitHub地址</h2><p><a href="https://github.com/dragonflyoss/Dragonfly/tree/v0.3.0" target="_blank" rel="noopener">https://github.com/dragonflyoss/Dragonfly/tree/v0.3.0</a></p>
<h2 id="4-注意事项"><a href="#4-注意事项" class="headerlink" title="4. 注意事项"></a>4. 注意事项</h2><p>目前Dragonfly的最新版本是v0.3.0，暂不支持对镜像仓库中的对私有镜像的认证。比如使用Harbor这种镜像仓库，把某个project的权限设置为私有权限，那么该project下的镜像是无法直接通过Dragonfly实现镜像的分发的。想通过Dragonfly实现镜像的分发，最简单的办法是必须把镜像对应的project的权限设置为公有权限。还有一种办法是为docker daemon设置http全局代理，但是必须使用0.0.1版本，最新的v0.3.0版本不支持这种方式。不过很遗憾，经测试该方式不好用。官方的Issues链接如下：<br><a href="https://github.com/dragonflyoss/Dragonfly/issues/138" target="_blank" rel="noopener">https://github.com/dragonflyoss/Dragonfly/issues/138</a></p>
<p>为docker daemon配置全局代理，方法如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /etc/systemd/system/docker.service.d</span><br><span class="line">vi /etc/systemd/system/docker.service.d/http-proxy.conf</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Environment=&quot;HTTP_PROXY=http://127.0.0.1:65001&quot;</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart docker.service</span><br><span class="line">systemctl show --property=Environment docker.service</span><br></pre></td></tr></table></figure></p>
<p>为docker daemon配置全局代理，参考链接如下：<br><a href="https://docs.docker.com/config/daemon/systemd/#httphttps-proxy" target="_blank" rel="noopener">https://docs.docker.com/config/daemon/systemd/#httphttps-proxy</a></p>
<h1 id="二、环境的相关信息"><a href="#二、环境的相关信息" class="headerlink" title="二、环境的相关信息"></a>二、环境的相关信息</h1><h2 id="1-版本信息"><a href="#1-版本信息" class="headerlink" title="1. 版本信息"></a>1. 版本信息</h2><p>Docker Engine Community 18.09.5<br>Harbor 0.5.0<br>Dragonfly v0.3.0</p>
<h2 id="2-服务器信息"><a href="#2-服务器信息" class="headerlink" title="2. 服务器信息"></a>2. 服务器信息</h2><p>172.16.170.134 <-> supernode<br>172.16.170.135 <-> dfclient<br>172.16.170.136 <-> dfclient</-></-></-></p>
<h1 id="三、实验过程记录"><a href="#三、实验过程记录" class="headerlink" title="三、实验过程记录"></a>三、实验过程记录</h1><h2 id="1-在172-16-170-134上安装supernode，如下所示："><a href="#1-在172-16-170-134上安装supernode，如下所示：" class="headerlink" title="1. 在172.16.170.134上安装supernode，如下所示："></a>1. 在172.16.170.134上安装supernode，如下所示：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name supernode --restart=always -p 8001:8001 -p 8002:8002 \</span><br><span class="line">    dragonflyoss/supernode:0.3.0 -Dsupernode.advertiseIp=172.16.170.134</span><br></pre></td></tr></table></figure>
<h2 id="2-分别在172-16-170-135和172-16-170-136上安装dfclient，如下所示："><a href="#2-分别在172-16-170-135和172-16-170-136上安装dfclient，如下所示：" class="headerlink" title="2. 分别在172.16.170.135和172.16.170.136上安装dfclient，如下所示："></a>2. 分别在172.16.170.135和172.16.170.136上安装dfclient，如下所示：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOD &gt; /etc/dragonfly/dfget.yml</span><br><span class="line">nodes:</span><br><span class="line">    - 172.16.170.134</span><br><span class="line">EOD</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOD &gt; /etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;http://127.0.0.1:65001&quot;],</span><br><span class="line">  &quot;insecure-registries&quot;: [&quot;10.0.55.126&quot;]</span><br><span class="line">&#125;</span><br><span class="line">EOD</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart docker.service</span><br><span class="line"></span><br><span class="line">docker run -d --name dfclient --restart=always -p 65001:65001 \</span><br><span class="line">    -v /etc/dragonfly:/etc/dragonfly \</span><br><span class="line">    dragonflyoss/dfclient:v0.3.0 --registry http://10.0.55.126</span><br></pre></td></tr></table></figure>
<h2 id="3-分别在172-16-170-135和172-16-170-136上安装拉取一个镜像，如下所示："><a href="#3-分别在172-16-170-135和172-16-170-136上安装拉取一个镜像，如下所示：" class="headerlink" title="3. 分别在172.16.170.135和172.16.170.136上安装拉取一个镜像，如下所示："></a>3. 分别在172.16.170.135和172.16.170.136上安装拉取一个镜像，如下所示：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># docker pull base/alpine:3.8</span><br><span class="line">3.8: Pulling from base/alpine</span><br><span class="line">16f532fbdc2a: Already exists</span><br><span class="line">Digest: sha256:78903b603e9fe76129d1a59ec94bc1ad47769b98e57e8f0c0a57760b12615960</span><br><span class="line">Status: Downloaded newer image for base/alpine:3.8</span><br><span class="line"></span><br><span class="line"># docker pull base/alpine:3.8</span><br><span class="line">3.8: Pulling from base/alpine</span><br><span class="line">Digest: sha256:78903b603e9fe76129d1a59ec94bc1ad47769b98e57e8f0c0a57760b12615960</span><br><span class="line">Status: Image is up to date for base/alpine:3.8</span><br></pre></td></tr></table></figure>
<h1 id="四、参考资料"><a href="#四、参考资料" class="headerlink" title="四、参考资料"></a>四、参考资料</h1><p><a href="https://github.com/dragonflyoss/Dragonfly/tree/v0.0.1" target="_blank" rel="noopener">https://github.com/dragonflyoss/Dragonfly/tree/v0.0.1</a><br><a href="https://github.com/dragonflyoss/Dragonfly/issues/138" target="_blank" rel="noopener">https://github.com/dragonflyoss/Dragonfly/issues/138</a><br><a href="http://dockone.io/article/4646" target="_blank" rel="noopener">http://dockone.io/article/4646</a><br><a href="https://mp.weixin.qq.com/s/95mX8cDox5bmgQ2xGHLPqQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/95mX8cDox5bmgQ2xGHLPqQ</a><br><a href="https://www.cnblogs.com/atuotuo/p/7298673.html" target="_blank" rel="noopener">https://www.cnblogs.com/atuotuo/p/7298673.html</a><br><a href="https://docs.docker.com/config/daemon/systemd/" target="_blank" rel="noopener">https://docs.docker.com/config/daemon/systemd/</a><br><a href="http://likakuli.com/post/2018/09/13/dragonfly/" target="_blank" rel="noopener">http://likakuli.com/post/2018/09/13/dragonfly/</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Dragonfly/">Dragonfly</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/P2P/">P2P</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-nginx_ingress" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/nginx_ingress/" class="article-date">
  	<time datetime="2019-05-27T04:35:38.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/nginx_ingress/">
        Nginx Ingress实验小记
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="克隆源代码，切换到当前最新的稳定版本"><a href="#克隆源代码，切换到当前最新的稳定版本" class="headerlink" title="克隆源代码，切换到当前最新的稳定版本"></a>克隆源代码，切换到当前最新的稳定版本</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/kubernetes/ingress-nginx.git</span><br><span class="line">cd ingress-nginx/</span><br><span class="line">git checkout -b nginx-0.21.0 nginx-0.21.0</span><br></pre></td></tr></table></figure>
<h1 id="创建Ingress代理的后端服务"><a href="#创建Ingress代理的后端服务" class="headerlink" title="创建Ingress代理的后端服务"></a>创建Ingress代理的后端服务</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p deploy/example</span><br><span class="line">vi deploy/example/nginx.yaml</span><br></pre></td></tr></table></figure>
<p>添加如下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.15.4</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">---</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br></pre></td></tr></table></figure></p>
<h1 id="Nginx-Ingress-Controller扮演四层负载均衡器，验证四层负载均衡"><a href="#Nginx-Ingress-Controller扮演四层负载均衡器，验证四层负载均衡" class="headerlink" title="Nginx Ingress Controller扮演四层负载均衡器，验证四层负载均衡"></a>Nginx Ingress Controller扮演四层负载均衡器，验证四层负载均衡</h1><p>在ConfigMap-[tcp-services]的data部分添加要暴露的端口到service和对应端口的映射关系，详见下面的示例：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-configuration</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: tcp-services</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">data:</span><br><span class="line">  8080: &quot;default/nginx:80&quot; # 添加要暴露的端口到service和对应端口的映射关系</span><br><span class="line">---</span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: udp-services</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br></pre></td></tr></table></figure></p>
<p>编辑deploy/with-rbac.yaml，把Pod模板中网络模式改为hostNetwork。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-controller</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app.kubernetes.io/name: ingress-nginx</span><br><span class="line">      app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app.kubernetes.io/name: ingress-nginx</span><br><span class="line">        app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">      annotations:</span><br><span class="line">        prometheus.io/port: &quot;10254&quot;</span><br><span class="line">        prometheus.io/scrape: &quot;true&quot;</span><br><span class="line">    spec:</span><br><span class="line">      hostNetwork: true # 注意这里设置为hostNetwork</span><br><span class="line">      serviceAccountName: nginx-ingress-serviceaccount</span><br><span class="line">      containers:</span><br><span class="line">        - name: nginx-ingress-controller</span><br><span class="line">          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0</span><br><span class="line">          args:</span><br><span class="line">            - /nginx-ingress-controller</span><br><span class="line">            - --configmap=$(POD_NAMESPACE)/nginx-configuration</span><br><span class="line">            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services</span><br><span class="line">            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services</span><br><span class="line">            - --publish-service=$(POD_NAMESPACE)/ingress-nginx</span><br><span class="line">            - --annotations-prefix=nginx.ingress.kubernetes.io</span><br><span class="line">            # - --http-port=8080 # 监听的http端口，默认80</span><br><span class="line">            # - --https-port=8443 # 监听的https端口，默认443</span><br><span class="line">          securityContext:</span><br><span class="line">            capabilities:</span><br><span class="line">              drop:</span><br><span class="line">                - ALL</span><br><span class="line">              add:</span><br><span class="line">                - NET_BIND_SERVICE</span><br><span class="line">            # www-data -&gt; 33</span><br><span class="line">            runAsUser: 33</span><br><span class="line">          env:</span><br><span class="line">            - name: POD_NAME</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.name</span><br><span class="line">            - name: POD_NAMESPACE</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.namespace</span><br><span class="line">          ports:</span><br><span class="line">            - name: http</span><br><span class="line">              containerPort: 80</span><br><span class="line">            - name: https</span><br><span class="line">              containerPort: 443</span><br><span class="line">          livenessProbe:</span><br><span class="line">            failureThreshold: 3</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /healthz</span><br><span class="line">              port: 10254</span><br><span class="line">              scheme: HTTP</span><br><span class="line">            initialDelaySeconds: 10</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            successThreshold: 1</span><br><span class="line">            timeoutSeconds: 1</span><br><span class="line">          readinessProbe:</span><br><span class="line">            failureThreshold: 3</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /healthz</span><br><span class="line">              port: 10254</span><br><span class="line">              scheme: HTTP</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            successThreshold: 1</span><br><span class="line">            timeoutSeconds: 1</span><br><span class="line"></span><br><span class="line">---</span><br></pre></td></tr></table></figure></p>
<h1 id="在Kubernetes中执行如下操作："><a href="#在Kubernetes中执行如下操作：" class="headerlink" title="在Kubernetes中执行如下操作："></a>在Kubernetes中执行如下操作：</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f deploy/namespace.yaml</span><br><span class="line">kubectl create -f deploy/configmap.yaml</span><br><span class="line">kubectl create -f deploy/rbac.yaml</span><br><span class="line">kubectl create -f deploy/with-rbac.yaml</span><br><span class="line">kubectl create -f deploy/provider/baremetal/service-nodeport.yaml</span><br></pre></td></tr></table></figure>
<h1 id="查看Nginx-Ingress-Controller所在的Node信息"><a href="#查看Nginx-Ingress-Controller所在的Node信息" class="headerlink" title="查看Nginx Ingress Controller所在的Node信息"></a>查看Nginx Ingress Controller所在的Node信息</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get pod -n ingress-nginx -o wide</span><br><span class="line">NAME                                        READY     STATUS    RESTARTS   AGE       IP               NODE</span><br><span class="line">nginx-ingress-controller-7799468ccb-5bzgv   1/1       Running   0          2m        172.16.170.129   server02</span><br></pre></td></tr></table></figure>
<h1 id="浏览器访问，四层负载均衡的效果如下图所示："><a href="#浏览器访问，四层负载均衡的效果如下图所示：" class="headerlink" title="浏览器访问，四层负载均衡的效果如下图所示："></a>浏览器访问，四层负载均衡的效果如下图所示：</h1><p><img src="/2019/05/27/nginx_ingress/ingress_4.png" alt="ingress_4"></p>
<h1 id="Nginx-Ingress-Controller扮演七层负载均衡器，验证七层负载均衡"><a href="#Nginx-Ingress-Controller扮演七层负载均衡器，验证七层负载均衡" class="headerlink" title="Nginx Ingress Controller扮演七层负载均衡器，验证七层负载均衡"></a>Nginx Ingress Controller扮演七层负载均衡器，验证七层负载均衡</h1><p>修改deploy/with-rbac.yaml，这里使用hostPort暴露http端口和https端口，不再使用hostNetwork。因为hostNetwork使用的是Node的网络栈，包括DNS服务器和解析。对于七层负载均衡器，暴露http和https的默认端口即可，可以通过不同的路径映射到不同的service上。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-controller</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app.kubernetes.io/name: ingress-nginx</span><br><span class="line">      app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app.kubernetes.io/name: ingress-nginx</span><br><span class="line">        app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">      annotations:</span><br><span class="line">        prometheus.io/port: &quot;10254&quot;</span><br><span class="line">        prometheus.io/scrape: &quot;true&quot;</span><br><span class="line">    spec:</span><br><span class="line">      # hostNetwork: true</span><br><span class="line">      serviceAccountName: nginx-ingress-serviceaccount</span><br><span class="line">      containers:</span><br><span class="line">        - name: nginx-ingress-controller</span><br><span class="line">          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0</span><br><span class="line">          args:</span><br><span class="line">            - /nginx-ingress-controller</span><br><span class="line">            - --configmap=$(POD_NAMESPACE)/nginx-configuration</span><br><span class="line">            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services</span><br><span class="line">            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services</span><br><span class="line">            - --publish-service=$(POD_NAMESPACE)/ingress-nginx</span><br><span class="line">            - --annotations-prefix=nginx.ingress.kubernetes.io</span><br><span class="line">            # - --http-port=8080 # 监听的http端口，默认80</span><br><span class="line">            # - --https-port=8443 # 监听的https端口，默认443</span><br><span class="line">          securityContext:</span><br><span class="line">            capabilities:</span><br><span class="line">              drop:</span><br><span class="line">                - ALL</span><br><span class="line">              add:</span><br><span class="line">                - NET_BIND_SERVICE</span><br><span class="line">            # www-data -&gt; 33</span><br><span class="line">            runAsUser: 33</span><br><span class="line">          env:</span><br><span class="line">            - name: POD_NAME</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.name</span><br><span class="line">            - name: POD_NAMESPACE</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.namespace</span><br><span class="line">          ports:</span><br><span class="line">            - name: http</span><br><span class="line">              containerPort: 80</span><br><span class="line">              hostPort: 80 # 使用hostPort暴露http端口</span><br><span class="line">            - name: https</span><br><span class="line">              containerPort: 443</span><br><span class="line">              hostPort: 443 # 使用hostPort暴露https端口</span><br><span class="line">          livenessProbe:</span><br><span class="line">            failureThreshold: 3</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /healthz</span><br><span class="line">              port: 10254</span><br><span class="line">              scheme: HTTP</span><br><span class="line">            initialDelaySeconds: 10</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            successThreshold: 1</span><br><span class="line">            timeoutSeconds: 1</span><br><span class="line">          readinessProbe:</span><br><span class="line">            failureThreshold: 3</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /healthz</span><br><span class="line">              port: 10254</span><br><span class="line">              scheme: HTTP</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            successThreshold: 1</span><br><span class="line">            timeoutSeconds: 1</span><br><span class="line"></span><br><span class="line">---</span><br></pre></td></tr></table></figure></p>
<p>创建deploy/example/ingress.yaml，添加如下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-app</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: foo.bar.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: nginx</span><br><span class="line">          servicePort: 80</span><br></pre></td></tr></table></figure></p>
<p>在Kubernetes集群中创建Ingress资源对象：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f eploy/example/ingress.yaml</span><br></pre></td></tr></table></figure></p>
<h1 id="浏览器访问，七层负载均衡的效果如下图所示："><a href="#浏览器访问，七层负载均衡的效果如下图所示：" class="headerlink" title="浏览器访问，七层负载均衡的效果如下图所示："></a>浏览器访问，七层负载均衡的效果如下图所示：</h1><p><img src="/2019/05/27/nginx_ingress/ingress_7.png" alt="ingress_7"></p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://github.com/kubernetes/ingress-nginx/tree/nginx-0.21.0" target="_blank" rel="noopener">https://github.com/kubernetes/ingress-nginx/tree/nginx-0.21.0</a><br><a href="https://kubernetes.github.io/ingress-nginx/" target="_blank" rel="noopener">https://kubernetes.github.io/ingress-nginx/</a><br><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/services-networking/ingress/</a><br><a href="https://www.cnblogs.com/iiiiher/p/8006801.html" target="_blank" rel="noopener">https://www.cnblogs.com/iiiiher/p/8006801.html</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Ingress/">Ingress</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Network/">Network</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-manual_clean_logging" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/manual_clean_logging/" class="article-date">
  	<time datetime="2019-05-27T04:35:29.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/manual_clean_logging/">
        手动清理Docker容器产生的日志
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>实际工作中，发现有些同事会有裸跑Docker容器的用法，最主要的是用json-file的日志驱动，还不配置日志轮转清理，具体原因不详。应用产生的日志量也不小，一段时间后，有趣的事情发生了：Docker莫名其妙不好使了，一查原因日志报盘了。上网找了一下资料，找到了比较好用的手动快速清理日志的方法，这里记录一下，供日后查阅使用。原创地址详见“参考资料”。</p>
<h2 id="1-在Docker默认配置下，利用下面的命令查看各个容器的日志文件大小；"><a href="#1-在Docker默认配置下，利用下面的命令查看各个容器的日志文件大小；" class="headerlink" title="1. 在Docker默认配置下，利用下面的命令查看各个容器的日志文件大小；"></a>1. 在Docker默认配置下，利用下面的命令查看各个容器的日志文件大小；</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls -lh $(find /var/lib/docker/containers/ -name *-json.log)</span><br></pre></td></tr></table></figure>
<h2 id="2-利用下面的脚本，可以手动清理宿主机上Docker容器产生的日志。"><a href="#2-利用下面的脚本，可以手动清理宿主机上Docker容器产生的日志。" class="headerlink" title="2. 利用下面的脚本，可以手动清理宿主机上Docker容器产生的日志。"></a>2. 利用下面的脚本，可以手动清理宿主机上Docker容器产生的日志。</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/sh</span><br><span class="line">echo &quot;==================== start clean docker containers logs ==========================&quot;</span><br><span class="line"> </span><br><span class="line">logs=$(find /var/lib/docker/containers/ -name *-json.log)</span><br><span class="line"> </span><br><span class="line">for log in $logs</span><br><span class="line">        do</span><br><span class="line">                echo &quot;clean logs : $log&quot;</span><br><span class="line">                cat /dev/null &gt; $log</span><br><span class="line">        done</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">echo &quot;==================== end clean docker containers logs   ==========================&quot;</span><br></pre></td></tr></table></figure>
<p>参考资料：<br><a href="https://blog.csdn.net/xunzhaoyao/article/details/72959917" target="_blank" rel="noopener">https://blog.csdn.net/xunzhaoyao/article/details/72959917</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Logging/">Logging</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kubernetes_trouble_shooting_002" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/kubernetes_trouble_shooting_002/" class="article-date">
  	<time datetime="2019-05-27T04:35:15.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/kubernetes_trouble_shooting_002/">
        如何保护系统级Pod不被Kubelet驱逐？
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、环境版本信息"><a href="#一、环境版本信息" class="headerlink" title="一、环境版本信息"></a>一、环境版本信息</h1><p>Docker 17.03.1-ce<br>Kubeadm v1.11.0<br>Kubelet v1.11.0<br>Kubectl v1.11.0<br>Calico v3.1.3</p>
<h1 id="二、问题描述"><a href="#二、问题描述" class="headerlink" title="二、问题描述"></a>二、问题描述</h1><p>使用Kubeadm搭建的Kubernetes集群在资源紧张的情况下，Kubernetes Control Plan开始挂掉了，集群开始变得不好用。比如你可以让你的Kubernetes Control Plan所在的服务器，磁盘使用率超过90%，问题就会立马复现。</p>
<h1 id="三、问题定位"><a href="#三、问题定位" class="headerlink" title="三、问题定位"></a>三、问题定位</h1><p>经排查发现，资源紧张的情况下，Kubelet会驱逐对应Node上的Pod，当然，这也包括Kubernetes Control Plan的相关Pod，会导致其被杀死及其相关镜像被删除。Kubelet这么做是为了释放资源，当然把Kubernetes Control Plan的相关Pod都干掉，并不是用户所希望看到的。一旦放大到Node节点上，问题就变成Node节点上非常重要的系统Pod会被干掉，导致相关的Node变成不好用的Node节点了。一旦放大到整个集群上，就变成Kubernetes Control Plan的相关Pod、Node节点上非常重要的系统级别Pod和集群上非常重要的Addon级别的Pod都会被干掉。问题的严重性，在规模效应下，可想而知了。</p>
<h1 id="四、问题分析"><a href="#四、问题分析" class="headerlink" title="四、问题分析"></a>四、问题分析</h1><p>有些Kubernetes的使用者可能会说，Kubeadm搭建的Kubernetes集群不推荐在生产环境使用，你非要用它，肯定会有问题。这一点也不奇怪。因为一直以来，使用Kubeadm搭建的Kubernetes集群都不推荐在生产环境使用，最开始是官方不推荐，大部分使用者也随着不推荐。随着Kubernetes的发展，我们逐渐发现使用Kubeadm搭建Kubernetes集群原有那句不推荐在生产环境使用的禁止提示不见了，你也会发现官方文档上花费了大量的笔墨介绍Kubeadm这种安装方式，甚至关于这种方式的高可用方案，官方文档也开始有了描述。但是对于其他的安装方式都是一笔带过，甚至只字未提。说明了什么？那便是官方推荐并鼓励用户使用这种方式部署Kubernetes集群。</p>
<p>该种方式搭建的集群真得就能应用于生产环境吗？如果不能，那么又是哪些问题导致的它不能应用于生产环境呢？笔者在实际工作中做了大量的实践，目前已经将其应用于生产环境了。实践过程中发现，Kubeadm已经相当成熟了，问题并不是很多，不过还是有一些的，但是就算有问题，读其规范的代码结构，改起来也相当方便。笔者在生产环境使用的版本，就做过一些修改。</p>
<p>这里笔者只给出Kubeadm在实际应用中两个大问题，并针对其中一个致命问题在本文中详细描述其解决方案。</p>
<ol>
<li>时区问题。一直以来，时区问题都是个看似简单又不引起重视的问题，特别对于美国用户，默认的UTC时区足够满足用户的需求了。那么生活在其他时区的用户怎么办？特别是我们这些生活在中国的用户该怎么办？修改Kubeadm的源码，在Kubernetes Control Plan的相关Pod的YAML上，加入社区设置的相关支持，详见《Docker容器的时区设置》。</li>
<li>当节点资源紧张，Kubelet开始驱逐Pod，删除其对应的镜像，来释放资源。看似没有问题，实际呢？很悲剧，这是个悲催的致命问题。为什么呢？对于Kubeadm这种安装方式，kube-apiserver、kube-controller-manager、kube-scheduler这种静态Pod没有问题，官方有解决方法不让其被驱逐，但是对于以Addon方式安装的系统级Pod呢？悲催了，没有保护机制。想重现问题很简单，磁盘使用率到90%以上，集群立刻开始往复于自杀和恢复之间。能解决吗？当然可以，不过社区现在并没有解决，笔者查看了GitHub上相关的Issue，详见参考资料。看来为了应用到生产环境，只能自己动手解决了。怎么解决呢？这个问题是Kubelet导致的，肯定是需要修改Kubelet的源码。</li>
</ol>
<h1 id="五、问题解决（这里以Kubernetes-v1-11-0版本为例，更高的版本会有所变化）"><a href="#五、问题解决（这里以Kubernetes-v1-11-0版本为例，更高的版本会有所变化）" class="headerlink" title="五、问题解决（这里以Kubernetes v1.11.0版本为例，更高的版本会有所变化）"></a>五、问题解决（这里以Kubernetes v1.11.0版本为例，更高的版本会有所变化）</h1><p>示例如下：（以示例描述，如何看笔者要添加和修改的代码）<br>example.go<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">这里是要添加和修改的代码</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>下面看具体的源码修改：</p>
<h2 id="1-kube-features-go"><a href="#1-kube-features-go" class="headerlink" title="1. kube_features.go"></a>1. kube_features.go</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">// defaultKubernetesFeatureGates consists of all known Kubernetes-specific feature keys.</span><br><span class="line">// To add a new feature, define a key for it above and add it here. The features will be</span><br><span class="line">// available throughout Kubernetes binaries.</span><br><span class="line">var defaultKubernetesFeatureGates = map[utilfeature.Feature]utilfeature.FeatureSpec&#123;</span><br><span class="line">	AppArmor:                                    &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	DynamicKubeletConfig:                        &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	ExperimentalHostUserNamespaceDefaultingGate: &#123;Default: false, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	ExperimentalCriticalPodAnnotation:           &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	...</span><br><span class="line">	ExperimentalNotEvictedPodAnnotation:         &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	...</span><br><span class="line">	DevicePlugins:                               &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	TaintBasedEvictions:                         &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	RotateKubeletServerCertificate:              &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	RotateKubeletClientCertificate:              &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	PersistentLocalVolumes:                      &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	LocalStorageCapacityIsolation:               &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	HugePages:                                   &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	Sysctls:                                     &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	DebugContainers:                             &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	PodShareProcessNamespace:                    &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	PodPriority:                                 &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	EnableEquivalenceClassCache:                 &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	TaintNodesByCondition:                       &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	MountPropagation:                            &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	QOSReserved:                                 &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	ExpandPersistentVolumes:                     &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	ExpandInUsePersistentVolumes:                &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	AttachVolumeLimit:                           &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	CPUManager:                                  &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	ServiceNodeExclusion:                        &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	MountContainers:                             &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	VolumeScheduling:                            &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	CSIPersistentVolume:                         &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	CustomPodDNS:                                &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	BlockVolume:                                 &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	StorageObjectInUseProtection:                &#123;Default: true, PreRelease: utilfeature.GA&#125;,</span><br><span class="line">	ResourceLimitsPriorityFunction:              &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	SupportIPVSProxyMode:                        &#123;Default: true, PreRelease: utilfeature.GA&#125;,</span><br><span class="line">	SupportPodPidsLimit:                         &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	HyperVContainer:                             &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	ScheduleDaemonSetPods:                       &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	TokenRequest:                                &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	TokenRequestProjection:                      &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	CRIContainerLogRotation:                     &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	GCERegionalPersistentDisk:                   &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	RunAsGroup:                                  &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	VolumeSubpath:                               &#123;Default: true, PreRelease: utilfeature.GA&#125;,</span><br><span class="line">	BalanceAttachedNodeVolumes:                  &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	DynamicProvisioningScheduling:               &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	PodReadinessGates:                           &#123;Default: false, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	VolumeSubpathEnvExpansion:                   &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	KubeletPluginsWatcher:                       &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	ResourceQuotaScopeSelectors:                 &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	CSIBlockVolume:                              &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line"></span><br><span class="line">	// inherited features from generic apiserver, relisted here to get a conflict if it is changed</span><br><span class="line">	// unintentionally on either side:</span><br><span class="line">	genericfeatures.StreamingProxyRedirects: &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	genericfeatures.AdvancedAuditing:        &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	genericfeatures.APIResponseCompression:  &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	genericfeatures.Initializers:            &#123;Default: false, PreRelease: utilfeature.Alpha&#125;,</span><br><span class="line">	genericfeatures.APIListChunking:         &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line"></span><br><span class="line">	// inherited features from apiextensions-apiserver, relisted here to get a conflict if it is changed</span><br><span class="line">	// unintentionally on either side:</span><br><span class="line">	apiextensionsfeatures.CustomResourceValidation:   &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line">	apiextensionsfeatures.CustomResourceSubresources: &#123;Default: true, PreRelease: utilfeature.Beta&#125;,</span><br><span class="line"></span><br><span class="line">	// features that enable backwards compatibility but are scheduled to be removed</span><br><span class="line">	ServiceProxyAllowExternalIPs: &#123;Default: false, PreRelease: utilfeature.Deprecated&#125;,</span><br><span class="line">	ReadOnlyAPIDataVolumes:       &#123;Default: true, PreRelease: utilfeature.Deprecated&#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	// owner: @vishh</span><br><span class="line">	// alpha: v1.5</span><br><span class="line">	//</span><br><span class="line">	// Ensures guaranteed scheduling of pods marked with a special pod annotation `scheduler.alpha.kubernetes.io/critical-pod`</span><br><span class="line">	// and also prevents them from being evicted from a node.</span><br><span class="line">	// Note: This feature is not supported for `BestEffort` pods.</span><br><span class="line">	ExperimentalCriticalPodAnnotation utilfeature.Feature = &quot;ExperimentalCriticalPodAnnotation&quot;</span><br><span class="line">...</span><br><span class="line">	// owner: @singhwang</span><br><span class="line">	// alpha: v1.5</span><br><span class="line">	//</span><br><span class="line">	// Ensures guaranteed scheduling of pods marked with a special pod annotation `scheduler.alpha.kubernetes.io/not-evicted-pod`</span><br><span class="line">	// and also prevents them from being evicted from a node.</span><br><span class="line">	ExperimentalNotEvictedPodAnnotation utilfeature.Feature = &quot;ExperimentalNotEvictedPodAnnotation&quot;</span><br><span class="line">...</span><br><span class="line">	// owner: @jiayingz</span><br><span class="line">	// beta: v1.10</span><br><span class="line">	//</span><br><span class="line">	// Enables support for Device Plugins</span><br><span class="line">	DevicePlugins utilfeature.Feature = &quot;DevicePlugins&quot;</span><br></pre></td></tr></table></figure>
<h2 id="2-pod-update-go"><a href="#2-pod-update-go" class="headerlink" title="2. pod_update.go"></a>2. pod_update.go</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">const (</span><br><span class="line">	ConfigSourceAnnotationKey    = &quot;kubernetes.io/config.source&quot;</span><br><span class="line">	ConfigMirrorAnnotationKey    = v1.MirrorPodAnnotationKey</span><br><span class="line">	ConfigFirstSeenAnnotationKey = &quot;kubernetes.io/config.seen&quot;</span><br><span class="line">	ConfigHashAnnotationKey      = &quot;kubernetes.io/config.hash&quot;</span><br><span class="line">	CriticalPodAnnotationKey     = &quot;scheduler.alpha.kubernetes.io/critical-pod&quot;</span><br><span class="line">	...</span><br><span class="line">	NotEvictedPodAnnotationKey   = &quot;scheduler.alpha.kubernetes.io/not-evicted-pod&quot;</span><br><span class="line">	...</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">// IsCriticalPodBasedOnPriority checks if the given pod is a critical pod based on priority resolved from pod Spec.</span><br><span class="line">func IsCriticalPodBasedOnPriority(priority int32) bool &#123;</span><br><span class="line">	if priority &gt;= scheduling.SystemCriticalPriority &#123;</span><br><span class="line">		return true</span><br><span class="line">	&#125;</span><br><span class="line">	return false</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">// IsNotEvictedPod returns true if the pod bears the not evicted pod annotation key.</span><br><span class="line">func IsNotEvictedPod(pod *v1.Pod) bool &#123;</span><br><span class="line">	return IsNotEvicted(pod.Namespace, pod.Annotations)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// IsNotEvicted returns true if parameters bear the not evicted pod annotation key.</span><br><span class="line">func IsNotEvicted(ns string, annotations map[string]string) bool &#123;</span><br><span class="line">	// NotEvicted pods are restricted to &quot;kube-system&quot; namespace as of now.</span><br><span class="line">	if ns != kubeapi.NamespaceSystem &#123;</span><br><span class="line">		return false</span><br><span class="line">	&#125;</span><br><span class="line">	val, ok := annotations[NotEvictedPodAnnotationKey]</span><br><span class="line">	if ok &amp;&amp; val == &quot;&quot; &#123;</span><br><span class="line">		return true</span><br><span class="line">	&#125;</span><br><span class="line">	return false</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h2 id="3-eviction-manager-go"><a href="#3-eviction-manager-go" class="headerlink" title="3. eviction_manager.go"></a>3. eviction_manager.go</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">func (m *managerImpl) evictPod(pod *v1.Pod, gracePeriodOverride int64, evictMsg string, annotations map[string]string) bool &#123;</span><br><span class="line">	// If the pod is marked as critical and static, and support for critical pod annotations is enabled,</span><br><span class="line">	// do not evict such pods. Static pods are not re-admitted after evictions.</span><br><span class="line">	// https://github.com/kubernetes/kubernetes/issues/40573 has more details.</span><br><span class="line">	if utilfeature.DefaultFeatureGate.Enabled(features.ExperimentalCriticalPodAnnotation) &amp;&amp;</span><br><span class="line">		kubelettypes.IsCriticalPod(pod) &amp;&amp; kubepod.IsStaticPod(pod) &#123;</span><br><span class="line">		glog.Errorf(&quot;eviction manager: cannot evict a critical static pod %s&quot;, format.Pod(pod))</span><br><span class="line">		return false</span><br><span class="line">	&#125;</span><br><span class="line">...</span><br><span class="line">	// If the pod is marked as not evicted, and support for not evicted pod annotations is enabled,</span><br><span class="line">	// do not evict such pods.</span><br><span class="line">	if utilfeature.DefaultFeatureGate.Enabled(features.ExperimentalNotEvictedPodAnnotation) &amp;&amp;</span><br><span class="line">		kubelettypes.IsNotEvictedPod(pod) &#123;</span><br><span class="line">		glog.Errorf(&quot;eviction manager: cannot evict a marked not evicted pod %s&quot;, format.Pod(pod))</span><br><span class="line">		return false</span><br><span class="line">	&#125;</span><br><span class="line">...	</span><br><span class="line">	status := v1.PodStatus&#123;</span><br><span class="line">		Phase:   v1.PodFailed,</span><br><span class="line">		Message: evictMsg,</span><br><span class="line">		Reason:  Reason,</span><br><span class="line">	&#125;</span><br><span class="line">	// record that we are evicting the pod</span><br><span class="line">	m.recorder.AnnotatedEventf(pod, annotations, v1.EventTypeWarning, Reason, evictMsg)</span><br><span class="line">	// this is a blocking call and should only return when the pod and its containers are killed.</span><br><span class="line">	err := m.killPodFunc(pod, status, &amp;gracePeriodOverride)</span><br><span class="line">	if err != nil &#123;</span><br><span class="line">		glog.Errorf(&quot;eviction manager: pod %s failed to evict %v&quot;, format.Pod(pod), err)</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		glog.Infof(&quot;eviction manager: pod %s is evicted successfully&quot;, format.Pod(pod))</span><br><span class="line">	&#125;</span><br><span class="line">	return true</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="4-重新编译源代码，生成新的Kubelet二进制文件，替换到对应的Node上，使用方式如下："><a href="#4-重新编译源代码，生成新的Kubelet二进制文件，替换到对应的Node上，使用方式如下：" class="headerlink" title="4. 重新编译源代码，生成新的Kubelet二进制文件，替换到对应的Node上，使用方式如下："></a>4. 重新编译源代码，生成新的Kubelet二进制文件，替换到对应的Node上，使用方式如下：</h2><p>注意–feature-gates部分，其中ExperimentalCriticalPodAnnotation为官方原生支持的功能，它可以保证kube-system下的Critical级别的静态Pod始终不被Kubelet所驱逐。另一个ExperimentalNotEvictedPodAnnotation是笔者加入的功能特性，它可以保证kube-system下的Pod，如果其Annotations中包括scheduler.alpha.kubernetes.io/not-evicted-pod，那么这样的Pod也始终不会被Kubelet所驱逐。这个功能特性的目的是，为了保证以Kubernetes为基础建立的容器系统的系统级Pod和重要Addon级别的Pod（这些系统级Pod和重要Addon级别的Pod，很可能是为了扩充Kubernetes功能而加入的，随意被驱逐显然是不正确的做法），允许用户通过这个scheduler.alpha.kubernetes.io/not-evicted-pod把其保护起来，提高Kubernetes容器系统的稳健性，避免集群资源紧张时，陷入自杀和自我修复的循环中而无法恢复。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KUBELET_EXTRA_ARGS=&quot;--cgroup-driver=cgroupfs --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1 --feature-gates=ExperimentalCriticalPodAnnotation=true,ExperimentalNotEvictedPodAnnotation=true&quot;</span><br></pre></td></tr></table></figure></p>
<h1 id="六、参考资料"><a href="#六、参考资料" class="headerlink" title="六、参考资料"></a>六、参考资料</h1><p><a href="https://github.com/kubernetes/kubernetes/tree/v1.11.0" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/tree/v1.11.0</a><br><a href="https://github.com/kubernetes/kubernetes/issues/53659" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/issues/53659</a><br><a href="https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/</a><br><a href="https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/</a><br><a href="https://www.centos.bz/2017/09/linux-命令行-创建指定大小的文件/" target="_blank" rel="noopener">https://www.centos.bz/2017/09/linux-命令行-创建指定大小的文件/</a><br><a href="https://www.hi-linux.com/posts/59095.html" target="_blank" rel="noopener">https://www.hi-linux.com/posts/59095.html</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Trouble-Shooting/">Trouble Shooting</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kubernetes_trouble_shooting_001" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/kubernetes_trouble_shooting_001/" class="article-date">
  	<time datetime="2019-05-27T04:35:00.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/kubernetes_trouble_shooting_001/">
        Kubernetes批量删除资源对象的注意事项：删除顺序和监测删除状态的重要性
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、问题描述"><a href="#一、问题描述" class="headerlink" title="一、问题描述"></a>一、问题描述</h1><p>公司内部的某平台，底层基于Kubernetes实现（Kubernetes的存储部分采用动态存储供应对接NFS）。平台内部有一个逻辑单元，可以调用接口创建。该逻辑单元对应到Kubernetes层面，是一系列紧密相关的namespace、deployment、statefulset、job、persistentvolumeclaim和persistentvolume等资源对象的集合体，我们可以把这个逻辑单元理解为一个复杂的Kubernetes上的应用，该应用还是跨namespace的。</p>
<p>同事们平时开发、调试和运维该逻辑单元的时候，通常会这么做：</p>
<ol>
<li>开发和调试该逻辑单元的清理接口，直接调用Kubernetes API删除该逻辑单元对应的各种资源对象，删除过程就是直接删除，不控制顺序，不监测各个资源对象的删除状态；</li>
<li>运维该逻辑单元的清理操作，为了省事，直接使用kubectl清理该逻辑单元相关的namespace。</li>
</ol>
<p>近期发现线上环境，频繁发生Pod无法删除，一直处于Terminating状态。关于这个问题，我自己偶尔见到，也频繁有同事和我反应这个问题。于是，我定位了一下，到无法删除的Pod所在的宿主机节点上，查看kubelet日志，发现报错信息如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">。。。。。。</span><br><span class="line">11月 16 14:11:34 node06 kubelet[5505]: E1116 14:11:34.324703    5505 nestedpendingoperations.go:267] Operation for &quot;\&quot;kubernetes.io/nfs/9c7bb592-e883-11e8-aae0-0050568a2af3-pvc-9c75962b-e883-11e8-aae0-0050568a2af3\&quot; (\&quot;9c7bb592-e883-11e8-aae0-0050568a2af3\&quot;)&quot; failed. No retries permitted until 2018-11-16 14:13:36.324623633 +0800 CST m=+15413.721922139 (durationBeforeRetry 2m2s). Error: &quot;error cleaning subPath mounts for volume \&quot;pvc-9c75962b-e883-11e8-aae0-0050568a2af3\&quot; (UniqueName: \&quot;kubernetes.io/nfs/9c7bb592-e883-11e8-aae0-0050568a2af3-pvc-9c75962b-e883-11e8-aae0-0050568a2af3\&quot;) pod \&quot;9c7bb592-e883-11e8-aae0-0050568a2af3\&quot; (UID: \&quot;9c7bb592-e883-11e8-aae0-0050568a2af3\&quot;) : error reading /var/lib/kubelet/pods/9c7bb592-e883-11e8-aae0-0050568a2af3/volume-subpaths/pvc-9c75962b-e883-11e8-aae0-0050568a2af3/orderer1: lstat /var/lib/kubelet/pods/9c7bb592-e883-11e8-aae0-0050568a2af3/volume-subpaths/pvc-9c75962b-e883-11e8-aae0-0050568a2af3/orderer1/0: stale NFS file handle&quot;</span><br><span class="line">11月 16 14:11:34 node06 kubelet[5505]: E1116 14:11:34.324809    5505 nestedpendingoperations.go:267] Operation for &quot;\&quot;kubernetes.io/nfs/b46aa346-e71d-11e8-b49a-0050568a2af3-pvc-5f5024f7-e6ed-11e8-989d-0050568a2af3\&quot; (\&quot;b46aa346-e71d-11e8-b49a-0050568a2af3\&quot;)&quot; failed. No retries permitted until 2018-11-16 14:13:36.324727477 +0800 CST m=+15413.722025987 (durationBeforeRetry 2m2s). Error: &quot;error cleaning subPath mounts for volume \&quot;pvc-5f5024f7-e6ed-11e8-989d-0050568a2af3\&quot; (UniqueName: \&quot;kubernetes.io/nfs/b46aa346-e71d-11e8-b49a-0050568a2af3-pvc-5f5024f7-e6ed-11e8-989d-0050568a2af3\&quot;) pod \&quot;b46aa346-e71d-11e8-b49a-0050568a2af3\&quot; (UID: \&quot;b46aa346-e71d-11e8-b49a-0050568a2af3\&quot;) : error reading /var/lib/kubelet/pods/b46aa346-e71d-11e8-b49a-0050568a2af3/volume-subpaths/pvc-5f5024f7-e6ed-11e8-989d-0050568a2af3/orderer1: lstat /var/lib/kubelet/pods/b46aa346-e71d-11e8-b49a-0050568a2af3/volume-subpaths/pvc-5f5024f7-e6ed-11e8-989d-0050568a2af3/orderer1/0: stale NFS file handle&quot;</span><br><span class="line">11月 16 14:11:34 node06 kubelet[5505]: E1116 14:11:34.325743    5505 nestedpendingoperations.go:267] Operation for &quot;\&quot;kubernetes.io/nfs/9b2a49c7-e726-11e8-a7a5-0050568a2af3-pvc-9b07268f-e726-11e8-a7a5-0050568a2af3\&quot; (\&quot;9b2a49c7-e726-11e8-a7a5-0050568a2af3\&quot;)&quot; failed. No retries permitted until 2018-11-16 14:13:36.325669009 +0800 CST m=+15413.722967535 (durationBeforeRetry 2m2s). Error: &quot;error cleaning subPath mounts for volume \&quot;pvc-9b07268f-e726-11e8-a7a5-0050568a2af3\&quot; (UniqueName: \&quot;kubernetes.io/nfs/9b2a49c7-e726-11e8-a7a5-0050568a2af3-pvc-9b07268f-e726-11e8-a7a5-0050568a2af3\&quot;) pod \&quot;9b2a49c7-e726-11e8-a7a5-0050568a2af3\&quot; (UID: \&quot;9b2a49c7-e726-11e8-a7a5-0050568a2af3\&quot;) : error reading /var/lib/kubelet/pods/9b2a49c7-e726-11e8-a7a5-0050568a2af3/volume-subpaths/pvc-9b07268f-e726-11e8-a7a5-0050568a2af3/cli: lstat /var/lib/kubelet/pods/9b2a49c7-e726-11e8-a7a5-0050568a2af3/volume-subpaths/pvc-9b07268f-e726-11e8-a7a5-0050568a2af3/cli/1: stale NFS file handle&quot;</span><br><span class="line">11月 16 14:11:34 node06 kubelet[5505]: E1116 14:11:34.325843    5505 nestedpendingoperations.go:267] Operation for &quot;\&quot;kubernetes.io/nfs/dc5262e0-e710-11e8-989d-0050568a2af3-pvc-66a1068f-e22d-11e8-a331-0050568a2af3\&quot; (\&quot;dc5262e0-e710-11e8-989d-0050568a2af3\&quot;)&quot; failed. No retries permitted until 2018-11-16 14:13:36.325772394 +0800 CST m=+15413.723070969 (durationBeforeRetry 2m2s). Error: &quot;error cleaning subPath mounts for volume \&quot;pvc-66a1068f-e22d-11e8-a331-0050568a2af3\&quot; (UniqueName: \&quot;kubernetes.io/nfs/dc5262e0-e710-11e8-989d-0050568a2af3-pvc-66a1068f-e22d-11e8-a331-0050568a2af3\&quot;) pod \&quot;dc5262e0-e710-11e8-989d-0050568a2af3\&quot; (UID: \&quot;dc5262e0-e710-11e8-989d-0050568a2af3\&quot;) : error reading /var/lib/kubelet/pods/dc5262e0-e710-11e8-989d-0050568a2af3/volume-subpaths/pvc-66a1068f-e22d-11e8-a331-0050568a2af3/org1-1000: lstat /var/lib/kubelet/pods/dc5262e0-e710-11e8-989d-0050568a2af3/volume-subpaths/pvc-66a1068f-e22d-11e8-a331-0050568a2af3/org1-1000/1: stale NFS file handle&quot;</span><br><span class="line">。。。。。。</span><br></pre></td></tr></table></figure></p>
<h1 id="二、问题定位"><a href="#二、问题定位" class="headerlink" title="二、问题定位"></a>二、问题定位</h1><p>起初看到这段日志有点不知所云，Google了一下，看社区描述，有人说是Kuberentes的一个bug。尽管查阅到的资料如此显示，我还始终有个想法一致在脑海中游荡。那便是资源对象本身而言，有一种潜在的依赖关系存在。这种潜在的依赖关系可以简单描述为：对于某种具体的容器应用而言，某个deployment创建的pod是依赖于某个persistentvolumeclaim的。如果我删除这个容器应用，要是先把那个persistentvolumeclaim删除干净了，再去删除那个依赖它的deployment，Kuberentes回收资源就可能会发生上述问题，毕竟依赖的资源已经不存在了。后来经过一段时间的排查和定位，问题的本质原因正是如此。<br>因此该问题的本质原因可以用一句话简单概括：Kuberentes底层在真正做某个资源对象的删除操作时，发现该资源对象依赖另一个资源对象，需要解除二者的绑定关系，但是很可惜它依赖的那个资源对象已经先它一步被删除了，绑定关系解除还可能要做资源回收操作，资源不存在了，回收操作一直无法成功，目标资源对象也就一直无法成功删除。</p>
<p>为了便于理解我所述的这种依赖关系，给出相应的YAML，假设nginx的终止比较耗时，删除时如果不控制顺序，就可以模拟出相同的情况：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  namespace: default</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">        image: nginx:1.15.3</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        name: nginx</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          protocol: TCP</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: /data/</span><br><span class="line">          name: data</span><br><span class="line">      restartPolicy: Always</span><br><span class="line">      volumes:</span><br><span class="line">      - name: data</span><br><span class="line">        persistentVolumeClaim:</span><br><span class="line">          claimName: nginx-pvc</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-pvc</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1G</span><br><span class="line">  storageClassName: managed-nfs-storage</span><br></pre></td></tr></table></figure></p>
<h1 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h1><ol>
<li>以第二点中的YAML为例，就是说删除时，一定要先删除deployment，然后监测该deployment和其对应的pod副本都删除干净后，再去删除persistentvolumeclaim。否则一同提交给Kubernetes，如果该deployment和其对应的pod副本终止比较耗时，persistentvolumeclaim就会优先于deployment先完成删除，那么该问题就出现了。</li>
<li>对于deployment和job两种资源对象，检测其自身的删除状态应该就可以，实测发现自身删除了，对应的pod副本也就删除干净了；但是对于statefulset这种资源对象就不行，一定要检测对应pod副本的删除状态，实测自身删除了，对应的pod副本并没有删除干净，还要等待一会儿，才能删除干净。因此必须单独检测pod副本的删除状态，这点在写程序时，一定要格外注意。</li>
</ol>
<h1 id="四、代码示例：删除资源对象（Deployment）的同时，监测其删除情况"><a href="#四、代码示例：删除资源对象（Deployment）的同时，监测其删除情况" class="headerlink" title="四、代码示例：删除资源对象（Deployment）的同时，监测其删除情况"></a>四、代码示例：删除资源对象（Deployment）的同时，监测其删除情况</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">type Metadata struct &#123;</span><br><span class="line">	Kind      string `json:&quot;kind&quot;`</span><br><span class="line">	Name      string `json:&quot;name&quot;`</span><br><span class="line">	Namespace string `json:&quot;namespace&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">metadataDepList := make([]module.Metadata, 0)</span><br><span class="line"></span><br><span class="line">for i := 0; i &lt; len(metadataList); i++ &#123;</span><br><span class="line">	metadata := metadataList[i]</span><br><span class="line">	glog.Info(&quot;metadata: &quot;, metadata)</span><br><span class="line">	if metadata.Kind == ResourceKindDeployment &#123;</span><br><span class="line">		metadataDepList = append(metadataDepList, metadata)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">for i := 0; i &lt; len(metadataDepList); i++ &#123;</span><br><span class="line">	metadataDep := metadataDepList[i]</span><br><span class="line">	dep, err := clientset.AppsV1beta1().Deployments(metadataDep.Namespace).Get(metadataDep.Name, v1.GetOptions&#123;&#125;)</span><br><span class="line">	if err != nil &#123;</span><br><span class="line">		return err</span><br><span class="line">	&#125;</span><br><span class="line">	labels := dep.Labels</span><br><span class="line"></span><br><span class="line">	sign := make(chan error, 1)</span><br><span class="line">	go func() &#123;</span><br><span class="line">		selector := pkglabels.FormatLabels(labels)</span><br><span class="line">		opts := v1.ListOptions&#123;</span><br><span class="line">			LabelSelector: selector,</span><br><span class="line">		&#125;</span><br><span class="line">		depWatch, err := clientset.AppsV1beta1().Deployments(metadataDep.Namespace).Watch(opts)</span><br><span class="line">		if err != nil &#123;</span><br><span class="line">			sign &lt;- err</span><br><span class="line">			return</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		deleteDepLoop:</span><br><span class="line">		for &#123;</span><br><span class="line">			select &#123;</span><br><span class="line">			case data := &lt;-depWatch.ResultChan():</span><br><span class="line">				glog.Infof(&quot;Deleted deployment.Name: %s. deployment.Namespace: %s. data.Type: %s&quot;, dep.Name, dep.Namespace, data.Type)</span><br><span class="line">				sign &lt;- nil</span><br><span class="line">				if data.Type == watch.Deleted &#123;</span><br><span class="line">					glog.Infof(&quot;Deleted deployment.Name: %s. deployment.Namespace: %s.&quot;, dep.Name, dep.Namespace)</span><br><span class="line">					break deleteDepLoop</span><br><span class="line">				&#125;</span><br><span class="line">			case &lt;-time.After(time.Duration(30) * time.Second):</span><br><span class="line">				sign &lt;- errors.New(&quot;delete deployment timeout. &quot;)</span><br><span class="line">				depWatch.Stop()</span><br><span class="line">				return</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;()</span><br><span class="line">	err = &lt;- sign</span><br><span class="line">	if err != nil &#123;</span><br><span class="line">		return err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	err = clientset.AppsV1beta1().Deployments(metadataDep.Namespace).Delete(metadataDep.Name, &amp;v1.DeleteOptions&#123;</span><br><span class="line">		PropagationPolicy: &amp;deletePropagationBackground,</span><br><span class="line">	&#125;)</span><br><span class="line">	if err != nil &#123;</span><br><span class="line">		return err</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Trouble-Shooting/">Trouble Shooting</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kubernetes_calico_000" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/kubernetes_calico_000/" class="article-date">
  	<time datetime="2019-05-27T04:34:27.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/kubernetes_calico_000/">
        Calico的网路通信过程跟踪（IPIP CrossSubnet 模式）
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、环境版本信息"><a href="#一、环境版本信息" class="headerlink" title="一、环境版本信息"></a>一、环境版本信息</h1><p>Kubernetes v1.11.0<br>Calico v3.1.3<br>Calicoctl v3.1.3</p>
<h1 id="二、核心概念解惑"><a href="#二、核心概念解惑" class="headerlink" title="二、核心概念解惑"></a>二、核心概念解惑</h1><h2 id="1-BGP协议是路由器与路由器之间的通信协议，建立在TCP上。路由器之间可以通过BGP协议交换彼此的路由信息。"><a href="#1-BGP协议是路由器与路由器之间的通信协议，建立在TCP上。路由器之间可以通过BGP协议交换彼此的路由信息。" class="headerlink" title="1. BGP协议是路由器与路由器之间的通信协议，建立在TCP上。路由器之间可以通过BGP协议交换彼此的路由信息。"></a>1. BGP协议是路由器与路由器之间的通信协议，建立在TCP上。路由器之间可以通过BGP协议交换彼此的路由信息。</h2><p>A路由器 <----> BGP协议 <----> B路由器 <----> BGP协议 <----> C路由器</----></----></----></----></p>
<h2 id="2-宿主机上运行了很多的Pod，这些Pod的IP地址和通信要怎么处理？"><a href="#2-宿主机上运行了很多的Pod，这些Pod的IP地址和通信要怎么处理？" class="headerlink" title="2. 宿主机上运行了很多的Pod，这些Pod的IP地址和通信要怎么处理？"></a>2. 宿主机上运行了很多的Pod，这些Pod的IP地址和通信要怎么处理？</h2><p>即把宿主机变成一台路由器。宿主机变身路由器后，现实的网络是怎么联通的，Pod之间就怎么联通，技术都是现成的，并且都已经支撑起连接全地球的互联网了。</p>
<h1 id="三、网络通信过程跟踪"><a href="#三、网络通信过程跟踪" class="headerlink" title="三、网络通信过程跟踪"></a>三、网络通信过程跟踪</h1><h2 id="1-以Daemonset形式，在default下部署一组Pod，用于测试Pod的跨主机网络通信："><a href="#1-以Daemonset形式，在default下部署一组Pod，用于测试Pod的跨主机网络通信：" class="headerlink" title="1. 以Daemonset形式，在default下部署一组Pod，用于测试Pod的跨主机网络通信："></a>1. 以Daemonset形式，在default下部署一组Pod，用于测试Pod的跨主机网络通信：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get node -o wide</span><br><span class="line">NAME       STATUS    ROLES     AGE       VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME</span><br><span class="line">server01   Ready     master    128d      v1.11.0   172.16.170.128   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-862.11.6.el7.x86_64   docker://17.3.1</span><br><span class="line">server02   Ready     &lt;none&gt;    128d      v1.11.0   172.16.170.129   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-514.el7.x86_64        docker://17.3.1</span><br><span class="line">server03   Ready     &lt;none&gt;    128d      v1.11.0   172.16.170.130   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-514.el7.x86_64        docker://17.3.1</span><br><span class="line"></span><br><span class="line"># kubectl get pod --all-namespaces -o wide</span><br><span class="line">NAMESPACE     NAME                               READY     STATUS    RESTARTS   AGE       IP               NODE</span><br><span class="line">default       network-5z4qp                      1/1       Running   0          12s       10.211.0.6       server01</span><br><span class="line">default       network-6k87z                      1/1       Running   0          12s       10.211.2.4       server03</span><br><span class="line">default       network-mngxw                      1/1       Running   0          12s       10.211.1.4       server02</span><br><span class="line">kube-system   calico-node-644wq                  2/2       Running   0          128d      172.16.170.128   server01</span><br><span class="line">kube-system   calico-node-hdkf6                  2/2       Running   0          128d      172.16.170.130   server03</span><br><span class="line">kube-system   calico-node-wltgp                  2/2       Running   0          128d      172.16.170.129   server02</span><br><span class="line">kube-system   coredns-777d78ff6f-6sjt5           1/1       Running   0          128d      10.211.0.3       server01</span><br><span class="line">kube-system   coredns-777d78ff6f-mr977           1/1       Running   0          128d      10.211.0.2       server01</span><br><span class="line">kube-system   etcd-server01                      1/1       Running   0          128d      172.16.170.128   server01</span><br><span class="line">kube-system   kube-apiserver-server01            1/1       Running   0          128d      172.16.170.128   server01</span><br><span class="line">kube-system   kube-controller-manager-server01   1/1       Running   0          128d      172.16.170.128   server01</span><br><span class="line">kube-system   kube-proxy-94k52                   1/1       Running   0          128d      172.16.170.129   server02</span><br><span class="line">kube-system   kube-proxy-czg29                   1/1       Running   0          128d      172.16.170.128   server01</span><br><span class="line">kube-system   kube-proxy-mnhrb                   1/1       Running   0          128d      172.16.170.130   server03</span><br><span class="line">kube-system   kube-scheduler-server01            1/1       Running   0          128d      172.16.170.128   server01</span><br></pre></td></tr></table></figure>
<h2 id="2-选取master节点的calico-node，进入安装calicoctl命令行工具："><a href="#2-选取master节点的calico-node，进入安装calicoctl命令行工具：" class="headerlink" title="2. 选取master节点的calico-node，进入安装calicoctl命令行工具："></a>2. 选取master节点的calico-node，进入安装calicoctl命令行工具：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.1.3/calicoctl</span><br><span class="line"># kubectl cp calicoctl calico-node-644wq:/usr/local/bin/ -n kube-system -c calico-node</span><br><span class="line"># kubectl exec calico-node-644wq -n kube-system -c calico-node -- chmod 0755 /usr/local/bin/calicoctl</span><br></pre></td></tr></table></figure>
<h2 id="3-进入master节点的calico-node，使用calicoctl查看需要关注的工作负载的相关信息："><a href="#3-进入master节点的calico-node，使用calicoctl查看需要关注的工作负载的相关信息：" class="headerlink" title="3. 进入master节点的calico-node，使用calicoctl查看需要关注的工作负载的相关信息："></a>3. 进入master节点的calico-node，使用calicoctl查看需要关注的工作负载的相关信息：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"># kubectl exec -it calico-node-644wq /bin/sh -n kube-system -c calico-node</span><br><span class="line">/ # calicoctl get workloadendpoint -n default</span><br><span class="line">NAMESPACE   WORKLOAD        NODE       NETWORKS        INTERFACE</span><br><span class="line">default     network-5z4qp   server01   10.211.0.6/32   cali50914021272</span><br><span class="line">default     network-6k87z   server03   10.211.2.4/32   calif17d1193010</span><br><span class="line">default     network-mngxw   server02   10.211.1.4/32   calid406b8b6c93</span><br><span class="line"></span><br><span class="line">/ # calicoctl get workloadendpoint -n default -o yaml</span><br><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">items:</span><br><span class="line">- apiVersion: projectcalico.org/v3</span><br><span class="line">  kind: WorkloadEndpoint</span><br><span class="line">  metadata:</span><br><span class="line">    creationTimestamp: 2019-01-16T06:46:19Z</span><br><span class="line">    labels:</span><br><span class="line">      app: network</span><br><span class="line">      controller-revision-hash: &quot;66086030&quot;</span><br><span class="line">      pod-template-generation: &quot;1&quot;</span><br><span class="line">      projectcalico.org/namespace: default</span><br><span class="line">      projectcalico.org/orchestrator: k8s</span><br><span class="line">    name: server01-k8s-network--5z4qp-eth0</span><br><span class="line">    namespace: default</span><br><span class="line">    resourceVersion: &quot;25227&quot;</span><br><span class="line">    uid: 6df53e81-195a-11e9-b5d1-000c2921eba0</span><br><span class="line">  spec:</span><br><span class="line">    endpoint: eth0</span><br><span class="line">    interfaceName: cali50914021272</span><br><span class="line">    ipNetworks:</span><br><span class="line">    - 10.211.0.6/32</span><br><span class="line">    node: server01</span><br><span class="line">    orchestrator: k8s</span><br><span class="line">    pod: network-5z4qp</span><br><span class="line">    profiles:</span><br><span class="line">    - kns.default</span><br><span class="line">- apiVersion: projectcalico.org/v3</span><br><span class="line">  kind: WorkloadEndpoint</span><br><span class="line">  metadata:</span><br><span class="line">    creationTimestamp: 2019-01-16T06:46:19Z</span><br><span class="line">    labels:</span><br><span class="line">      app: network</span><br><span class="line">      controller-revision-hash: &quot;66086030&quot;</span><br><span class="line">      pod-template-generation: &quot;1&quot;</span><br><span class="line">      projectcalico.org/namespace: default</span><br><span class="line">      projectcalico.org/orchestrator: k8s</span><br><span class="line">    name: server03-k8s-network--6k87z-eth0</span><br><span class="line">    namespace: default</span><br><span class="line">    resourceVersion: &quot;25084&quot;</span><br><span class="line">    uid: 6dfe39ec-195a-11e9-b5d1-000c2921eba0</span><br><span class="line">  spec:</span><br><span class="line">    endpoint: eth0</span><br><span class="line">    interfaceName: calif17d1193010</span><br><span class="line">    ipNetworks:</span><br><span class="line">    - 10.211.2.4/32</span><br><span class="line">    node: server03</span><br><span class="line">    orchestrator: k8s</span><br><span class="line">    pod: network-6k87z</span><br><span class="line">    profiles:</span><br><span class="line">    - kns.default</span><br><span class="line">- apiVersion: projectcalico.org/v3</span><br><span class="line">  kind: WorkloadEndpoint</span><br><span class="line">  metadata:</span><br><span class="line">    creationTimestamp: 2019-01-16T06:46:19Z</span><br><span class="line">    labels:</span><br><span class="line">      app: network</span><br><span class="line">      controller-revision-hash: &quot;66086030&quot;</span><br><span class="line">      pod-template-generation: &quot;1&quot;</span><br><span class="line">      projectcalico.org/namespace: default</span><br><span class="line">      projectcalico.org/orchestrator: k8s</span><br><span class="line">    name: server02-k8s-network--mngxw-eth0</span><br><span class="line">    namespace: default</span><br><span class="line">    resourceVersion: &quot;25082&quot;</span><br><span class="line">    uid: 6e0213f6-195a-11e9-b5d1-000c2921eba0</span><br><span class="line">  spec:</span><br><span class="line">    endpoint: eth0</span><br><span class="line">    interfaceName: calid406b8b6c93</span><br><span class="line">    ipNetworks:</span><br><span class="line">    - 10.211.1.4/32</span><br><span class="line">    node: server02</span><br><span class="line">    orchestrator: k8s</span><br><span class="line">    pod: network-mngxw</span><br><span class="line">    profiles:</span><br><span class="line">    - kns.default</span><br><span class="line">kind: WorkloadEndpointList</span><br><span class="line">metadata: &#123;&#125;</span><br><span class="line"></span><br><span class="line">/ # calicoctl get workloadendpoint server01-k8s-network--5z4qp-eth0 -n default -o yaml</span><br><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">kind: WorkloadEndpoint</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: 2019-01-16T06:46:19Z</span><br><span class="line">  labels:</span><br><span class="line">    app: network</span><br><span class="line">    controller-revision-hash: &quot;66086030&quot;</span><br><span class="line">    pod-template-generation: &quot;1&quot;</span><br><span class="line">    projectcalico.org/namespace: default</span><br><span class="line">    projectcalico.org/orchestrator: k8s</span><br><span class="line">  name: server01-k8s-network--5z4qp-eth0</span><br><span class="line">  namespace: default</span><br><span class="line">  resourceVersion: &quot;25227&quot;</span><br><span class="line">  uid: 6df53e81-195a-11e9-b5d1-000c2921eba0</span><br><span class="line">spec:</span><br><span class="line">  endpoint: eth0</span><br><span class="line">  interfaceName: cali50914021272</span><br><span class="line">  ipNetworks:</span><br><span class="line">  - 10.211.0.6/32</span><br><span class="line">  node: server01</span><br><span class="line">  orchestrator: k8s</span><br><span class="line">  pod: network-5z4qp</span><br><span class="line">  profiles:</span><br><span class="line">  - kns.default</span><br><span class="line"></span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure>
<h2 id="4-选取master节点上的用于测试网络通信的Pod，进入查看它的网络配置信息："><a href="#4-选取master节点上的用于测试网络通信的Pod，进入查看它的网络配置信息：" class="headerlink" title="4. 选取master节点上的用于测试网络通信的Pod，进入查看它的网络配置信息："></a>4. 选取master节点上的用于测试网络通信的Pod，进入查看它的网络配置信息：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"># kubectl exec -it network-5z4qp /bin/sh -n default</span><br><span class="line">/ # ip link</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">2: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">4: eth0@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether 4e:30:25:dc:21:88 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">/ # ip address</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">4: eth0@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether 4e:30:25:dc:21:88 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 10.211.0.6/32 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::4c30:25ff:fedc:2188/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">/ # ip neigh</span><br><span class="line">/ # ping -c 3 10.211.0.1</span><br><span class="line">PING 10.211.0.1 (10.211.0.1) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.0.1: icmp_seq=1 ttl=64 time=0.085 ms</span><br><span class="line">64 bytes from 10.211.0.1: icmp_seq=2 ttl=64 time=0.072 ms</span><br><span class="line">64 bytes from 10.211.0.1: icmp_seq=3 ttl=64 time=0.069 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.0.1 ping statistics ---</span><br><span class="line">3 packets transmitted, 3 received, 0% packet loss, time 2001ms</span><br><span class="line">rtt min/avg/max/mdev = 0.069/0.075/0.085/0.009 ms</span><br><span class="line">/ # ip neigh</span><br><span class="line">169.254.1.1 dev eth0 lladdr ee:ee:ee:ee:ee:ee REACHABLE</span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure>
<h2 id="5-回到master节点上，查看calico的workload的yaml中cali50914021272对应的设备信息和所有设备的地址信息："><a href="#5-回到master节点上，查看calico的workload的yaml中cali50914021272对应的设备信息和所有设备的地址信息：" class="headerlink" title="5. 回到master节点上，查看calico的workload的yaml中cali50914021272对应的设备信息和所有设备的地址信息："></a>5. 回到master节点上，查看calico的workload的yaml中cali50914021272对应的设备信息和所有设备的地址信息：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"># ip link show cali50914021272</span><br><span class="line">11: cali50914021272@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 2</span><br><span class="line"></span><br><span class="line"># ip address</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:21:eb:a0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.16.170.128/24 brd 172.16.170.255 scope global ens33</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::20c:29ff:fe21:eba0/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default</span><br><span class="line">    link/ether 02:42:ca:a4:09:a5 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.17.0.1/16 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::42:caff:fea4:9a5/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">4: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1440 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">    inet 10.211.0.1/32 brd 10.211.0.1 scope global tunl0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">5: cali009d9b46eef@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet6 fe80::ecee:eeff:feee:eeee/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">6: calib99e709bd2c@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br><span class="line">    inet6 fe80::ecee:eeff:feee:eeee/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">11: cali50914021272@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 2</span><br><span class="line">    inet6 fe80::ecee:eeff:feee:eeee/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>
<h2 id="6-验证master上的Pod的网络数据包是否可以发送到master上：（其中10-211-0-1为tunl0设备的地址）"><a href="#6-验证master上的Pod的网络数据包是否可以发送到master上：（其中10-211-0-1为tunl0设备的地址）" class="headerlink" title="6. 验证master上的Pod的网络数据包是否可以发送到master上：（其中10.211.0.1为tunl0设备的地址）"></a>6. 验证master上的Pod的网络数据包是否可以发送到master上：（其中10.211.0.1为tunl0设备的地址）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 开启一个新的终端执行</span><br><span class="line"># tcpdump -i cali50914021272 icmp -v</span><br><span class="line">tcpdump: listening on cali50914021272, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">02:00:24.930193 IP (tos 0x0, ttl 64, id 51910, offset 0, flags [DF], proto ICMP (1), length 84)</span><br><span class="line">    10.211.0.6 &gt; server01: ICMP echo request, id 18, seq 1, length 64</span><br><span class="line">02:00:24.930235 IP (tos 0x0, ttl 64, id 22594, offset 0, flags [none], proto ICMP (1), length 84)</span><br><span class="line">    server01 &gt; 10.211.0.6: ICMP echo reply, id 18, seq 1, length 64</span><br><span class="line"></span><br><span class="line"># 在原有终端上执行</span><br><span class="line"># kubectl exec -it network-5z4qp /bin/sh -n default</span><br><span class="line">/ # ping -c 1 10.211.0.1</span><br><span class="line">PING 10.211.0.1 (10.211.0.1) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.0.1: icmp_seq=1 ttl=64 time=0.081 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.0.1 ping statistics ---</span><br><span class="line">1 packets transmitted, 1 received, 0% packet loss, time 0ms</span><br><span class="line">rtt min/avg/max/mdev = 0.081/0.081/0.081/0.000 ms</span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure>
<h2 id="7-查看master上的路由表，看我们即将测试的目标Pod（地址为10-211-1-4）在master上对应的下一跳在哪里："><a href="#7-查看master上的路由表，看我们即将测试的目标Pod（地址为10-211-1-4）在master上对应的下一跳在哪里：" class="headerlink" title="7. 查看master上的路由表，看我们即将测试的目标Pod（地址为10.211.1.4）在master上对应的下一跳在哪里："></a>7. 查看master上的路由表，看我们即将测试的目标Pod（地址为10.211.1.4）在master上对应的下一跳在哪里：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># ip route</span><br><span class="line">...</span><br><span class="line">blackhole 10.211.0.0/24 proto bird</span><br><span class="line">10.211.0.2 dev cali009d9b46eef scope link</span><br><span class="line">10.211.0.3 dev calib99e709bd2c scope link</span><br><span class="line">10.211.0.6 dev cali50914021272 scope link</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">10.211.1.0/24 via 172.16.170.129 dev ens33 proto bird</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h2 id="8-验证发往目标Pod（地址为10-211-1-4）的网络数据包是否发往master的主机网卡ens33上："><a href="#8-验证发往目标Pod（地址为10-211-1-4）的网络数据包是否发往master的主机网卡ens33上：" class="headerlink" title="8. 验证发往目标Pod（地址为10.211.1.4）的网络数据包是否发往master的主机网卡ens33上："></a>8. 验证发往目标Pod（地址为10.211.1.4）的网络数据包是否发往master的主机网卡ens33上：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 开启一个新的终端执行</span><br><span class="line"># tcpdump -i ens33 icmp -v</span><br><span class="line">tcpdump: listening on ens33, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">02:30:22.537037 IP (tos 0x0, ttl 63, id 23441, offset 0, flags [DF], proto ICMP (1), length 84)</span><br><span class="line">    10.211.0.6 &gt; 10.211.1.4: ICMP echo request, id 23, seq 1, length 64</span><br><span class="line">02:30:22.537476 IP (tos 0x0, ttl 63, id 62650, offset 0, flags [none], proto ICMP (1), length 84)</span><br><span class="line">    10.211.1.4 &gt; 10.211.0.6: ICMP echo reply, id 23, seq 1, length 64</span><br><span class="line"></span><br><span class="line"># 在原有终端上执行</span><br><span class="line"># kubectl exec -it network-5z4qp /bin/sh -n default</span><br><span class="line">/ # ping -c 1 10.211.1.4</span><br><span class="line">PING 10.211.1.4 (10.211.1.4) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.1.4: icmp_seq=1 ttl=62 time=0.526 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.1.4 ping statistics ---</span><br><span class="line">1 packets transmitted, 1 received, 0% packet loss, time 0ms</span><br><span class="line">rtt min/avg/max/mdev = 0.526/0.526/0.526/0.000 ms</span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure>
<h2 id="9-验证发往目标Pod（地址为10-211-1-4）的网络数据包是否发送到Node（地址为172-16-170-129）的网卡ens33上："><a href="#9-验证发往目标Pod（地址为10-211-1-4）的网络数据包是否发送到Node（地址为172-16-170-129）的网卡ens33上：" class="headerlink" title="9. 验证发往目标Pod（地址为10.211.1.4）的网络数据包是否发送到Node（地址为172.16.170.129）的网卡ens33上："></a>9. 验证发往目标Pod（地址为10.211.1.4）的网络数据包是否发送到Node（地址为172.16.170.129）的网卡ens33上：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 在地址为10.211.1.4的Pod所在的Node上</span><br><span class="line"># tcpdump -i ens33 icmp -v</span><br><span class="line">tcpdump: listening on ens33, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">02:33:17.808254 IP (tos 0x0, ttl 63, id 61238, offset 0, flags [DF], proto ICMP (1), length 84)</span><br><span class="line">    10.211.0.6 &gt; 10.211.1.4: ICMP echo request, id 33, seq 1, length 64</span><br><span class="line">02:33:17.808415 IP (tos 0x0, ttl 63, id 1878, offset 0, flags [none], proto ICMP (1), length 84)</span><br><span class="line">    10.211.1.4 &gt; 10.211.0.6: ICMP echo reply, id 33, seq 1, length 64</span><br><span class="line"></span><br><span class="line"># 回到地址为10.211.0.6的Pod所在的Node上</span><br><span class="line"># kubectl exec -it network-5z4qp /bin/sh -n default</span><br><span class="line">/ # ping -c 1 10.211.1.4</span><br><span class="line">PING 10.211.1.4 (10.211.1.4) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.1.4: icmp_seq=1 ttl=62 time=0.556 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.1.4 ping statistics ---</span><br><span class="line">1 packets transmitted, 1 received, 0% packet loss, time 0ms</span><br><span class="line">rtt min/avg/max/mdev = 0.556/0.556/0.556/0.000 ms</span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure>
<h2 id="10-在Node（地址为172-16-170-129）上查看发往目标Pod（地址为10-211-1-4）的下一跳在哪里："><a href="#10-在Node（地址为172-16-170-129）上查看发往目标Pod（地址为10-211-1-4）的下一跳在哪里：" class="headerlink" title="10. 在Node（地址为172.16.170.129）上查看发往目标Pod（地址为10.211.1.4）的下一跳在哪里："></a>10. 在Node（地址为172.16.170.129）上查看发往目标Pod（地址为10.211.1.4）的下一跳在哪里：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># ip route</span><br><span class="line">...</span><br><span class="line">10.211.1.4 dev calid406b8b6c93 scope link</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h2 id="11-验证发往目标Pod（地址为10-211-1-4）的网络数据包是否发送到Node（地址为172-16-170-129）的网络设备calid406b8b6c93上："><a href="#11-验证发往目标Pod（地址为10-211-1-4）的网络数据包是否发送到Node（地址为172-16-170-129）的网络设备calid406b8b6c93上：" class="headerlink" title="11. 验证发往目标Pod（地址为10.211.1.4）的网络数据包是否发送到Node（地址为172.16.170.129）的网络设备calid406b8b6c93上："></a>11. 验证发往目标Pod（地址为10.211.1.4）的网络数据包是否发送到Node（地址为172.16.170.129）的网络设备calid406b8b6c93上：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># 在地址为10.211.1.4的Pod所在的Node上</span><br><span class="line"># ip link show calid406b8b6c93</span><br><span class="line">7: calid406b8b6c93@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line"></span><br><span class="line"># tcpdump -i calid406b8b6c93 icmp -v</span><br><span class="line">tcpdump: listening on calid406b8b6c93, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">02:39:15.303283 IP (tos 0x0, ttl 62, id 44511, offset 0, flags [DF], proto ICMP (1), length 84)</span><br><span class="line">    10.211.0.6 &gt; 10.211.1.4: ICMP echo request, id 38, seq 1, length 64</span><br><span class="line">02:39:15.303392 IP (tos 0x0, ttl 64, id 47955, offset 0, flags [none], proto ICMP (1), length 84)</span><br><span class="line">    10.211.1.4 &gt; 10.211.0.6: ICMP echo reply, id 38, seq 1, length 64</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 回到地址为10.211.0.6的Pod所在的Node上（即master上），开启一个新的终端</span><br><span class="line"># kubectl exec -it network-5z4qp /bin/sh -n default</span><br><span class="line">/ # ping -c 1 10.211.1.4</span><br><span class="line">PING 10.211.1.4 (10.211.1.4) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.1.4: icmp_seq=1 ttl=62 time=0.602 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.1.4 ping statistics ---</span><br><span class="line">1 packets transmitted, 1 received, 0% packet loss, time 0ms</span><br><span class="line">rtt min/avg/max/mdev = 0.602/0.602/0.602/0.000 ms</span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure>
<h2 id="12-验证发往目标Pod（地址为10-211-1-4）的网络数据包是否发送到它的网络设备eth0上："><a href="#12-验证发往目标Pod（地址为10-211-1-4）的网络数据包是否发送到它的网络设备eth0上：" class="headerlink" title="12. 验证发往目标Pod（地址为10.211.1.4）的网络数据包是否发送到它的网络设备eth0上："></a>12. 验证发往目标Pod（地址为10.211.1.4）的网络数据包是否发送到它的网络设备eth0上：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"># kubectl exec -it network-mngxw /bin/sh -n default</span><br><span class="line">/ # ip link</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">2: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN mode DEFAULT group default qlen 1</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">4: eth0@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link/ether e2:89:27:9e:6f:a6 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">/ # ip address</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1</span><br><span class="line">    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">4: eth0@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether e2:89:27:9e:6f:a6 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 10.211.1.4/32 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::e089:27ff:fe9e:6fa6/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">/ # tcpdump -i eth0 icmp -v</span><br><span class="line">tcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">07:42:45.870953 IP (tos 0x0, ttl 62, id 13540, offset 0, flags [DF], proto ICMP (1), length 84)</span><br><span class="line">    10.211.0.6 &gt; network-mngxw: ICMP echo request, id 43, seq 1, length 64</span><br><span class="line">07:42:45.871065 IP (tos 0x0, ttl 64, id 19616, offset 0, flags [none], proto ICMP (1), length 84)</span><br><span class="line">    network-mngxw &gt; 10.211.0.6: ICMP echo reply, id 43, seq 1, length 64</span><br><span class="line"></span><br><span class="line"># 回到地址为10.211.0.6的Pod所在的Node上（即master上）</span><br><span class="line"># kubectl exec -it network-5z4qp /bin/sh -n default</span><br><span class="line">/ # ping -c 1 10.211.1.4</span><br><span class="line">PING 10.211.1.4 (10.211.1.4) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.211.1.4: icmp_seq=1 ttl=62 time=0.602 ms</span><br><span class="line"></span><br><span class="line">--- 10.211.1.4 ping statistics ---</span><br><span class="line">1 packets transmitted, 1 received, 0% packet loss, time 0ms</span><br><span class="line">rtt min/avg/max/mdev = 0.602/0.602/0.602/0.000 ms</span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure>
<h1 id="四、参考资料"><a href="#四、参考资料" class="headerlink" title="四、参考资料"></a>四、参考资料</h1><p><a href="https://docs.projectcalico.org/v3.1/usage/calicoctl/install" target="_blank" rel="noopener">https://docs.projectcalico.org/v3.1/usage/calicoctl/install</a><br><a href="https://mp.weixin.qq.com/s/MZIj_cvvtTiAfNf_0lpfTg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/MZIj_cvvtTiAfNf_0lpfTg</a><br><a href="https://mp.weixin.qq.com/s/oKxsWDTvoLeOSHAuPIxnGw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/oKxsWDTvoLeOSHAuPIxnGw</a><br><a href="https://blog.csdn.net/ccy19910925/article/details/82424275" target="_blank" rel="noopener">https://blog.csdn.net/ccy19910925/article/details/82424275</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Calico/">Calico</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Network/">Network</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
    <article id="post-kubernetes_trouble_shooting_000" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/05/27/kubernetes_trouble_shooting_000/" class="article-date">
  	<time datetime="2019-05-27T04:34:05.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/27/kubernetes_trouble_shooting_000/">
        如何清理kubelet产生的挂载点？
        
      </a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>有时Kubernetes节点故障，需要从集群中移除，修复后再加入集群。在这个修复过程中，清理Kubelet相关挂载点是个非常重要的操作。在实际使用过程中，我发现kubeadm工具的reset子命令中有这个操作，因此特意查阅了Kubeadm的源码，找到了源码里的实现方法，在这里做个简单的记录。</p>
<p>清理kubelet相关挂载点的命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 停止Kubernetes的某一个Node节点</span><br><span class="line">systemctl stop kubelet.service &amp;&amp; systemctl stop docker.service</span><br><span class="line"></span><br><span class="line"># 清理Kubelet的相关挂载点</span><br><span class="line">awk &apos;$2 ~ path &#123;print $2&#125;&apos; path=/var/lib/kubelet /proc/mounts | xargs -r umount</span><br><span class="line"></span><br><span class="line"># 恢复Kubernetes的某一个Node节点</span><br><span class="line">systemctl start docker.service &amp;&amp; systemctl start kubelet.service</span><br></pre></td></tr></table></figure>
<p>参考资料：<br><a href="https://github.com/kubernetes/kubernetes/blob/v1.11.0/cmd/kubeadm/app/cmd/reset.go" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/blob/v1.11.0/cmd/kubeadm/app/cmd/reset.go</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Trouble-Shooting/">Trouble Shooting</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/容器云技术/">容器云技术</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
      
    
  </div>
  
</article>







  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/3/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2020 Singh Wang
      </div>
        <div class="footer-right">
          <a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/smackgg/hexo-theme-smackdown" target="_blank">Smackdown</a>
        </div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="/js/main.js"></script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js"></script>


  </div>
</body>
</html>